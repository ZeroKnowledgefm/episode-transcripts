[Anna Rose] (0:05 - 2:06)


Welcome to Zero Knowledge, I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralised web, as well as new paradigms that promise to change the way we interact and transact online.


This week, we kick off our 6-part miniseries on leanEthereum. For these special episodes, we have Nico as our host, and we are also releasing this podcast in video form over on our YouTube channel. Links are in the show notes in case you want to check it out over there. And be sure to subscribe to the YouTube channel while you're at it.


For this first episode, Nico is joined by Justin Drake, researcher at the Ethereum Foundation, to introduce leanEthereum.


They discuss this vision for Ethereum, spanning the consensus, data, and execution layers. Justin outlines how post-quantum cryptography, faster finality, and enshrined ZK are all being used to future-proof Ethereum's core.


They also lay out some of the topics that will be covered in subsequent parts of the series. Hope you will watch and listen along and let us know what you think.


Before we kick off, three quick points.


First, zkSummit is coming back with its 14th edition. Applications to attend and/or speak are now open. It will be on May 7th in Rome, so I hope to see you there.


Second, zkMesh+, our paid subscription tier, is rolling. We have released a few bonus clips over there, as well as some of our exclusive research reports. If you want to get access, please join the paid tier.


Third, if you're looking for other ways to support the show, we have donation accounts visible in the footer of our website. You can donate in ETH, BTC, ZEC, or SOL. Support definitely goes a long way. So if you do decide to donate, thank you in advance.


That's all for now. Links are in the show notes.


And here is Nico's interview with Justin Drake, all about leanEthereum.


[Nico Mohnblatt] (2:10 - 2:29)


Today, I'm here with Justin Drake, researcher at the Ethereum Foundation.


Justin, you were on the show recently to tell us about the ETHproofs initiative. You're back very soon after to tell us about something that I think is a lot bigger and a lot more ambitious, and that's leanEthereum.


Yeah. Welcome back to the show, Justin.


[Justin Drake] (2:30 - 2:31)


Absolutely. Thanks for having me.


[Nico Mohnblatt] (2:32 - 2:42)
So this is the first episode in a series that's going to be dedicated to leanEthereum. So I was hoping that you could help us introduce the topic and understand what it is.


[Justin Drake] (2:42 - 4:01)


Absolutely. So leanEthereum has multiple components that correspond to the various layers of Ethereum L1. We have the consensus layer, the data layer, and the execution layer.


It's easy to remember because it's C-D-E.


At the execution layer, we have the zkEVMs, which is what ETHproofs is all about, increasing the throughput of the L1 and increasing the gas limit.


And then at the consensus and data layer, one very big component is post-quantum, making sure that we have post-quantum attestations and post-quantum blobs. Right now, we're using BLS signatures and KZG, which are not post-quantum.


And at the consensus layer, another aspect is improving the speed of inclusion of transactions and the speed of finality.


So we want to shorten the slot durations and accelerate finality. And here we have this notion of endgame finality. 


And one of the observations for the consensus layer is that the two big upgrades, post-quantum and faster finality, they're very invasive. And so what if we were to combine them in a single fork? Because each individually is largely a rewrite of the consensus layer.


[Nico Mohnblatt] (4:01 - 4:06)


Right. So this is like Ethereum from the ground up, a whole new system?


[Justin Drake] (4:07 - 5:11)


Yes. So when we do embrace the ambition of making this rewrite, then we get further opportunities. So one of them is that we can clear up a lot of technical debt.


For example, we can rewrite the specs using the latest and greatest techniques. And we can also make sure that none of the components are SNARK unfriendly within the Beacon Chain.


And that gives us, as a cherry on top, the ability to SNARKify in real time the consensus layer, which will help for all sorts of use cases, like being able to have trustless wallets that are more secure than the current light clients, as well as having very trustless bridges between the R1, and as well to unlock cheaper validation for those who are on extremely weak devices.


And the meme is, you know, a Raspberry Pi, if not a Raspberry Pi Pico, which is the $8 version.


[Nico Mohnblatt] (5:12 - 5:19)


Wow. Okay. So through the layers, consensus, data, execution, are we going to see ZK everywhere?


[Justin Drake] (5:19 - 5:28)


Yeah. So one of the things that we've decided on relatively early is that we need ZK at the execution layer for the zkEVMs.


[Nico Mohnblatt] (5:29 - 5:19)


And this is back to ETHproofs, the sort of stuff we were already discussing last time?


[Justin Drake] (5:19 - 5:53)


Yes, exactly. But you know, I would make the argument that we also need ZK at the consensus layer.


And the reason is, the post-quantum signatures are very, very large. And so we need to find a way to aggregate them. And the only good way to do that is using SNARKs.


[Nico Mohnblatt] (5:53 - 6:02)


Maybe we can have a bit of context here. So what's happening at the consensus layer? Why do we have signatures, and why do we want to aggregate them? And how does ZK fit into all this?


[Justin Drake] (6:02 - 7:59)


Yes. So the way that proof-of-stake works is that you have these votes, every epoch, they're known as attestations, and we have basically every validator make a vote per epoch. And that corresponds to roughly 32,000 signatures per slot.


So we're talking thousands of votes every single second. And in order to process this incredible throughput of signatures, we need to aggregate them or batch them.


And that's for two reasons. One is to save on bandwidth. And two, to save on verification time.


Because if each signature takes one millisecond to verify, then if you have thousands of them per second, that would be seconds of verification time. You wouldn't be able to keep up.


But the problem becomes even more pronounced in a post-quantum world. And the reason is that BLS signatures are 96 bytes, whereas the post-quantum signatures are, in the best case, on the order of 1 kilobyte,  so 10 times larger.


We need to find solutions to recreate this idea of aggregation at the consensus layer, and there's just not many options.


There's only two flavours of post-quantum cryptography that we would consider from a maturity and security standpoint. These are the two flavours that have been standardised by NIST, specifically the hash-based signatures and the lattice-based signatures.


And you can try and do aggregation with lattice-based signatures, but it's quite hard and it's also quite inflexible.


So we have these requirements that we want to aggregate across distinct messages. We want to be able to recursively aggregate.
So if you have aggregator A and aggregator B each produce their own aggregates, you want to be able to meta-aggregate these signatures.


[Nico Mohnblatt] (7:59 - 8:03)


All these nice things BLS does and post-quantum doesn't.


[Justin Drake] (8:03 - 10:02)


So BLS, unfortunately, doesn't do the meta-aggregation. So if you have two sets of signatures that you're aggregating and they have a non-zero intersection, then you can't do the aggregation there. And so what we end up doing in practise is just concatenation of aggregates BLS signatures.


And that works fine because the aggregates are so short, they're just 96 bytes. But if you're dealing with much larger objects, then aggregation... sorry, concatenation just doesn't work anymore.


And so we're kind of forced to go down the route of SNARKs to solve this aggregation problem. And one of the things that we want to do is make sure that whatever cryptography we put in place for the consensus layer is harmonious with the rest of all of the other layers.


And so the solution that we put in place, which is called leanVM or introduced for the consensus layer, it looks like we actually have an opportunity to deploy it at every single layer, the data layer and the execution layer, in order to solve the problems that we have there as well.


And my personal belief is that leanVM is actually going to escape the bounds of Ethereum for all of the blockchains or most of the blockchains.


And the reason is that every other blockchain has a very similar problem at the execution 
layer. So they're using, for example, ECDSA as the way that you sign transactions.


And if blockchains like Bitcoin or Solana were to move to a post-quantum system naively without the aggregation, they would suffer roughly a 10x decrease in performance. So Bitcoin would go from 3 TPS to 0.3 TPS, and Solana would go from 1,000 TPS to 100 TPS.


And so as far as I can tell, the Ethereum Foundation is the primary entity that's looking into solving this problem for the industry. And I think we have an opportunity to be a first mover, set an example, and ultimately create a blockchain standard.


[Nico Mohnblatt] (10:02 - 10:14)


So you've introduced a lot of different ideas here already. One was SNARKs to fix the aggregation problem, and maybe we should explain how that works. And then leanVM, what is that and how does that fit in?


[Justin Drake] (10:14 - 11:05)


So when you have a SNARK, you can prove knowledge of multiple valid signatures, and these signatures can go in the so-called private witness. They don't need to be shared as public inputs, which means that you don't actually have to put the input signatures onchain.


And one of the crazy consequences of that is that moving to post-quantum will actually be a scalability increase relative to what we have now. And the reason is that you have a fixed amount of data footprint per block, which is just the one proof, and everything else from a signature standpoint is for free.






And actually, because we have a proof per block already with the zkEVMs, there's like zero marginal overhead to moving to post-quantum, which is very nice.




[Nico Mohnblatt] (11:05 - 11:11)


Right. So we'd have one SNARK that proves the consensus stuff and the execution stuff at the same time?


[Justin Drake] (11:11 - 11:39)


So we could do it that way, but more likely than not, there would be subslots, each with their own separate proof. 


But if you look at the execution layer, there's two parts to that. One is the zkEVM stuff, and two would be the aggregate signature for all of the user transactions. And those two proofs can be merged in a single proof.


And you know, you're asking, why do we need leanVM? Well, first, I need to explain what it is.


[Nico Mohnblatt] (11:39 - 11:40)


Yes. Actually, what is it?


[Justin Drake] (11:42 - 12:28)


leanVM is a very, very minimal zkEVM to tackle specifically hash-based cryptography.


So the proposal that we have here is to have these hash-based signatures, both at the CL and the EL, and to basically have a very minimal zkEVM that gives us the expressivity, the power to aggregate them, but also to recursively aggregate, meaning that we want the zkEVM to be able to verify its own proofs, which are also hash-based proofs.


So the idea is to build everything with a single hash function and really embrace the robustness and simplicity that comes with that.


[Nico Mohnblatt] (12:28 - 12:37)


Okay. So this is the idea of like a custom VM. This is not a RISC VM. This is not a circuit either. It's a VM where you design the ISA yourself.


[Justin Drake] (12:38 - 12:57)


Exactly. It's like an ultra RISC.


So RISC stands for Reduced Instruction Set. It's ultra-reduced. Specifically, we have five instructions: add, mul, jump, a memory instruction, and a hash precompile for Poseidon2. So it's as minimal as you can get.


[Nico Mohnblatt] (12:57 - 12:59)


Okay. So that thing is going to be hashing all the time.


[Justin Drake] (13:00 - 13:35)


Yes. So when we do the benchmarks, we actually reached a point where the hashing is so optimised that not that much time is spent on hashing, maybe 10, 20%.


So all of the glue logic that expresses, for example, how you're going to verify the Merkle paths, not just do the hashing itself, that is starting to dominate.


And very roughly speaking, one cycle of the zkEVM is roughly as expensive as one hash function.


[Nico Mohnblatt] (13:36 - 13:54)


Wow. Okay. So we said consensus was a challenge because the signatures were big and we needed to aggregate them. 


Solution there was a SNARK with a dedicated VM for it.


Does the efforts from ETHproofs feed into what is happening here, or are these distinct?


[Justin Drake] (13:54 - 15:06)


So right now they're quite distinct.


So one of the technical details with the consensus layer is that we have to enshrine it from day one, meaning that it has to be part of the protocol and that has to do with slashing.


So in order for everyone to come to consensus that something bad happened, we need the proof to be enshrined.


With the zkEVM, we can take a different approach, which is to rely on off-chain proofs, if you will, that are not enshrined at the beginning.


And this is actually quite appropriate because what's happening at the zkEVM level is a lot of complexity, right? We're dealing with the entirety of the EVM plus the RISC-V virtual machines. And so more likely than not, in the short term, every implementation will have a bug.


And so you want to hedge against these bugs through diversity, just like we have diversity at the EL level.


At the CL, we can actually kind of one-shot it, if you will, with formal verification and have very high confidence that there's no bugs because we're dealing with an object that is so simple.


[Nico Mohnblatt] (15:07 - 15:20)
So actually, this is a good segue into another aspect of leanEthereum, which I find super interesting is formal verification.


Is it coincidental that it's called lean like the theorem prover or not really?


[Justin Drake] (15:20 - 15:21)


It's coincidental, yes.


[Nico Mohnblatt] (15:22 - 15:34)


Okay. Okay. And so I guess maybe it's a good time to introduce our listeners to what formal verification is and how it helps with our certainty that these systems can be secure.


[Justin Drake] (15:35 - 17:05)


Yes. Formal verification can be applied both to mathematics and theorems and to code.


In the context of mathematics, you would have some sort of a theorem statement. And instead of writing the proof for that theorem in a math paper or by hand, you would actually have a machine-checkable proof.


And what "machine checkable" means is that you start with the very basic axioms of mathematics and then you build intermediate lemmas in order to eventually reach your lemma.


And we want to have formal verification of all of the mathematics that is involved in leanVM.


So for example, there's the so-called proximity gaps result, which is the most involved part mathematically.


And here, one of the crazy outcomes is that to a very large extent, this has been proven a couple of weeks ago by an AI. So Alex Hicks, who leads the formal verification effort, kind of gave the theorem statement of what's called the [?] Polishchuk-Spielman lemma.


He let this kind of AI that specialised in mathematics run for eight hours. It cost him $200 and in one shot, it was able to prove the lemma.


And basically what it means is that there's just a bunch of Lean4 code that you can just rerun and know with extremely high certainty that the lemma is indeed correct.


[Nico Mohnblatt] (17:05 - 17:10)


This is the same property that the EF put out a million dollar bounty for?


[Justin Drake] (17:11 - 18:28)


Yes. So there's two separate things. So we have this $20 million formal verification effort that Alex Hicks is leading, and we've been giving a lot of grants for multiple different formal verification efforts.


And then there's this other thing, which is the proximity gaps.


Now, the way that the proximity gaps is handled in practise is that either you go with a proven proximity gap, meaning that we know for sure that we have soundness, or you go with the best known attack to the proximity gap.


And you're making a conjecture that the best known attack of today will remain the best known attack in the future. 


And historically, a lot of the teams were using this conjecture. And what we're seeing right now is the industry migrating from conjectured proximity gaps to provable proximity gaps.


Now, the reason why a lot of teams are very tempted to go with conjectured proximity gaps is that they give you roughly a 2x improvement in the proof size.


And so we have this additional $1 million bounty for anyone who can close the gap between provable and conjectured. But that is separate from the $20 million formal verification.


[Nico Mohnblatt] (18:29 - 18:42)


And actually, if anyone feels like we've been over this super quickly, don't worry, we have an episode planned on proximity gaps and all these security questions and an episode planned on formal verification with Alex, who you were mentioning a second ago.


[Justin Drake] (18:42 - 19:08)


Yes. And very, very briefly, in addition to proving the theorems and the mathematics are correct, we can also prove that the code is correct.


So when you're building a zkEVM, for example, you want to make sure that the arithmetization corresponding to your ISA is correct.


And what you would do there is that you would have a formalisation of the ISA, and then you'd have the implementation in code, and you want to show that these two things are equivalent.


[Nico Mohnblatt] (19:09 - 19:14)


Right. So here is what I want my machine to do. Here is the polynomials I generated from my code.


Do they match?


[Justin Drake] (19:15 - 19:15)


Exactly.


[Nico Mohnblatt] (19:16 - 19:18)


So airtight security through and through, essentially.


[Justin Drake] (19:19 - 19:43)


Exactly. It's what we call end-to-end formal verification. And when you're building a zkEVM, there's multiple layers of the stack. You know, there's the mathematics, there's the backend, there's the frontend.


And in our case, there's also the programme that you could be running on top of the zkEVM, which in our case would be the signature aggregation programme, as well as the recursive verifier. And that's also something that we want to formally verify.


[Nico Mohnblatt] (19:44 - 19:52)


Wow. So we're inventing leanVM, you're formally verifying leanVM, and then you're going to formally verify programmes written for leanVM.


[Justin Drake] (19:53 - 19:53)


Exactly. Yes.


[Nico Mohnblatt] (19:54 - 20:08)


Wow. We're talking a lot about hash functions, signatures from hash functions, SNARKs from hash functions. The elephant in the room is which hash function are we going to use?


[Justin Drake] (20:08 - 22:12)


Yeah. So the answer is that we don't know today, but we do have a top candidate. Which is called Poseidon2.


It's very, very SNARK friendly, meaning that if we build our basic hash-based SNARKs with it, we'll be able to aggregate way more signatures than if we were to use a traditional hash function like SHA256 or Blake.


We're talking about more than a 10x improvement in performance.


Now, when we introduce a new hash function, there's this baking time for the community to get comfortable with the hash function. And the heuristic that I have in mind here is roughly 8 years.


And the reason is that when Satoshi picked SHA256, it was 8 years old. And when Vitalik picked Keccak for Ethereum, it was also 8 years old.


And Poseidon is roughly 5, 6 years old. So there's more time that we need in order to have confidence in it.


But we don't want to just wait for this function to age naturally. We kind of want to accelerate the maturation process, and we're seeing that in multiple ways.


One is that Ethereum Foundation is organising all sorts of workshops. So we had three last year. We're going to have at least two this year.


These are cryptanalysis workshops where we invite experts, we give them grants to try and break the hash function. We also have a natural bounty programme, which is happening through the zk-Rollups.


So several billions of dollars are being secured today by Poseidon2 indirectly through the zk-Rollups. And what we announced maybe a week ago or so is this new programme called the Poseidon Prize, which is an additional $1 million for anyone who can break Poseidon.




We'll be revealing some of the details of that prize in ETHproofs Call number 8, which should be on March 20th.


[Nico Mohnblatt] (22:13 - 22:19)


Amazing. It should be worth mentioning as well, why can't we just formally verify the security of Poseidon?


[Justin Drake] (22:19 - 23:56)


When you do cryptography, you have these basic computational assumptions. It goes back to what I was saying about the proximity gaps. The best that you can do is look at the best known attacks and try and improve those, but you can't actually prove them.


And so you can reduce everything to your most minimal assumption, which might be that the hash function is preimage resistant or second preimage resistant. But proving that it actually is is extremely difficult.


The best that you can do is prove that it's not secure by showing an algorithm that would break it.


Now, interestingly, and this is a bit of a side note, but the cryptography that we're building doesn't require what's known as collision resistance, and so the hash functions that have broken in the past, for example, SHA1 and MD5, they've lost collision resistance, but they haven't lost second preimage resistance.


And so if we were to instantiate the cryptography, even with such a broken hash function like MD5 or SHA1, the schemes would actually be still secure.


And so even if Poseidon were to break its collision resistance, we would have a period of time where we could safely migrate away to something else, and that could act as a canary in the coal mine, if you will.


[Nico Mohnblatt] (23:56 - 24:07)


Wow. Okay. So second preimage is sort of a weaker security property than collision resistance, and so... or like a weaker requirement. I guess you can break one without breaking the other, right?


[Justin Drake] (24:07 - 24:08)


Yeah, exactly.


[Nico Mohnblatt] (24:08 - 24:09)


Yeah. Cool.


[Justin Drake] (24:10 - 24:15)


Yeah. It's easier to break collision resistance than it is to break second preimage.


[Nico Mohnblatt] (24:16 - 24:41)


Okay. So I think the picture is starting to draw out. We have SNARKs signatures all relying on Poseidon.
I'm curious, how is this going to be rolled out?


Actually, maybe two questions in one. One is like, we wanted more throughput. How is the network going to survive more throughput when our signatures are bigger? And then the next question is, how do we test this out?


[Justin Drake] (24:42 - 27:17)


So the throughput aspect of the zkEVMs, in some sense is largely at this point, unrelated with post-quantum.


So really what we're trying to do at the consensus layer is retain the throughput that we have today. Now, what is the throughput?


I mentioned that it was 32,000 votes per slot, which is a couple thousand signatures per second. And we want to retain that.


Today, we have a million validators, and the voting is spread over 32 slots. But in the future, we want to have closer to 32,000 validators, each casting a vote every single slot. And this is the basis of faster finality.


Now, in order to retain this voting throughput, we need to come up with the equivalent of BLS.


The basic idea here is that we take all of the attesters and we split them up in subnets. Within an individual subnet, which might be only a thousand validators, we do a first round of aggregation.


And note here that recursive aggregation is already useful in case that you have the aggregator doing censorship of a minority of validators. So anyone can take the best known aggregate and enrich it with any censorship that might have happened.


And then we want to do aggregation across the subnets, so that we have a final aggregate if we have this two-layer aggregation.


But we're also considering what's called three-layer aggregation, where we have very, very small subnets to start with, and then we do aggregation across some subsets of those, and then we do the final proof again.


And the advantage of moving to three-layer aggregation is that it would allow us to have significantly more validators. Because basically, the number of validators would go to the power three as opposed to the power two with two layers.


And one of our endgames that we want to try and achieve here is to have a million validators be able to participate in Ethereum proof-of-stake so that the minimum deposit amount to become a validator is as low as possible.


[Nico Mohnblatt] (27:17 - 27:26)


Just to make sure I understand this correctly, this layering of aggregation layers is completely new to leanEthereum. It's not something that's happening already with BLS, right?


[Justin Drake] (27:26 - 28:39)


No. It is actually happening with BLS. So today we have the two-layer aggregation. We have some constraints with BLS that I mentioned.


So for example, to solve this censorship problem, what we're doing is that we have multiple nominated aggregators per subnet. For example, we have eight of them. And they each do a best effort at doing the aggregation.


And then the final aggregate is basically the concatenation of these eight things.


But this concatenation idea just doesn't work very well in the context of post-quantum because you're dealing with much bigger objects. And ironically, the flexibility that comes from recursive aggregation allows us to have a much wider design space for the aggregation topology.


So instead of nominating eight aggregators per subnet, we can just allow anyone who wants to be an aggregator to be an aggregator. So it's permissionless and it can be much more unstructured.


Instead of being like a strict hierarchy of aggregators, we can have more of a kind of fluid emergent mesh network type of aggregation.


[Nico Mohnblatt] (28:39 - 28:56)


Okay. I want to touch upon the last layer that, and not aggregation layer here, like the C-D-E layers you mentioned at the very beginning.


We talked a lot about consensus. We talked a bit about execution. Data, we didn't really mention. Is that also part of the leanEthereum upgrade?


[Justin Drake] (28:57 - 29:22)


Yes, it is. Like moving the data layer to post-quantum blobs is part of it. And one of the solutions that we're considering doing is to take your blob, erasure code it, for example, with a Reed-Solomon code, Merkle-ize that, and then prove within leanVM that the Merkle root is correct.


[Nico Mohnblatt] (29:23 - 29:25)


Beautiful. So leanVM once again rears its head.


[Justin Drake] (29:27 - 30:32)


Yes, exactly. Ultimately, we're trying to have as many synergies as possible and reduce the total amount of cryptographic machinery. And since we're biting the bullet with this cryptographic zkEVM, we might as well use it for many different layers.


One of the question marks is, is it going to be efficient enough to SNARKify this erasure coding and the Merkle tree, especially when we're dealing with very, very large blobs?


One of my hopes is that we can unlock the so-called tera gas per second kind of era, where all of the L2s combined that consume Ethereum DA can have enough DA to support the one tera gas per second, which corresponds to 10 million transactions per second.


And from a throughput perspective, we're talking about one gigabyte per second for all of the L2s. And it's several orders of magnitude larger than what we have today.


[Nico Mohnblatt] (30:32 - 30:40)


I think that cuts back into the ETHproofs discussion we had last time you were on the show, I think almost 6 months ago.


[Justin Drake] (30:40 - 30:49)


Yes, exactly. So on L1, we want one giga gas per second, which is 10,000 TPS and a thousand times more for all of the L2s combined.


[Nico Mohnblatt] (30:49 - 31:22)


Amazing. So, so far, what I have planned for this series, Justin, is to talk about the signatures in one episode and how we argue them, to talk about the security of post-quantum-ness, I guess, or post-quantum migration.


We'll have one on leanVM, obviously, and implementing all this.


There was one about the networking, DevNet, and one about formal verification.


My question to you is, have we missed any big aspects? And is there something we should cover, you and I, now, before splitting off into smaller episodes?


[Justin Drake] (31:23 - 32:29)


No. I think this covers it a lot. And one of the difficult things with the leanEthereum project was to find the right people to work at the different layers.


And one of the observations is that we have a very lean team. And I've been trying to put forward this meme of the leanEthereum Foundation, where we have this elite team of devs and researchers. And nowadays, a lot of the help is coming from AI.


So just recently, with Opus 4.5, things kind of flipped where AI was kind of net slowing things down for some developers, and now it's speeding things up. And, for example, on leanVM, we primarily have one person, who is Emile. And he possibly can single-handedly do most of the R&D for the virtual machine.


[Nico Mohnblatt] (32:29 - 32:36)


Which is absolutely wild, right? Because 2, 3 years ago, we had entire teams dedicated to coming up with a VM, implementing it.


[Justin Drake] (32:37 - 33:00)


It's absolutely wild. I mean, we also have a lot of the heavy lifting that's happening in the libraries underneath it.


So we're reusing Plonky3, and we also have an implementation of WHIR in Plonky3, which is doing a lot of the heavy lifting.


And there was a lot of work done on [?] Thomas and others on that specifically.


[Nico Mohnblatt] (33:00 - 33:07)


That makes sense.


So, Justin, what's, I guess, next for leanEthereum? What's the path forward? And how can people follow along if they're interested?


[Justin Drake] (33:08 - 33:56)


Yes. We have a weekly call, which is the Post-Quantum Interop Call, where a lot of the new teams that have started implementing the leanSpec  are collaborating through DevNet. So this is very technical. 


We also have various events that are being organised. So in Cannes at EthCC, we're going to have a post-quantum day that we're calling Fort Mode. And we're also going to repeat the very successful post-quantum workshop that we did last year in October. So this is a three-full-day post-quantum workshop in October.


And yeah, we have a limited number of seats, but if you are interested in it, please feel free to DM me.


[Nico Mohnblatt] (33:57 - 34:15)


Yeah. Having been to the previous one, I can only recommend it. It was a truly amazing experience. It felt really like, The Imitation Game, Bletchley Park, Alan Turing kind of situation.


That's kind of what it felt like. This group of people meeting every day, like, let's do this post-quantum thing.


Yeah. Truly amazing workshop.


[Justin Drake] (34:16 - 34:21)


Yeah. And like various papers came out of it, possibly even the one from you.


[Nico Mohnblatt] (34:21 - 34:35)


Yes, absolutely. We were talking about FRI and like how to do things simply. And all the proximity gaps, a lot of them came out of workshops happening there.


And we're going to touch upon this in the third episode, actually.


[Justin Drake] (34:36 - 35:14)


Yeah. Absolutely.


And I think the research and development will reach a point of maturity that the vibe of the workshop will be slightly different. It will be more about productionizing and standardising these things. And how do you make sure that it's a solution that's applicable, not just for Ethereum, but also for Bitcoin.


And I'm hoping to make a big effort to make sure that the Bitcoiners and the Solana folks will also use it because it would be a bit of a disaster if every single blockchain came up with their own post-quantum solution, and that's lots of duplication of effort.


[Nico Mohnblatt] (35:15 - 35:21)
Yeah, absolutely.


Justin, thank you so much for coming on the show and for introducing leanEthereum.


[Justin Drake] (35:21 - 35:21)


Absolutely.


[Nico Mohnblatt] (35:22 - 35:32)


Yeah. It's super fun to get to do this series and to get to dig deeper with all this amazing team that you mentioned, because a lot of guests for the next episodes are part of that core lean team, right?


[Justin Drake] (35:33 - 35:34)


Yes. You're in for a treat.


[Nico Mohnblatt] (35:34 - 35:47)


Yeah. So thank you very much.


I also want to thank the podcast team, Anna, Rachel, Henrik, Tanya and Hector.


To those of you catching us on video, thank you for watching.


And to our listeners, thanks for listening.