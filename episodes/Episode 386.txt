[Anna Rose] (0:05 - 0:21)
Welcome to Zero Knowledge, I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero-knowledge research and the decentralised web, as well as new paradigms that promise to change the way we interact and transact online.


[Anna Rose] (0:28 - 2:03)
This week, Nico and I chat with Pratyush Mishra, Assistant Professor of Computer and Information Science at the University of Pennsylvania.


We catch up with him on the research he's produced since the last time he was on the show back in 2021. We discuss the topics and themes that he is covering, such as working on SNARKs with tiny proofs with Garuda and Pari, folding schemes based on hash functions with his work Arc, IOPP a.k.a the proximity proofs work with FICS and FACS, low-memory SNARKs, SNARK applications outside of blockchain, and much more.


A lot of these topics will be familiar to longtime listeners of the show as his work weaves in and out of some of the most cutting-edge topics in ZK research today. So it's great to catch up with him.


Now before we kick off, I just want to share that we've launched Season 3 of the ZK Whiteboard Sessions. The ZK Whiteboard Sessions are produced by ZK Hack in collaboration with Bain Capital Crypto. This series goes deep on some of the key concepts in ZK.


I've added a link in the show notes. Be sure to check out these videos. They are a great way to learn about the fundamentals and techniques powering the ZK systems we often discuss on the show.


Also if you're looking to jump into ZK professionally, or just looking for a new role, I wanted to point you towards the ZK Jobs Board. There you can find job postings from top teams working in ZK.


And if you're a team looking to hire, you can also post your job there today. We've heard great things from teams who found their perfect hire through this platform, and we hope it can help you as well.Find out more over at  jobsboard.zeroknowledge.fm. You can find this on our website, and I've added it to the show notes.


Now Tanya will share a little bit about this week's sponsor.


[Tanya Karsou] (2:05 - 2:49)
Aztec invented the math, wrote the language, and proved the concept that privacy is in fact the missing link to mass adoption. And now Aztec is on the road to mainnet.


Aztec is a privacy-first layer 2 on Ethereum, supporting smart contracts with both private and public state, and private and public execution.


It is one of the most exciting ZK projects out there, and we are excited to see it go live. There are a tonne of ways to get involved. You can join the testnet, explore the Noir ecosystem, become part of the community, or look out for ways to participate in the last steps before the upcoming mainnet launch.


Details about Aztec's technology, research, and community programmes are available at aztec.network.


And now, here's our episode.


[Anna Rose] (2:52 - 3:02)
Today, Nico and I are here with Pratyush Mishra, Assistant Professor of Computer and Information Science at the University of Pennsylvania.


Welcome back to the show, Pratyush.


[Pratyush Mishra] (3:02 - 3:09)
Thank you, Anna, for having me. And yes, wonderful to be back after quite a while, and I'm very excited for today's episode.


[Anna Rose] (3:09 - 3:14)
Very cool. For this episode, I have Nico as my lovely co-host.


Hey, Nico.


[Nico Mohnblatt] (3:14 - 3:19)
Hey, Anna. Hey, Pratyush. Looking forward to chatting about some nice SNARK research.


[Anna Rose] (3:19 - 3:25)
Yeah. And I'm happy to have you here, Nico, because I think in this episode, I have the sense we're going to go deep into a lot of research papers.


[Nico Mohnblatt] (3:25 - 3:26)
I have a whole list ready to go.


[Anna Rose] (3:27 - 3:52)
Yeah. Perfect. Very cool.


So the last time we had you on, Pratyush, was back in 2021. We covered the Arkworks library. And before that, we had you on for Zexe. But just going back to our last episode, Arkworks, it's been a very important component of the ZK space, and I'm just wondering if it's still an active project, like what's your connection to it?


[Pratyush Mishra] (3:52 - 4:08)
Yeah. So I'm still a co-maintainer of Arkworks. I definitely have less time than I used to as a PhD student, but we have my students working on various parts of it here and there, and some folks from other universities are also contributing.


[Anna Rose] (4:09 - 4:21)
Actually, and I want to hear sort of what's happened with you professionally as well, because last time you were on, I'm pretty sure you were still a student at Berkeley.


So what's been your path? Now you're an assistant professor. Tell me how that happened.


[Pratyush Mishra] (4:21 - 4:53)
Yeah. Yeah. So I think last time I was on was in 2021. I wrapped up my PhD in September of that year. So I think a few months after that podcast.


And after that for a year and a half-ish, I was Aleo, working full time as a cryptography research engineer, kind of a mix of both.


But then in 2023, after that, I started at the University of Pennsylvania as an assistant professor. And that's where I've been since then.


[Anna Rose] (4:53 - 5:02)
Cool. What's the department like there? And is there some sort of group you're part of? What's the sort of ZK scene like there?


[Pratyush Mishra] (5:02 - 5:57)
Yeah. I think we actually have quite a strong cryptography and computer security group here at Penn. So in terms of other than me, there's Sebastian Angel. He's another professor who works on cryptography and has done a lot of work on more of the, maybe, applied side of zero-knowledge.


So designing new kinds of zero-knowledge schemes for specific applications. We might touch on one work that we've done together maybe later on.


And we also have Tal Rabin. She's on leave right now, but she's kind of a legend in the field of cryptography, but she's done a lot of work on threshold signing, for example, something of interest to listeners of this podcast.


And I think we're actually looking to expand the group broadly in computer security. And outside of that, the department has folks basically in every area of computer science. So it's been really fun.


[Anna Rose] (5:57 - 6:10)
I've also noticed you have been doing a lot of collaborations with like Benedikt, I feel, over the years. And you have some other, I mean, Howard, Alessandro. There's a few people I see you collaborate a lot with.


Who else would you list as your kind of collaborators?


[Pratyush Mishra] (6:11 - 7:38)
So Alessandro was my advisor. So I think the collaboration there makes complete sense.


I think we started working with Benedikt. So there was this programme at the Simons Institute in Berkeley that Alessandro had organised, I think, Blockchains and Decentralising Society or something along those lines. It's a semester-long programme with some workshops.


And one of the focusses was on proof systems. One of these workshops was on proof systems. And I think Benedikt was visiting up from Stanford for that semester. So that's when we kicked off the initial work on accumulation.


So we saw the Halo paper and we were like, what's going on here? We just tried to understand it. So that's how the collaboration with Benedikt got started, and that's been continuing since then.


Other collaborators, so I guess Ian Miers, I think who you had on the podcast recently. So collaboration with him actually started way back in 2016 or 2015 or so. He had already worked with Alessandro on Zerocash.


And so then when I started my PhD, we started exploring follow-ups to Zerocash. So we had this one paper on micropayments, and then I guess the next main paper was Zexe.


And then since then, yeah, we've had a couple of follow-up works like Hekaton and also some ongoing work, which will hopefully be on ePrint this week or so.


[Anna Rose] (7:38 - 7:40)
Oh, cool. Maybe by the time this airs.


[Pratyush Mishra] (7:40 - 7:41)
Yeah, hopefully.


[Anna Rose] (7:41 - 8:19)
Nice. So I think, I mean, there's a lot to cover. We, in sort of prepping this, Nico, you kind of put together some themes. I picked out some papers. We sort of merged them into a bit of a plan.


So we're going to probably go through some of these topics that you mentioned. And obviously, we're going to hear more about the collaborations you've been doing along the way.


I do have just one question, and I don't know how you want to answer it, but what have you been up to since 2021?


You sort of talked a little bit about where you're working, but what are the themes that you think have really stood out to you? What are the lines of research that you're following?


[Pratyush Mishra] (8:20 - 10:12)
Yeah. That's a good question and something I think I'm still figuring out myself.


I think in general, the ZKP space has been moving very fast and kind of diversifying and broadening into all kinds of different directions that people have been taking on, from protocol design to application to everything in between.


So a lot of my work has been on ZKPs. And I guess since my PhD, I've kind of broadened out beyond maybe these elliptic curve-based SNARKs, is what I really focused on during my PhD, and I've started to work with more hash-based SNARKs and error-correcting code-based SNARKs. So that's been one direction.


Another direction has been essentially trying to investigate properties of ZKPs and SNARKs in different kinds of computational environments. So when you have low memory for the prover or when you have, I don't know, lots of machines and you want to do distributed proving.


So basically moving beyond just the standard, I don't know, single server style proving, seeing what's possible in that space.


Beyond that, also I've been looking at applications of SNARKs, both in maybe more blockchain-oriented applications, but also in kind of non-blockchain settings. Because I think SNARKs are kind of a perfect fit for the blockchain application space where you want sometimes the privacy property, and SNARKs are perfect for that. Sometimes you want really the succinct integrity properties, and SNARKs are again perfect for that.


But I think in general, outside of this kind of perfect match between blockchains and ZKPs, in other areas of computer science, it's been a little bit difficult to provide a value proposition for SNARKs. So I've been trying to figure out various problems in computer security, which would benefit from the kinds of guarantees that SNARKs can give you.


[Anna Rose] (10:13 - 10:45)
Nice. That idea of ZK sort of accelerating past blockchain, I mean, we've seen hints of it. We've seen it in any sort of overlap with AI systems. Sometimes the blockchain is just not even involved there.


Do you think... I mean, it's interesting you're saying ZK couldn't really find its place within the other realms of computer science, but do you think ZK is now, and SNARK development, do you think it's at the point where the performance is good enough that it's actually a viable option? Is that what's changed, kind of?


[Pratyush Mishra] (10:45 - 13:07)
Yeah. I think that's certainly one component. If you go back 5, 6 years ago, SNARKs were just too slow for kind of non-trivial real software that people ran on their computers. So that's improved.


I think there's still quite a ways to go, and people are... there's a massive industry now devoted to that aspect of improving SNARKs.


But I think it's also a question of do business needs... outside blockchain actually need the kinds of guarantees that SNARKs give you?


So in blockchain, it's kind of the entire point is not to have to trust anyone or to really minimise your trust in certain entities.


But in the real world... in the non-blockchain world, not real world, because blockchain is very real, but in the non-blockchain world, people are maybe okay with just signing a business contract and saying that, okay, if you don't satisfy this particular part of the agreement, then you will have to pay this fine or you will have this kind of remediation procedure or some other kind of mechanism to handle that. So you have other ways to enforce that kind of trust requirement.


So there's a lot of cases, especially business to business where you might not want the super strong guarantees that SNARKs give you. But there are plenty of problems in computer security, which you still need these kinds of guarantees.


So I don't know, for example, today, if you download some software off the internet and run it, you don't really have any guarantees that it's not going to do anything bad.


So if you can give a proof that this software adheres to certain kinds of it only does behaviour within some realm of allowed behaviours, that would be, for example, a guarantee that would allow you to develop more secure systems, to deploy software more securely, maybe reduce some kinds of mechanisms that we have right now, which impose efficiency overheads, like sandboxing of virtual machines and things like that.


So there's things that computer security focuses on, which so far we've tried to develop alternative solutions for where the strong guarantees of SNARKs can... at least we can hope that they can provide some benefits.


And I think the time is coming. We're coming to a time when we can start investigating and looking into that direction.


[Anna Rose] (13:06 - 13:06)
Cool.


[Nico Mohnblatt] (13:07 - 13:15)
It's almost like we're finally at a point where we can stop looking at how to make better SNARKs and just look at, let's use these. Like where can we use these?


[Pratyush Mishra] (13:15 - 13:37)
Yeah, partially. I think that's certainly one direction that we can, or approach that we can take. I think, unfortunately, we're not quite there that we can just throw a SNARK at a problem and it'll just work.


I think there's still room for doing kind of intelligent protocol design where you take the application, look at specific characteristics of that workload, and then go back and use that to inform your protocol design.


[Nico Mohnblatt] (13:37 - 13:39)
By protocol, you mean the proof system?


[Pratyush Mishra] (13:40 - 13:52)
Yeah, the proof system.


So I think we're seeing this with ZKML, where people are designing specialised protocols for the machine learning workloads. And I think that kind of approach is very fruitful still.


[Nico Mohnblatt] (13:53 - 14:31)
We semi-dismissed the idea of making better SNARKs, but that's not true. And Pratyush has been working a lot towards making better SNARKs. So some of the topics I've outlined is you have works on making tiny proofs, and by tiny, I mean Groth16 or smaller.


You have work on folding or what we call accumulation from hash functions. There's more hash functions in the land of FRI-like protocols or protocols that take on that functionality.


You were telling us before the show that you have, and actually during this intro, that work on low-memory environments, how do we prove?


So I think those are the main ones we want to hit in that realm of improving SNARKs. Where would you like to start from, Pratyush?


[Pratyush Mishra] (14:32 - 14:38)
Good question. Maybe we can start off with the tiny proofs because I think that may be more self-contained.


[Nico Mohnblatt] (14:38 - 14:59)
Okay. So to give a bit of context, I guess we've been using Groth's, famous Groth 16 proof for a while now. As the name indicates, it dates almost from 10 years ago. And back then Jens Groth proved that this was the smallest proof that we could get, except not really, right?


Could you help us qualify that statement and how you got to something smaller?


[Pratyush Mishra] (14:59 - 16:42)
Yeah. So Groth16 is kind of this legendary proof system that we all know and love, and it still powers many, many applications today.


So Jens in that paper, he proved both the construction and also he gave this kind of barrier which said that maybe this is the best, something like particular barriers the best you can do. So concretely, the construction, I'll be a little bit technical, gets a size of two group elements, G1 elements and one G2 element in your pairing-friendly elliptic curve. 


And the result from... so that was the construction and the lower bound or barrier says that the best you can do is a SNARK with one G1 element and one G2 element, if you are constrained to the kind of construction or construction paradigm that the Groth16 proof system uses.


So this is the kind of paradigm which goes back to the first, maybe I would say concretely efficient SNARK, which is GGPR 2013 or the Pinocchio construction. There were follow-up works like BCTV14, but essentially all of these works they follow this paradigm called constructing a SNARK from Linear Probabilistically Checkable Proof or LPCP.


And Groth essentially showed that if you take this approach, the best you can do is a SNARK with one G1 element and one G2 element.


So that was kind of a benchmark that he had set. And his paper didn't actually achieve that barrier. So he had two G1 elements and one G2 element.


So it was kind of an open question to see whether by working in that framework, you could achieve even that lower bound. So that was an open question for... I think it still actually is an open question.


[Nico Mohnblatt] (16:42 - 16:50)
So the paper you have, I think titled Garuda and Pari, does it not resolve that question? How come it's still open?


[Pratyush Mishra] (16:50 - 18:33)
Yeah. So that's a good question. So what we did was we said, okay, this seems like a hard problem. So let us not try to solve exactly that question, and we'll make our lives easier by using a tool that we've since figured out how to apply to SNARKs, which is working in this random oracle model.


So today, essentially, maybe after Groth16, we got this flurry of works which tried to construct proofs using a different approach, which relies on polynomial interactive oracle proofs and polynomial commitment schemes and combines these to achieve, first, an interactive argument, and then you apply what we call this Fiat-Shamir transform, which basically gives you a non-interactive proof.


And the Fiat-Shamir transform, it basically says, instead of interaction, I'm just going to derive the verifier messages using a hash function or a random oracle.


So this is kind of the approach underlying Plonk, Spartan, Marlin, essentially every proof system that we use in practise today, maybe outside of Groth16, every proof system uses kind of this direction.


And the interesting thing to note is that Groth's lower bound does not apply to this model. So once you start invoking the random oracle, you don't have this barrier anymore.


So what we did in Garuda and Pari... we didn't actually, first of all, set off to solve the problem of designing very small proofs. That happened as a happy accident, which we can dive into in a bit. But specifically, there's two SNARKs in that paper.


Pari is the one which achieves a smaller proof size, but it works in a modification of this polynomial IOP plus polynomial commitment scheme model and invokes a random oracle to bypass this barrier, to avoid the barrier.


[Nico Mohnblatt] (18:34 - 18:43)
So I'm curious now, what was the goal of this paper, if not making tiny proofs? Because that's how I conceptualise it when I sort of read it. But what was the hidden goal?


[Pratyush Mishra] (18:44 - 20:27)
So this paper actually started... I started thinking about it when I was still at Aleo and kind of the motivation there was actually to understand custom gates better.


Because at the time this was like, I don't know, 2022, early 2023 or so, people were starting to deploy custom gates, but it was always very tied into the Plonk ecosystem. But I guess everybody had their own variant of like, okay, we have our own custom gates and here's our own variant of this Plonk-ish way of writing constraints.


And I was like, there's got to be something here which we can extract out and distil and figure out what is kind of... there must be some orthogonal component.


Because at the time, like before that, everybody was using R1CS and it wasn't clear that this new idea of custom gates or things like lookups can be integrated into R1CS. And I think as a result, people like moving away from R1CS towards these Plonk-ish constraints for the larger flexibility that they offered.


But I thought, okay, maybe there's... I wanted to understand it better and see what was like the core there. So I tried to, initially, I guess the work on Garuda and Pari started off by trying to take something like Spartan or Marlin and add support for custom gates in there.


And basically trying to create a version of R1CS, which supported, I guess, more exotic gates beyond just a standard multiplication gate.


So we were investigating the line of work and we had some ideas which didn't really give us anything particularly exciting from, I would say, a research perspective. And also right around that time, I think there was this paper which came out.


[Nico Mohnblatt] (20:27 - 20:28)
Is it the CCS paper?


[Pratyush Mishra] (20:28 - 20:32)
Yeah. So from, I think, Srinath, Riyadh, and...


[Nico Mohnblatt] (20:32 - 20:33)
Justin Thaler.


[Pratyush Mishra] (20:33 - 21:20)
I think Justin Thaler. That came out and it had essentially all of the ideas that we wanted to explore on that front.


And essentially, they were able to demonstrate that, okay, there's nothing specific to Plonk-ish about these custom gates and lookup tables. You can take those ideas and port them to work in the generalisation of R1CS and oftentimes actually maybe get better efficiency because Spartan, for example, is a very efficient proof system.


So that's kind of where we stopped thinking about that particular problem or that line of thinking.


And then what we maybe started investigating was a slightly related question, which is that, okay, so now we know that you can have generalisations of R1CS which can support...


[Nico Mohnblatt] (21:20 - 21:22)
Custom gates, lookups, yeah.


[Pratyush Mishra] (21:22 - 22:33)
Yeah. Exactly. And then we also saw that, okay, if you have something like Spartan or Marlin or things like that, you can extend those to support this kind of generalised constraint system.


But what we wanted to do was to maybe take that idea and try to port it back to these trusted setup SNARKs and see if you could design trust or maybe circuit-specific trusted setup SNARKs to be concrete, and see if those could also be generalised to support these custom gates.


And that's where the investigation initially started. This is what ended up becoming Garuda. So one of the SNARKs in Garuda and Pari.


And we were able to show that actually you could get a SNARK which is even faster than Groth16 for the circuit computations by leveraging things like custom gates, but even actually for plain R1CS.


So we were able to tie in or bring in sumcheck-like techniques into this circuit-specific trusted setup world. But that eventually led to Garuda and we were following that direction.


And then one day I was cycling back from work, back home, and then I was like, how far can we take these ideas in this paper? And then I was stopped at a... and this is like a very vivid moment for me.


[Anna Rose] (22:33 - 22:34)
I like that.


[Pratyush Mishra] (22:34 - 23:03)
I had like a eureka moment, but I was stopped at a traffic light and I was like, wait, what if you did this? You moved away from multilinear polynomials, moved to univariate polynomials and then got rid of one group element?


I don't know what was going on in my head at that moment, but I went home and I started writing down and I was like, this should work. And then that became Pari.


And that was essentially led us to have a SNARK with just two group elements and, but not quite Groth's lower limit. We also had two field elements in there.


[Nico Mohnblatt] (23:03 - 23:06)
Are these now the smallest proofs that we know, specifically Pari?


[Pratyush Mishra] (23:07 - 23:27)
That's a good question. I think in very recent work from Liam Eagan, he was able to reduce the proof size of Pari further.


So Pari, we get two G1 elements and two scalar field elements. I think in Liam's work, he was able to bring that down to just one field element, which makes that the smallest SNARK.


[Anna Rose] (23:27 - 23:28)
Was that Glock?


[Pratyush Mishra] (23:29 - 23:29)
Yeah, I think so. I think.


[Nico Mohnblatt] (23:29 - 23:32)
But is that in the designated verifier setting?


[Pratyush Mishra] (23:32 - 23:49)
So the core idea, like the construction that he analyses and proposes is in the designated verifier setting, but the technique can be ported to the public verifier setting as well. And it ports in an idea from Groth16 into Pari to get rid of that one field element.


[Nico Mohnblatt] (23:49 - 24:03)
So in a very succinct way, just to wrap up this topic on Garuda and Pari, could you maybe explain how using the random oracle benefits sort of the existing way of making SNARKs from like Groth16, et cetera?


[Pratyush Mishra] (24:04 - 25:04)
Yeah. So roughly the idea is that instead of starting with the approach that Groth16 takes, which is these linear PCPs, we go back to the PIOP plus PC scheme approach. And one key idea is that if you look at SNARKs like Marlin and Spartan, they have two components, one which proves the R1CS matrix vector multiplication, and one which does the Hadamard product check.


In Groth16, that matrix vector multiplication is kind of encoded in the trusted setup, which is why it's circuit-specific. 


So we kind of combine the two. So we say we're going to use the circuit-specific trusted setup to do the matrix vector multiplication, and then we're going to use a standard PIOP for doing the Hadamard product or custom gate check.


And this required us to generalise polynomial commitments to also allow us to enforce some linear constraints on the committed polynomials, and leading to a new notion of PC schemes. But essentially, that's the high-level trick, which we did. We combined the two lines of work to kind of take the best from both.


[Nico Mohnblatt] (25:05 - 25:05)
Really cool.


[Anna Rose] (25:05 - 25:09)
Nico, shall we move on to our next topic that we wanted to cover?


[Nico Mohnblatt] (25:09 - 25:15)
Yeah. I think it's a good time to do so. The next one on the list was folding from hash functions specifically.


[Anna Rose] (25:15 - 25:40)
Yeah, folding with hash functions. I mean, we just had Binyi on the show, and we were talking with him about folding. We kind of did a redefinition of folding, but in the context of lattices.


I mean, lattices build themselves as post-quantum secure. My first question to you, using folding with hash functions, do you get that post-quantum secureness as well? Or is it a lesser post-quantum secure? How does it compare?


[Pratyush Mishra] (25:41 - 26:07)
Yeah. So the post-quantum security guarantee that you would get here is very similar to the post-quantum security guarantee that you get for other hash-based SNARKs, like FRI-based SNARKs. So it's kind of exactly the same guarantee.


In the case of hash-based SNARKs, people have done the legwork to actually prove that you do get this guarantee. We have not done it so far, but adapting the existing work should not be like a massive lift.


[Nico Mohnblatt] (26:07 - 26:13)
So are these all the works that revisit things like Fiat- Shamir and Merkle trees in the quantum random oracle model?


[Pratyush Mishra] (26:14 - 26:36)
Yes. In particular, I think there is this work in 2019, I think, from Chiesa, Manohar, and Spooner, which shows that applying this BCS transform to IOPs gives you a post-quantum secure argument. I suspect the same ideas can be lifted to the folding constructions as well.


But yes, so the high-level bit is probably, we don't have any reason to think the answer is no.


[Anna Rose] (26:36 - 26:41)
Okay. How does it stack up? Are you following the lattice stuff as well?


[Pratyush Mishra] (26:41 - 27:27)
At a high level. I'm not so familiar with the low-level details. But I think the key tradeoff between, or maybe the difference between the lattice work and the hash-based work is, I suspect that you could potentially get better efficiency in the lattice-based setting compared to hash-based accumulation or folding.


But I think that this comes at the expense of relying on lattice-based assumptions versus in hash-based settings, you're just relying on hashes. So there's a slight tradeoff there.


To be fair, I don't think... I've not kept up with the lattice-based schemes too much, but at least to my knowledge, there's not been an implementation of the hash-based accumulation or folding schemes.


So the concrete efficiency, I think, is still...


[Anna Rose] (27:28 - 27:28)
Questionable. Okay.


[Pratyush Mishra] (27:27 - 27:28)
Yeah, up in the air. 


[Nico Mohnblatt] (27:28 - 28:09)
It might be a good Arkworks inspiration if anyone's looking for it.


I was actually curious with these hash-based schemes. So we also have, and a lot of the ZKVMs out there that are hash-based, do something equivalent to folding or accumulation just by doing recursive proofs. So like a proof of a proof of a proof.


And usually the promise with folding is my folding verifier is going to do less work than my SNARK verifier. So when I do this proof of a proof, or this fold of a fold, it'll be cheaper to do in a circuit.


With hash-based accumulation, I'm not sure, is it really true that the hash-based folding verifier is cheaper than the SNARK verifier?


[Pratyush Mishra] (28:09 - 29:07)
That's a good question.


So I think it depends on how you adapt or use accumulation inside your system. So today, how these ZKVMs do recursion is that really the most expensive part of the recursion is handling the part of the proof which corresponds to a proximity check or the polynomial commitment opening, if you want to take that perspective. And that kind of is what oftentimes ends up dominating, at least asymptotically, and I think concretely as well, the costs of doing this recursion.


And today, if you move to a very efficient proximity proof, like let's say WHIR, then the gap between WHIR and accumulating the polynomial or the proximity claim to the polynomial opening claims will not really net you much of an efficiency benefit.


You will go from Lambda log log n hashes or Merkle tree paths to Lambda Merkle tree paths, which is like a factor of four, which is not small, but not...


[Nico Mohnblatt] (29:07 - 29:08)
Not one.


[Pratyush Mishra] (29:08 - 30:42)
It's not one, and it's also not something which necessarily leads to you rethinking your system design and changing how you implement things. It come with other tradeoffs, which we can get into in a second.


But on the other hand if you look at things like Nova, they have this really nice Nova or HyperNova things follow-up works. They have this really nice property that all you need to do is you need to commit to the witness.


You don't need to... even if you have like very fancy custom gates, you don't need to commit to any additional vectors, which might increase your proving costs.


Once you start going down that route and designing your system from the ground up to really just incorporate those techniques, really you will only have to commit to an open one Merkle tree path inside your recursive proof.


So this kind of like leads to, I guess, maybe what we described in our papers as two benefits of accumulation of folding.


One is that your size of your recursive verifier gets smaller, and also the work that the prover has to do just to even prove your original computation, excluding the recursive part, that also gets faster. So you get both of these benefits. 


And now, if you're just trying to throw an accumulation into an existing system, you probably will not get the faster prover benefit. You will get the smaller verifier benefit. But as we discussed, that is a little bit less compelling than the faster prover benefit.


But if you design your entire system from scratch to leverage these properties, you will... or like the benefits of accumulation, then you can get concretely, I suspect, much, much larger speedups than just doing naive recursion.


[Anna Rose] (30:43 - 30:56)
The work that sort of showcased some of this research, at least the one that we noticed was Arc: Accumulation for Reed-Solomon Codes. Can you share a little bit about that and what you found?


[Pratyush Mishra] (30:57 - 31:55)
Yeah, yeah. So the starting point for that was actually a little bit prior, and it was this paper called Accumulation Without Homomorphism. We tried to first investigate accumulation for, I guess, hash-based accumulation.


And the scheme that we constructed in that did have some limitations. It was like the first scheme which did hash-based accumulation, but it had this limitation that you could only fold or accumulate up to some fixed bound. So when you set up the system, you would say, okay, I can fold, I don't know, 10 SNARKs or 10 proofs, and then I'm done.


Now I don't have any soundness guarantees even. And moreover, your efficiency would worsen. Like if you increase the bound... you could increase the bound, like from 10 to 20, but now every step you're paying like 20x as opposed to 10x.


So this was the initial work. And then so Arc is basically, I would say, really a brainchild of William Wang, who's a student of Benedikt's at NYU. And the key insight came from looking at —


[Nico Mohnblatt] (31:55 - 31:55)
Was it STIR?


[Pratyush Mishra] (31:56 - 32:43)
STIR. Yeah, which essentially said that the core problem in accumulation without homomorphism was that we were doing these kinds of spot checks to check that the folded output was consistent with the inputs, and we were incurring some kind of soundness error, which kept growing over the number of foldings that you did.


And the key insight that William had was that — I guess William and Benedikt had — was that you can actually leverage techniques from STIR to avoid that loss entirely, avoid that additional soundness error by leveraging techniques developed in STIR.


And that allowed us to get kind of what you would want out of an accumulation or folding scheme, which is unlimited number of foldings.


[Nico Mohnblatt] (32:43 - 33:03)
For those that are curious, there's also a follow-up paper called Linear Time Accumulators, which kind of does the same thing, but uses ideas from WHIR. So STIR... WHIR is an improvement on WHIR, by improving the exact ideas that helped here for accumulation.


And so the natural thing to do then was to say, all right, let's improve that technique also in the accumulation scheme.


[Pratyush Mishra] (33:04 - 34:06)
Yeah. So there are actually two works. One from Benedikt and Alessandro, which took Arc in one direction, which led to this paper WARP, which is linear-time accumulation. And then my group here, also my students and I, we also had an alternate way of constructing this linear-time accumulation scheme.


So I guess the thing that was left to do from Arc in some sense was that Arc worked with Reed-Solomon codes, which meant that you had to do things like FFTs, which lead you to have like this n log n prover time and also place constraints on the kinds of fields that you can work with in your FFT-friendly fields.


But recently people have developed these things called linear-time encodable codes, which offer o of n encoding time, as opposed to o of n log n encoding time.


So basically both WARP and this work FACS, they say, okay, let's take Arc and try to fix this or remedy this n log n prover time overhead and bring it down to linear time by working for linear-time encodable codes.


So just expanding on your comment.


[Nico Mohnblatt] (34:06 - 34:28)
Actually, I'm really happy that you mentioned FACS, because that also constructs paper FICS and FACS that almost does the link to the next topic I wanted to talk about is you also have works on proofs of proximity. So we talked about STIR and WHIR, but you have FICS.


And I guess in a way, we've been talking about this duality. Every time there's an improvement on one side, you can port it to the other. It makes sense that you would have a paper that looks at both at the same time, right?


[Pratyush Mishra] (34:29 - 35:19)
Yeah, yeah. So we can segue into talking about proofs of proximity. So I think we've vaguely mentioned proximity claims and how these are the main bottleneck in SNARKs, but just in like one sentence.


So today, when I construct these hash-based SNARKs, usually what the prover does is it encodes its messages using some error-correcting code. And then one part of the protocol is to prove that these messages actually are close to valid codewords.


Okay. So this portion is called the proximity proof.


And so I guess really what kickstarted the efficient hash-based SNARK revolution was this protocol called FRI, which you guys have talked about multiple times on the podcast and everybody really in SNARK land has heard of, I'm sure. 


But this was the first protocol which kicked things off. And then I would say that we kind of...


[Anna Rose] (35:20 - 35:29)
Yeah, we used it. It just kept getting used. There was like DEEP-FRI. There's a little bit of variation, but it was really only with WHIR or STIR.


Which one came first, Nico? Was it WHIR?


[Nico Mohnblatt] (35:29 - 35:29)
STIR.


[Anna Rose] (35:29 - 35:34)
STIR. So it was only really with STIR where we saw this like step change, I feel.


[Pratyush Mishra] (35:34 - 36:41)
Yeah. Exactly as you said, Anna, we ran with it, we applied it, but then we didn't really see too many new constructions. Then we had STIR in, I think, 2024, which gave us some glimmer that, okay, we can do better.


But then we saw a flurry of works in the small proof setting. We saw a flurry of works. There was BaseFold, and then like recently there was Blaze, which kind of showed that, okay, you don't actually have to stick to just working with these Reed-Solomon codes. You can try to do proximity proofs for... small proximity proofs for other kinds of codes. 


And in particular, Blaze does this for at least linear-time encodable codes, which as we talked about they only require linear time to encode as opposed to quasi-linear or n log n time for Reed-Solomon codes.


But essentially Blaze showed that, okay, you can get for particular kinds of fields, IOPU, Interactive Oracle Proof of Proximity, or IOPP, where after you apply your Fiat-Shamir and Merkle trees and all of these things, you can get something with the proof size, I think it's Lambda log squared n hashes, something like that.


[Nico Mohnblatt] (36:41 - 36:46)
Everyone at home is following and there are some topics in mind and they know exactly why this is good.


[Pratyush Mishra] (36:48 - 37:32)
So Lambda is a security parameter, but yeah, so basically it's scaled with log squared n.


But if you look at the Reed-Solomon side, what STIR and WHIR were able to do was to bring that down to log n log log n times lambda hashes.


So basically reducing it by this factor, which led to a concrete improvement of like 2 to 3x, which is important when you want to put these proofs, hopefully on the blockchain.


So essentially what we did in FICS was try to see, can we port these same benefits over to these linear time and codable code settings? And so what we took was this core idea, which had appeared in Blaze and maybe a couple of predecessor theoretical works, which is code switching.


And we can talk about that in a little bit, if there's not other questions that you want to go over first.


[Nico Mohnblatt] (37:32 - 37:46)
I mean, I definitely want to keep some time for the non-blockchain applications because we did tout those early in the show. But curious about maybe a few words on how FICS applies code switching. Does FACS do it too?


[Pratyush Mishra] (37:47 - 38:48)
Yes. So both FICS and FACS do code switching.


So essentially the summary is that Reed-Solomon codes, they have this really nice structure, which allows you to do things like folding in FRI very efficiently. But linear-time encodable codes, they generally don't have this kind of nice structure.


So code switching is basically this technique which says, okay, we don't have the structure. What we're just going to do is kind of just prove that our codewords are actually close to the code. We're going to design specialised proofs for this task.


So Blaze does that in one way. And in FICS, we take an alternate approach.


We design new code switching techniques and also show that existing code switching techniques can all be put in like one framework, which allows you to do code switching for a large class of linear-time encodable codes. And kind of brings over these proof size benefits that you got with STIR and WHIR also now to these linear-time encodable codes.


And then it's kind of culminated in this very recent work, which went up on ePrint, which allowed us to kind of even get rid of the log log n factor and get proof size of like Lambda log n hashes.


[Anna Rose] (38:48 - 38:49)
What was that work?


[Pratyush Mishra] (38:49 - 38:53)
So this is a paper called Query-Optimal IOPPs.


[Anna Rose] (38:53 - 38:58)
Ah, this is the IOPPs. Okay. Interactive Oracle Proofs of Proximity.


[Pratyush Mishra] (38:59 - 39:13)
Yes. So we actually show that kind of this is the best kind of proof size that you can get for these proofs of proximity, which is why we call it "optimal". But it's mostly a theoretical work for now.


I don't think the ideas would lead to concretely better proof sizes.


[Anna Rose] (39:14 - 39:17)
What does FICS and FACS stand for? They look like acronyms.


[Pratyush Mishra] (39:18 - 39:30)
Yeah. Great question. So we kind of took inspiration from the FRI name. So FRI is like Fast Reed-Solomon IOPP.


So we just were like Fast IOPP via code switching. And that became FICS.


[Anna Rose] (39:31 - 39:36)
Okay. So that's what it is. And then the other one would be Fast A-?


[Pratyush Mishra] (39:36 - 39:36)
Accumulation via code switching.


[Anna Rose] (39:37 - 39:38)
Accumulation. Okay.


[Pratyush Mishra] (39:39 - 39:40)
It's not too creative, I would say.


[Anna Rose] (39:42 - 39:43)
It looks cool in short form.


[Pratyush Mishra] (39:44 - 39:48)
Yeah, it's like sticks and stones or like FICS and FACS. That's kind of what I was going for.


[Anna Rose] (39:48 - 39:48)
Nice.


[Nico Mohnblatt] (39:49 - 39:53)
Very briefly, you talked about code switching, but why do we want to switch codes?


[Pratyush Mishra] (39:53 - 40:34)
So the idea is that I can go from a claim about basically like this codeword is close to code one. I can reduce that to a claim about this other code word is close to a claim in code two. And now code two can be either a smaller size code, or it could be something like a Reed-Solomon code, for which we already have existing IOPPs.


So by being able to switch claims from code one to code two, we're able to kind of make progress either by reducing the size or being able to invoke tools that we already do.


So FICS kind of does both. We use code switching to reduce size. And then once it's small enough, we directly invoke something like a STIR or WHIR.


[Nico Mohnblatt] (40:34 - 40:40)
I see. And so in that case, we don't care that we're doing Reed-Solomon codes because the problem is already smaller than the original one.


[Pratyush Mishra] (40:41 - 40:50)
Yeah, we invoke it like enough, we invoke the size reducing code switching to get us down to like n over log n size. And then at that point, Reed-Solomon codes are linear time.


[Nico Mohnblatt] (40:50 - 41:04)
So wrapping up on our topics of making better SNARKs, the last one that I listed or mentioned earlier was how do we get provers for SNARKs in an environment where you have restricted memory?


[Anna Rose] (41:04 - 41:05)
What's an example of that?


[Nico Mohnblatt] (41:04 - 41:10)
So I mentioned this because you have a few papers. Exactly. What's an example of that setting?


[Pratyush Mishra] (41:11 - 42:03)
Yeah. So I think that's a very exciting area that people have been starting to focus on recently. But it's really motivated by applications where you want to produce proofs on very constrained devices, like let's say your web browser or your phone, or maybe even more restricted than that.


Maybe to the extreme, you can take it like on a YubiKey. Can you do proving on a YubiKey? That would be really cool. We're not there yet.


But yeah, and generally kind of this low-memory proving is focused on trying to do proving when your prover has only a small amount of memory, like trying to prove maybe large computations in that small amount of memory.


I think it's become interesting recently because for a long time we were focused on improving prover time, but now we have made good progress on that. So now we kind of end up being bottlenecked by the amount of memory on a single machine or device.


[Nico Mohnblatt] (42:03 - 42:17)
Right. So we've had Muthu on the show from Ligero, who very proudly advertises that Ligero doesn't need much memory to run the prover. So I was wondering, is it a solved problem, or do we want to go smaller and smaller and smaller?


[Pratyush Mishra] (42:18 - 43:08)
Yeah, so Muthu's work, I think, is Ligetron was the underlying proof system. So they do like a very clever system design and proof system co-design, which allows them to get this kind of prover, which is called a streaming prover, which we can talk a little bit more about later. But I think at least as far as I understand the downside of that approach is that you end up with a linear-time verifier.


And at least for Ligero, and I'm not sure if this is inherent to this approach, it might be, at least for Ligero, and this also gives you like a square root proof size, which might be fine for a lot of applications concretely, but it does mean that the verifier is linear time in the original size of the computation.


I think once you start caring about smaller proof sizes and very succinct verifiers, I think it's still an open question, which given a lot of groups have been trying to make progress on, including my students and I.


[Anna Rose] (43:09 - 43:14)
What are some of the breakthroughs that have come around to allow for this low-memory SNARK construction?


[Pratyush Mishra] (43:15 - 43:21)
Yeah. So I think, I mean, in some sense we did already know one way to do this very efficiently, which is via recursion.


[Anna Rose] (43:21 - 43:21)
Okay.


[Pratyush Mishra] (43:21 - 43:28)
If you do recursion or folding, then you only ever pay for the size of one step as opposed to the entire computation.


[Anna Rose] (43:28 - 43:32)
Yeah. I think Mina back in the day was always kind of... it's very small.


[Pratyush Mishra] (43:33 - 43:52)
Yeah. And then that gets you small proof sizes. The downside is that with recursion and folding, in general, we don't have a very good handle on the security in some sense, like our proofs of security or soundness are only for a constant number of steps, which is not how we use these things in the real world.


[Nico Mohnblatt] (43:52 - 44:02)
Yeah. This is also something we mentioned, by the way, on our recent episode with Binyi. So for those interested in more details about these security properties, I really recommend that episode.


[Anna Rose] (44:02 - 44:03)
Yeah. We'll link to that.


[Pratyush Mishra] (44:04 - 44:17)
Yeah, yeah. And it's kind of also become more of a concern, but this attack on GKR-based proof systems, because you need to use hashes inside the circuit. But anyway, so those things work. They do have certain, both theoretical limitations.


[Anna Rose] (44:17 - 44:18)
And security ones.


[Pratyush Mishra] (44:18 - 45:39)
And security ones. I think it generally is also interesting to see if you don't want to go for recursion, what is the best that you can do? Are there some barriers that you run into?


So I guess in that vein, what we've had to do, I guess my students and I, is to construct kind of these monolithic SNARKs. We're just trying to prove a full circuit instead of breaking it up into chunks and proving each chunk separately.


And there we had two approaches. The first approach is this paper called SCRIBE. And the idea there was that, okay, even on many devices, which do have limited amount of RAM or memory, what they do often have is a large amount of disc space. Gigabytes, even in your entry level iPhone.


So what we did was design the prover where instead of storing all of the prover state in memory, we were able to store it on disc, and we made sure to design protocol in such a way that accessing that memory was efficient.


So normally accessing the disc was efficient. So normally accessing disc is way slower than accessing memory, like 10x at least, but the algorithms that we designed were able to kind of completely mask the overhead.


And in particular, we designed this version of HyperPlonk in this model where the prover state is on disc, but the overhead compared to in-memory proving was like 10%.


[Nico Mohnblatt] (45:39 - 45:43)
Is this what you described earlier as a streaming prover?


[Pratyush Mishra] (45:43 - 44:44)
Yeah. So streaming prover is...


[Anna Rose] (45:44 - 45:44)
Cool name.


[Pratyush Mishra] (45:44 - 46:30)
Yeah. It's actually an entire area of research in theoretical computer science and streaming algorithms. But a streaming prover is a further restriction of this, which just says that this prover is only allowed to have some small amount of memory and it can access the witness and the circuit description via stream.


So sequentially, it can read the first bit of the witness, and then it has to throw it away and read the next bit and so on and so forth. This is a very restrictive model.


What we said was, okay, you don't have to throw it away. You can just write back that bit of witness or whatever you processed to disc so that if you need it again in the future, you can read it again.


I guess streaming provers, they actually are slower than the memory-intensive ones, but with our construction, we were able to kind of both asymptotically and concretely bring the overhead down to just 10%.


[Nico Mohnblatt] (46:30 - 46:33)
By the way, is that why you called it SCRIBE? Because it's streaming and it's also allowed to write?


[Pratyush Mishra] (46:35 - 46:38)
Yes, it's SCRIBE because it writes down.


[Nico Mohnblatt] (46:38 - 46:38)
Nice.


[Anna Rose] (46:39 - 46:40)
What's the second approach?


[Pratyush Mishra] (46:40 - 48:22)
So the second approach actually is to stick to streaming. And this was this other work called the Time-Space Trade-Offs for Sumcheck. So the sumcheck protocol is kind of now the workhorse of all efficient SNARKs, essentially.


But the algorithms that we use for it, they kind of fall into two categories. Either you use a very small space, but then you incur like n log n proving time, which concretely translates to a 30x overhead for the sumcheck. Or you use like a linear amount of space and you take linear time, which is the best you could hope for.


So we were trying to investigate what if you have a middle ground, like you use not a tiny amount of space, like a medium amount of space. Can you do better?


And so the predecessor, maybe the inspiration for this was this paper called Blendy from Alessandro's group at EPFL. Then my students and I kind of picked that up and we reached out to them and we were able to say, okay, are the results in Blendy the best you can do? They were also like only for a particular class of sumcheck, which is not actually how we use it in protocols.


So we were able to generalise that to actually how we use sumcheck in protocols and showed that you can do better than kind of the two extremes that I mentioned earlier. You can get something which is almost as fast as the high memory version. I think in practise only 2x slower, but using, I don't know, like 100x less memory for large sizes.


And we also, I think the cool part is we were able to show that kind of this is the best that you can do. If you want to use a particular amount of memory, you must incur, let's say, this 2x overhead. You can't do better than that, which I think is kind of a rare occurrence in SNARKs. We don't usually have these kinds of lower bounds.


[Anna Rose] (48:22 - 48:33)
So these two approaches, both low-memory SNARK constructions, are either of them similar to Ligero or to other systems that we've maybe talked about on the show?


[Pratyush Mishra] (48:33 - 49:28)
I would say you can take ideas from Ligero and apply them here and maybe hope to get a better system overall. Maybe some limitations of Ligero can be lifted by using, for example, our time-space trade-off, the new sumcheck algorithm.


Actually, concretely, there are some barriers to what you can do with Ligero-style systems where you have these hash-based streaming SNARKs.


I'll paraphrase. You must either have not small proof size, like not polylogarithmic, like tiny proof size, FRI-style proof size. You can either have that or you can have streaming provers. You can't have both at the same time.


So if you take this kind of SCRIBE model, then you can actually do both. Or we think that you can do both. This is some follow-up work that we're working on, and we're hoping to show that you can actually get both very small proofs and low-memory provers.


So I think that's an interesting direction for people to investigate in the future.


[Anna Rose] (49:28 - 49:59)
So I think we've covered the work to date in some of these themes that, Nico, you first presented and, Pratyush, you brought up.


There is another category that you were talking about at the beginning of the episode, which is like non-blockchain. The sort of move away from the problem space that we mostly talk about on this show.


What are the kinds of things you're doing over there? And I'm assuming there's also some collaborations happening there. Maybe you can share a few of those initiatives.


[Pratyush Mishra] (50:00 - 50:41)
Yeah. Yeah. So I guess like all of the things that we talked about so far are very much on the SNARK construction side.


On the other side, on the application front, I've been looking at with some collaborators at maybe non-strictly blockchain applications. I think they have applications to blockchain use cases as well.


But one direction has been, this is in collaboration with Sebastian, who's a professor at Penn, on proving that if you have some kind of committed document that it actually satisfies some kind of structure. So for example, it's a valid JSON document with a particular entry or it's a valid C-file or something along those lines.


Anything which can be captured by a context-free grammar.


[Anna Rose] (50:41 - 50:49)
Is it like proving a certain language or is it saying this is written in XYZ or is it more general than that?


[Pratyush Mishra] (50:49 - 51:41)
Yeah. So in general, it's proving that it matches the specification or format of a particular document format. So it looks like a C-file or looks like a JSON file. And applications for this we discussed a few in the paper, some of them kind of relate back to very blockchain-adjacent applications.


So lik zkTLS. Right now, when you get a response from the server, you just assume that it's a properly formatted response, and the email field in the response would always be at the same location.


But if the server who does not know anything about your application, changes the response format even a tiny bit, suddenly now maybe all your guarantees about your zkTLS application go out the window.


Similarly for things like zkLogin or Aptos Keyless, which rely on particular formatting of the JWT tokens.


[Anna Rose] (51:41 - 51:46)
Yeah, we can add some links to that. We covered that on the show with a couple of those teams.


[Pratyush Mishra] (51:46 - 52:00)
And so where Coral would come in here would be that you actually can get a proof that now this response from the server or whatever actually matches this exact specification that you wanted. So you can get like a stronger guarantee that way.


[Nico Mohnblatt] (52:00 - 52:04)
Does it also prove statements about the contents of the file or just the formatting?


[Pratyush Mishra] (52:04 - 52:15)
Yeah. So you can prove that it contains, I don't know, nested two levels deep an email field, which has, I don't know, pratyush@penn.com or whatever or penn.edu has the email there.


[Anna Rose] (52:15 - 52:26)
That's cool. The title of this work, Coral: Fast Succinct Non-Interactive Zero-Knowledge CFG Proofs, is CFG, that's context-free grammar. Is that what that stands for?


[Pratyush Mishra] (52:26 - 52:39)
Yes. It's context-free grammar. And it's just like a follow-up work, I guess, that Sebastian's group has done previously on RegEx, or proofs of RegEx matching, which people have also used in different blockchain-adjacent application contexts.


[Nico Mohnblatt] (52:40 - 52:50)
I was interested to see authors from Brave as well on that paper, so the browser company. Are they looking into these sort of zkTLS proofs? Is that why they're here?


[Pratyush Mishra] (52:50 - 53:04)
Yes, I think Sofia, who's the co-author from Brave, she and her collaborators have been working on some zkTLS work. And so she made that connection and that link between Coral and this limitation of existing zkTLS protocols.


[Anna Rose] (53:05 - 53:25)
That sounds like that would be really fun to explore with them, actually. This is a team with users who are now... it's like the browser producing the zkTLS, unlike most of the teams that we've talked to, which are zkTLS teams, kind of approaching browsers potentially as customers.


Yeah. That'd be really cool.


[Nico Mohnblatt] (53:25 - 53:32)
So you mentioned this is an extension of previous work from Sebastian's group. And I now remember there's a paper called Reef. Is that the one that's being extended?


[Pratyush Mishra] (53:32 - 53:36)
Yes. That's why we called it Coral, because it generalises Reef.


[Nico Mohnblatt] (53:36 - 53:41)
And so it uses Nova, right? Like deep inside it.


[Pratyush Mishra] (53:41 - 53:45)
Yeah, yeah. So yeah, it uses our modifications of Nova.


[Nico Mohnblatt] (53:46 - 53:41)
I'm curious, what other non-blockchain applications have you been looking at or are working on?


[Pratyush Mishra] (53:53 - 55:19)
Yeah. So I think one exciting one, just slightly different from Coral, is we've been working on this proof for verifiable database queries, or this proof system for verifiable database queries.


And the idea there is that you have a database owner who provides a commitment to the database which contains whatever you would have, I guess, in a standard database. And then you could have client and server who interact.


And whenever the client wants to make a query into the database, the server can now provide a guarantee that, hey, this is the result of the database. And moreover, here's a proof which shows that this result is correct. And I did not cheat in answering your query.


And this is verified with respect to that commitment to the database that was provided originally. So it's like the high-level application.


And there's been a few works on this previously. The first one, I think, was this verifiable SQL back in 2017 or so from Yupeng Zhang, who's at UIUC now. But I think the line of works, they incurred relatively high overheads even for, I would say, not too complex queries and for not too large databases.


So we have this kind of ongoing work, which tries to reduce that overhead a lot. And I think we're able to get sub-minute, I don't know, proofs for complex queries on, I would say, fairly large databases.


[Nico Mohnblatt] (55:20 - 55:36)
Interesting. Yeah. I remember seeing a paper by Matteo Campanelli on that topic as well.


And so is this what you were saying, was sort of advocating for, in some contexts, it might be better to design our proof systems with the application in mind rather than using the SNARKs that we already know and love?


[Pratyush Mishra] (55:36 - 56:07)
Yeah, yeah. So that's exactly where that comment was coming from. Yeah, yeah.


So because what we do for this project is actually we design very specialised PIOPs, essentially, for the core SQL operators, like joins and selections and group bys and all of these things. And because of that, and by not going via the circuit approach, I think we're able to get orders of magnitude speedups compared to a circuit-based approach and throwing a state-of-the-art SNARK at it.


[Anna Rose] (56:08 - 56:31)
Just on the topic of where this might be used, and I don't know if this is the correct link, but a few episodes ago, we did an episode on Turnkey, which was a TEE solution. They talked about not trusting the database. And I wonder if something like this could be helpful in their situation.


I don't know if you've... I might be making a connection that isn't there, but I don't know if you've heard anything about that.


[Pratyush Mishra] (56:32 - 57:10)
I did not listen to that podcast episode yet. It's on the backlog.


I think in general, there's lots of applications where this would be helpful. There is one blockchain application, which I think is being explored by some industry folks.


So I think the Lagrange Labs, this was like what they had started off trying to do, basically trying to provide... you view the historical blockchain state as a database. And then, because smart contracts, it's very expensive to give them access to this historical state, you instead just give them proof that here's a state and here's a proof that the state is valid with respect to the, I don't know, commitment to the blockchain, to the blocks or whatever.


[Nico Mohnblatt] (57:10 - 57:14)
So this is the whole coprocessor narrative from a while back, right?


[Pratyush Mishra] (57:15 - 57:23)
Yeah, kind of, yeah.


So that's one application that I think Space... there's a company called SpaceTimeLabs or something like that.


[Anna Rose] (57:23 - 57:25)
Yeah. That's been around for a while.


[Pratyush Mishra] (57:25 - 57:27)
Yeah, so they've been looking at this application.


[Nico Mohnblatt] (57:27 - 57:35)
And there's a third company called Provably. So this Matteo paper that I mentioned, I think, is with co-authors from Provably and that's also what they do.


[Pratyush Mishra] (57:36 - 58:16)
I see, interesting.


Yeah, so I think that's a very interesting application because you do actually have very constrained verifiers.


Another application that we've been thinking about for this system is basically to give you a proof of deletion. So in a non-blockchain context, like under GDPR, if I make a request that you delete my data, right now you have to take the company's word for it and auditors have to do a really expensive job of checking this or they just do some spot checks.


But now you can have the company give you a proof that here is my database and here's the updated commitment, which I'm going to publish for everyone to see. And now I can prove to you that your entry is not in my database anymore.


[Anna Rose] (58:16 - 58:24)
That's so cool. I mean, this sort of makes me think of the zkID cases where they talk about removals or what is it?


[Nico Mohnblatt] (58:24 - 58:25)
Revocations.


[Anna Rose] (58:25 - 58:33)
Revocations, exactly. But this seems even more general purpose. What you're describing could be used for so many things.


Yeah. That's so cool.


[Pratyush Mishra] (58:33 - 58:35)
Yeah. We're very excited about the project.


[Anna Rose] (58:35 - 58:35)
Nice.


[Pratyush Mishra] (58:36 - 59:20)
So I think in general, kind of these non-blockchain applications, they motivate both designing SNARKs for applications and also kind of the low-memory proving because oftentimes now the focus or the proof generation, I guess, device is not a beefy server anymore, but is a client phone or browser or whatever.


So I think I'm excited for progress in that direction. We're actually investigating everything on all of the topics that I mentioned. My students are keeping me very busy.


Essentially, all of the projects that I mentioned are the hard work of my students in my group and my collaborators. And I'm mostly there as like a figurehead.


[Anna Rose] (59:23 - 59:41)
I wanted to ask you, this is sort of a last question about your move to professor, to assistant professor. Are there ZK-focused classes happening in the cryptography departments at this point? Or is it like a chapter in a larger class? I'm just curious how it's sort of spreading into education.


[Pratyush Mishra] (59:42 - 1:00:00)
Yeah. So it's actually both. So I teach cryptography, which is the broad course. And in that I have a portion at the end where I teach ZK.


Actually, the last time I taught it, I taught like a very simplified version of the proof underlying Nova. I don't know how many of my students understood what I was saying.


[Anna Rose] (1:00:01 - 1:00:03)
It's like a cutting-edge one to start with.


[Pratyush Mishra] (1:00:05 - 1:00:24)
Yeah. Actually, it's simple enough that you can explain it, but it's actually good because they can feel like they're at the cutting edge of research. And I also teach in like the other semester, kind of a proof system-oriented class, which actually, we just finished up one of the lectures before we were recording this.


[Anna Rose] (1:00:24 - 1:00:25)
Cool.


[Pratyush Mishra] (1:00:25 - 1:00:42)
But that one talks about both the applications and design of SNARKs. So that's more orientated towards PhD students. But I know if at other places, folks are also investigating more like a mix, like where they talk about other crypto as well, and that's orientated towards undergrads.


[Anna Rose] (1:00:42 - 1:01:13)
Dan Boneh told me last year, I think that he was doing a ZK course for the first time. And I think he's doing it again, like next semester. I just think that's quite amazing.


You know, I think at first it was a footnote and maybe a cryptocurrency class or something. Then it's a chapter and maybe a lecture. And now it might be a class for some people. I think that's so exciting.


In your courses, in the work you're doing, are you also touching on any of these more advanced cryptography topics like FHE, MPC, or iO?


[Pratyush Mishra] (1:01:13 - 1:01:21)
Not in my course, no, but I know other folks who do. Even in the applied courses, they talk about FHE and multiparty computation.


[Anna Rose] (1:01:22 - 1:01:41)
Very cool. Pratyush, I want to say thanks again for coming back, sharing with us all of the work you've been doing since 2021, and it's a lot. A lot of different directions.


So it was really fun to sort of go on that journey. And it does sound like... I mean, there's overlap between them too. There's clearly like through lines.


So thanks so much for sharing that with us.


[Pratyush Mishra] (1:01:41 - 1:01:46)
Yeah. Thank you so much for having me. It was a wonderful time.


[Nico Mohnblatt] (01:01:44 - 01:01:46)
Thank you both. It was great chatting.


[Anna Rose] (1:01:46 - 1:01:55)
Cool. And I want to say thank you to the podcast team, Rachel, Henrik, Tanya, and Hector.


And to our listeners, thanks for listening.