Anna Rose [00:05] Welcome to Zero Knowledge. I'm your host Anna Rose. In this podcast, we will be exploring the latest in zero-knowledge research and the decentralized web as well as new paradigms that promise to change the way we interact and transact online.


This week, I chat with Norbert, head of product at ZkCloud, which was previously Gevulot. We dive back into the topic of prover networks, mapping out the different actors in the ZK proving supply chain. We discuss the different ways in which teams building prover marketplaces build the economics of these systems, and the tradeoffs each of these models offer. We then discussed the ZkCloud system, and what he thinks large scale industrial grade proving will look like in the future.


Now before we kick off, I do want to just let you know about the zkSummit. zkSummit13 is coming up. It's happening on May 12th in Toronto. If you've never been to a zkSummit, I definitely recommend checking it out. This is the spot to find out about the latest research, the newest applications, to find out who are the most important players in ZK today. It's also a wonderful way to get to know the larger ZK community. The application to attend is now open. Links as always are in the show notes, and I hope to see you there.


Now Tanya will share a little bit about this week's sponsor.


Tanya [01:33] ZK is finally easy with Noir, the fastest growing zero-knowledge programming language. Build privacy-preserving apps without any ZK experience. Apps today all look alike, especially across chains. Noir gives you simple privacy tools to build something new. Noir is Rust-based, backend agnostic, and already used by leading ecosystems like Starkware, World and Aptos.


Aztec Labs is running a 4-week program, NoirHack. With 200k in funding and grants, you'll get hands-on support from a dedicated team of 10 DevRels, weekly deep dives, partner workshops, and a demo day where you'll pitch to judges from Base, a16z crypto, IOSG Ventures, and Paradigm. Applications close soon. Sign up now at noirhack.com. That's, noirhack.com, N-O-I-R-H-A-C-K.com.


And now here's our episode.


Anna Rose [02:26] Today, I'm here with Norbert, head of product at ZkCloud, which was previously called Gevulot. Welcome to the show Norbert.


Norbert Vadas [02:35] Hi, Anna. Thank you very much for having me.


Anna Rose [02:37] I'm excited to have you on. So, we've actually been in touch quite often because ZKV, my other project, has been working with ZkCloud. ZKV is right now an operator, both a validator and prover on the permissioned early network Firestarter and ZkCloud.


Cohen from my team, I know speaks to you quite regularly, but he is the one doing most of the work. I think you'll hear from this interview, I still have a lot to learn. I'm so glad that you're on the show to share with us a little bit like the state of prover networks, how they've evolved over time, what you guys are doing specifically. Yeah, I'm looking forward to it.


Norbert Vadas [03:14] Thanks a lot. Actually, we are very happy to be working with you guys. Yes. I've mostly been in touch with Cohen because as you mentioned, you are one of the prover node operators on our scalable permission network Firestarter. But then, as we get closer to the mainnet launch, and we started validator onboarding, I think ZKV is also participating there. So super happy to be working with you guys in different fronts.


Anna Rose [03:41] Yeah, very cool. Norbert, I think you and I met actually in Kraków. You had joined, on behalf of what was at the time the Gevulot team, as a sponsor. We did the ZK Hack Kraków event. And I just really remember this T-shirt that you printed which had this -- basically, Gaylord from the ZK Hack team had done a bunch of stickers, and one of them had this dragon, and you managed to grab it and print it. And it's quite a stunning shirt. I've seen it a few times around since then.


Norbert Vadas [04:14] Yeah. I mean, people have been coming up to me and asking me about that shirt, whether we do have a couple of samples that could be distributed, because it's been really popular. And I also hear -- funny thing, some of those shirts are still walking around in Kraków on random people, because the local people love it so much.


Anna Rose [04:36] It's fashion.


Norbert Vadas [04:36] I don't know how it landed there. Yeah. But it's a funny story.


Anna Rose [04:40] Nice. So what I want to do with today's episode is really look at prover networks. I think we have covered this previously on the show. And so, we'll dig up any old episodes where we kind of introduce the concept. But I think it's really exciting to have you on to do a bit of a snapshot of where we're at today with prover networks.


We hear a lot about a lot of teams that are launching prover networks, or partnering with prover networks. And I think trying to understand what they are, how they've evolved, how different teams are kind of interacting with them will be really helpful.


So let's start by going way back in time to the beginning of prover networks, a.k.a, 3 years ago? 4 years ago? I don't think it's that long ago actually.


Norbert Vadas [05:24] Somewhere around there.


Anna Rose [05:26] Yeah. 2021 possibly. So I want to talk to you a bit about what were those early prover networks or prover marketplaces as they were called back then.


Norbert Vadas [05:35] Yeah. So to be honest with you, I was just coming to the blockchain space back then, so I may have a little smaller grasp on that era of proving, basically. But I do know that, basically, the first ones were Mina's Marketplace, which they used to call Snarketplace.


Anna Rose [05:54] Snarketplace, yes?


Norbert Vadas [05:55] Maybe even today. And then, I think, actually =nil; was also starting off creating a prove market which then -- and then they pivoted to building an L2-based on zkSharding. But that's a different story.


Regarding the Snarketplace, what I remember having heard and having read is that -- so it was mainly generating proofs for the Mina blockchain. And what was very interesting and very eye opening also for us particularly is that some entities started offering proofs in that marketplace for free. And it just basically killed the competition, killed economics, basically it killed the marketplace. And obviously there are ways to come around that.


However, it was a really important moment where, and I think architecturally, it's a very important topic how you want to distribute proving, because auctions have their advantages, but also as we've seen with the Snarketplace, they are gameable and there can be situations where large entities could easily gain monopoly. On the other hand, other approaches may have their own benefits and advantages and happy to dive deeper on that later on.


Anna Rose [07:18] Cool. Yeah. ZKV actually also ran, I guess it was a prover on the Snarketplace.


Norbert Vadas [07:24] Nice.


Anna Rose [07:25] But I remember -- like we were participating early on, but I think eventually, like you just described, first, I think there wasn't enough demand for proofs, because it was just for the blockchain itself, not for applications living on the blockchain yet. Later, they released the language, and then you could create applications, and then you needed a lot more proof, so it started to make more sense. But back then it was just the recursive proof that kind of powers the Mina blockchain being created, and it just wasn't enough demand. And then like you said, like someone -- if people came in with zero fees, zero cost, and it sort of tanks the marketplace.


I would mention one other team that I believe -- that I think has a prover network of some kind, which is Aleo because they're running hardware provers as part of their offering as well. But as far as I know they're only proving the Aleo Network. I think it was last year where we started to hear -- maybe the year before actually -- where we started to hear this sort of story of prover networks, prover marketplaces coming back where teams were talking also about decentralized prover networks, where you'd have multiple provers potentially proving multiple L2s, or anyone who would need these sort of proof requesters, people who needed ZKPs.


And so it stopped being sort of this vertical integration, where it was often a blockchain plus their own prover network, and more like this prover network, like true network as a service, proving to multiple parties. Tell me a little bit about -- because, I think, Gevulot/ZkCloud comes from that era.


Norbert Vadas [09:07] Exactly.


Anna Rose [09:08] So tell me a little bit about what the thinking was there.


Norbert Vadas [09:10] That's a very interesting era. I think it was in summer 2023, I was in Paris, EthCC, and actually, it was Toghrul Maharramov talking about the different ways to approach proving for example for an L2. So currently, it's a well-known fact that basically the largest proof demand is coming from ZK L2s, validity rollups mainly.


And there's been quite some talk about whether they should build an enshrined prover network, meaning an in-protocol prover network, and bring that network entity beside the sequencer into the protocol, bake it in and progress that way, or go with a third party proof provider who could optimize for proving. But then you know --  the thing is that the go-to-market is much easier with a third party network, because you don't need to build everything yourself.


On the other hand, it's very important to ensure that whoever you outsource your proving to does not introduce unnecessary or unhealthy dependencies. Or maybe even, you build a nicely decentralized validator set, but then you have a centralized prover who is introducing tradeoffs to your network that you've built. I mean, this is not the situation with the current L2s because most of them are running in a centralized fashion and exploring --


Anna Rose [10:35] They're running their own. Right?


Norbert Vadas [10:37] Yeah. And they do sequencing, except for Taiko obviously, because that's a based rollup. But most of the L2s are doing both sequencing and proving for themselves. There are some that are already actively exploring decentralization of sequencing governance proving, et cetera, et cetera. And also some others, like Aztec, for example, who want to go decentralized from day one. So both on their testnet and on their mainnet. And I think also ZKV has been involved in sequencing and proving of the Aztec testnet. Yep.


Anna Rose [11:18] That phenomenon of rollups, I mean, it's true now nowadays there are these rollups that are hopefully going to launch with that decentralized. But when they first launched, all of them, or most of them were quite centralized. So centralized sequencer, centralized prover. I always wondered, where do those actually happen? Like where -- you talk about it being centralized, but is it like AWS? Is it like hardware? Do they have distributed provers at least or distributed sequencers, but all controlled by them? Do you know this?


Norbert Vadas [11:49] I mean, I do have some information about different teams. Most of them were basically renting large clusters from Google or Amazon.


Anna Rose [11:59] Okay.


Norbert Vadas [12:00] So it was a very kind of one of the approaches dominant there. And then, I do know of some teams who had private engagements with GPU farms or data center operators, and renting compute from a couple of different places.


But what's very important, I think, and that's already leading or pointing towards the dynamics changing, is that when you rent your own machines, and you run your own proving, you need to rent machines to cover your redundancy or else you will have an extended and prolonged finality in times of transaction peaks, et cetera, et cetera. So it's very interesting to see the different approaches, but it's also very interesting to see that, basically, over time, the industry started leaning toward third party proving, and not anymore thinking about building their own in-protocol prover networks.


And then, third party proving, obviously, has a couple of advantages that are worth mentioning. It's the paper proof model versus you rent your servers and they are sitting idle if there is no proof to be generated or if transaction volume are low. That's burning a lot of runway for the different teams. So basically, the paper proof models that are available through prover marketplaces or prover networks, or even some centralized proof providers out there, they come in very handy.


What I would add on top is that, and this may even be a larger chunk of the cost, but saving the idle time for all these teams may actually be a larger saving than just making proving the cost per proof cheaper. So that's an interesting dynamic there.


Anna Rose [13:54] Interesting. I feel like we should name some of the actors, sort of the roles that exist in this. I'm going to call it supply chain or chain of actors that are creating proofs. We can use maybe the L2 as an example. So we've mentioned this, like sequencer and prover.


In this case, the way that I think this is often described in the prover network context is the L2 is a proof requester, it needs proofs. In the case that we just described, where that L2 is running its own provers, it is also the proof provider or the prover.


Prover networks though kind of live in between that. Right? So you have proof requesters, could be an L2, could be something else, and then you have this prover network which can create proofs for the proof requester. But they're often, as I understand it, almost bringing in these prover machines from different places in order to satisfy. Is that how you would define a prover network?


Norbert Vadas [14:56] Yes. I think that's more or less what a prover network is. Like where they source the compute capacity from, that may differ though pretty significantly, because in that sense, I'd rather maybe call the bucket proof providers or like proving services.


Anna Rose [15:15] Okay.


Norbert Vadas [15:15] And as you said, so there is the proof requester, there is a proving service which needs to source compute from somewhere, and then the supply chain progresses with the verifier and the settlement of the proof itself.


But regarding prover networks and marketplaces, I mean, the marketplace itself already hints at anyone who has hardware, either through renting or having their own clusters on-prem. They could come in, they could submit bids in the marketplace to win the ride for different proving jobs.


But then, in a prover network, that's a little bit different, I think. And it's particularly because the prover networks may have different workload allocation mechanisms. And what I mean by that is a prover network can be decentralized and permissionless, allowing anyone to come in and contribute compute, and then still use an auction mechanism to win proving jobs, and to distribute the different jobs to certain providers, certain node operators in the network. 


However, with for example, Gevulot and ZkCloud, we actually build our network in a way that we randomly assign proving workloads to prover node operators. So in that sense, it's a permissionless network that we are building. Anyone can join who meets the minimum hardware requirements, and then the randomness based selection ensures neutrality, minimizes centralization risks, et cetera, et cetera.


What's there and what's important, I think, for all these L2s, even though most of them are running in a centralized fashion now, and exploring the way to decentralize both sequencing and the proving. On the other hand, it's going to be a very important decision, I'd say, what kind of proof provider they select, because the architectures of those providers may come with significant tradeoffs in certain cases.


Anna Rose [17:22] The proof provider, the way you've defined it, this could be a prover network, could be also a direct prover. Proof provider is anyone who's creating proofs, or proofs are being generated somehow by this entity.


Norbert Vadas [17:34] Yes.


Anna Rose [17:34] But when we go to the prover network, what do you call the provers that feed just the prover network?


Norbert Vadas [17:42] I usually call them prover node operators.


Anna Rose [17:45] Okay.


Norbert Vadas [17:46] They are the ones with the actual hardware plugging into the prover network and contributing the compute.


Anna Rose [17:55] Got it. The prover node operators, like I have heard -- and I think maybe we're far off from this, but I've heard the proposition that one day maybe anyone can be a prover node operator. You could prove from your phone, you could prove from your laptop. It seems like, and also because we're in it, we're experimenting with this as well. So right now, it seems like we're pretty far off from that. The prover node operators tend to need to actually have pretty significant hardware. And like you're not running this on sort of a random laptop at this point.


Norbert Vadas [18:30] Yeah. So I think that there are different approaches, and it's absolutely possible that there will be certain teams who will take the approach and allow any type of consumer grade hardware, even browser-based proving, or mobile phones to come in and contribute compute.


The approach, for example, we've taken with ZkCloud, is that we have to have a certain minimum hardware requirement to be met because that's part of our optimization strategy from different angles. Since we use randomness to distribute workloads in our network, we need to ensure that everybody has a uniform minimum capacity.


If you use, let's say a consumer grade laptop, and I have a huge cluster of -- we cannot be in the same random selection in that sense.


Anna Rose [19:24] Totally.


Norbert Vadas [19:24] So it's important for us to have some uniform capacity for all operators in our network. But also, on the other hand, it helps us to break down costs and to further disperse the cost of proving, because one large machine could actually process multiple totally independent, smaller proving jobs, further dispersing the operational costs.


On the other hand, also, it may happen that you need multiple of those large capacity machines even to complete certain jobs, for example, Ethereum block proving. And when you want to bring it down to Ethereum's block time, you need maybe 50 or 100 GPU nodes at minimum to be able to get to that level.


So I think the approaches are different. There will definitely be networks there. I think Nexus recently had a couple of testnets where they were actually exploring people contributing through browser-based proving. And that's an interesting story. Obviously, the architecture for that, and how they orchestrate workloads, breaking larger workloads into tiny chunks, you have networking overhead there, and then aggregates all those tiny proofs generated in a browser, or on different machines to come to the final single proof. I think that's a different challenge. So basically the different approaches have their own architecture challenges and their own viability, I think.


Anna Rose [21:01] I want to kind of continue on that reframing of what the prover network, prover marketplace, like the role that it plays. Because what I described kind of at the start was this idea of this prover network which had a bunch of -- like you called them prover node operators, so different teams, maybe with a standardized prover capacity, all running these nodes and making proofs.


And then I always pictured that the prover network would be servicing multiple networks, like multiple L2s, for example, multiple proof requesters. But we've seen a lot of the prover networks come online or at least be proposed with like one proof requester customer in mind, sometimes from the same company. So you'd have, like I'm a zkVM, I'm creating lots of proofs. I'm also going to run a prover network where other people can help satisfy those proofs, create those proofs for this proof requester, which is me.


And so there's almost like this link between those two things. This was something that I felt we heard a lot last year. Would you say teams are moving away from that or moving towards that? Like how is that relationship changing and how is actually ZkCloud maybe a little bit different from that?


Norbert Vadas [22:25] That's a very good observation. And it is indeed true that most of the prover networks -- if we step back maybe 18 months, it was autumn 2023, I think, that's when the initial architect of Gevulot was released. And basically largely it kicked off kind of this prover network craze where early 2024 different teams started coming in and announcing they're building a proof market or a prover network.


I think in March -- yeah, it was March last year, when we deployed our first version of the network. Basically, this wasn't a scalable network, but it already allowed universal proving. And by universal proving, I mean, is that anyone could come in and deploy their own prover program on the network, and have proofs generated.


Currently, I mean, this is just a sidetrack, but if I may mention that, currently, universal proving is used for teams supporting different provers, and whatever they integrate, proofs can be generated on their networks for those. In our case, universal even goes above bit further because we allow you -- we don't just support any proof system from our perspective, but we allow anyone to bring their own prover program, deploy it, and generate proofs. So that's very unique in that sense.


But coming back to the initial topic -- sorry, I don't want to distract too much -- coming back to the initial topic, I think it's a very interesting observation. So many of the teams that had some kind of product, like Lagrange, their coprocessor initially, or Succinct was building the zkVM or RISC Zero zkVM. All of them started to offer proving services. For RISC Zero, it was Bonsai, which was basically a centralized service they've provided.


But then others started announcing different prover networks. And I think it's a very viable thing that they want to -- and it's a very natural thing that they want to service those people who come in with demand that they want proof generated on those zkVMs or on those products.


Now some of these teams were actually planning to go more universal, and open up proving and support other zkVMs or zkEVMs potentially. But I also know of a couple who actually decided to go more focused and optimized for what their own product is. So it's kind of like, I think we've got the prover networks that have been launched by teams that have some kind of product, and they may be a little more focused on satisfying the proof demand for that. And then you've got some more universal networks supporting all different zkVMs and zkEVMs out there.


I think with Gevulot and ZkCloud, we went even one step further where we just offered anyone to package, and deploy their own program easily, and then just start generating proofs on our network. So, it's basically just as simple as referencing the prover ID that they deployed on the network, sending the inputs, defining the amount of hardware to be allocated to that particular job, and then off it goes. Once the proof has been generated, they can fetch the proof. So it's even one step further. And I think that's a key differentiator there.


But the dynamics are interesting on how the different prover networks have been evolving. And also, I think, the marketplaces seem to be interesting, because marketplaces are -- in my head, a marketplace, it includes the word "market, so it's naturally an auction-based platform.


Anna Rose [26:34] Basically, everyone puts up an offer for how expensive a proof would be, and then the proof requester can choose?


Norbert Vadas [26:41] Yes, exactly. So it's a very good point you bring up. And I think it's very interesting how also the different workload -- we call these workload allocation mechanisms work. And I think that's also an interesting topic. Maybe we could touch upon that later.


Proof mining, which is a little bit similar to Bitcoin mining, where you've got a race, everybody is racing for the job, and whoever generates the proof first gets rewarded. However, this is never going to be the cheapest option because someone has to pay for all that redundant, and I could even say, wasted computation involved in the process.


Then there are order book based mechanisms where there are basically bids and asks, how much do proof providers ask for generating a certain proof? And how much proof requesters are willing to pay for a certain proof. And when there is a match, then the workload is allocated and then the proving job can start.


Obviously, there can be situations where there is no match for certain amount of time, and for fairly time constrained workloads, that's not necessarily the best option. Then you've got these different auction mechanisms where it could be a simple reverse auction, where basically there is a proving job that you want to have proven, and you accept bids from operators, or from different prover networks with a quote for how much they are willing to actually generate that proof. And usually the proving, since it's a compute heavy task, and it's not a cheap task to be performed, it's very cost sensitive. And actually that was one of the key reasons to start developing Gevulot back then.


So the founders of Gevulot wanted to have, one, a very cheap proof provider, an order of magnitude cheaper than the traditional Web2 cloud compute providers. Plus, they wanted to have a decentralized alternative. Because we've seen what happened in Web2 with the compute space. There are very few large providers that dominate the space. And this is something that since we are in the blockchain space, we have this ethos of decentralization and all the benefits and advantages that come with it.


Landing with the same centralized setup, in terms of compute heavy tasks, would not be an optimal scenario at all. So that's why building a decentralized network that is also providing super cheap proofs for any user. Because we've seen that, the cost of proving, it did not make projects economically viable for quite some time. And as soon as we see that we hit maybe an order of magnitude drop in proving costs, those projects and implementing ZK in those projects, actually becomes realistic, and it's not a bottleneck anymore. And then, there is randomness coming back to the workload allocation mechanisms.


Anna Rose [29:55] And this is how you guys do it, right?


Norbert Vadas [29:57] Yeah, that's how -- that's the decision that we have made. We've seen happened with the Snarketplace. Also, we know that -- like we've analyzed a bunch of different auction mechanisms, starting from a very simple reverse Dutch auction to more complex mechanisms. Unfortunately, whatever sophisticated architecture you managed to figure out, 80 - 90% of the allocation still depends on the cost of proving. And so that's a dominant factor. And until we are not able to solve that, auctions are very easily gameable. Meaning, any large entity with a lot of resources could come in and just underbid everyone else.


Anna Rose [30:41] And make it unprofitable for them, and basically run them out of the market, and then be able to put their prices up. Yeah.


Norbert Vadas [30:45] Exactly. So an auction, and that's a very key point, I think. An auction can be a way to find -- to let the market find the cheapest cost for proving. But as soon as it's gamed, and some dominant entities emerge, they can actually introduce unhealthy economic dynamics and start influencing the cost of proving a lot. Obviously, go-to-market is much easier if you let the market decide the cost of proving, because you don't need to build and figure out the entire economics yourself.


We've taken a very different approach. So we wanted to be very independent in terms of any underlying economics. That's why we went with a very minimal Cosmos-based L1 that is managing our workload allocation, and the economics. And basically, this way we had a clean slate. We don't depend on the economics of Ethereum or an L2 or any other network that we would be building on top of.


So with this L1, we've designed the economics in a way that it makes sense for node operators, and it ensures profitability, but then it provides super cheap proving for all the users. And with randomness, the extra benefit that you get is, you can introduce actually backup mechanisms, and fallback mechanisms into your protocol. While with an auction, for example, if I win the auction, and I don't deliver the proof, then you need to start the auction process all over. So it's very hard to build by default these fallback mechanisms or backup mechanisms, because you just need to start it all over. It's a one-to-one match, one requester matched to one proof provider


But with our network and with randomness being used for workload allocation, we can actually initially select more than one prover, kind of secondary, or third provers, backup provers to come in. And then we can also, as a last resort, open up the proving job for anyone to come in from the network. First come, first serve. Whoever delivers the proof first, they will get rewarded. So in a sense there are multiple layers of execution guarantees that can easily be built on top of this architecture that we've designed. So I think long term, this is the safe solution for L2s, or for proving jobs where actually the finality of the chain, or the progress of the chain depends on those proofs arriving.


Anna Rose [33:29] So where liveness is the most important.


Norbert Vadas [33:32] Yes, exactly.


Anna Rose [33:33] This is interesting. So, I mean, you've defined these different kind of economic approaches. The proof mining speed, the order book, the auction, the random. You are identifying the random one as this is potentially an ideal case for liveness, if it's very critical that you can't have a case where the proofs don't come in, or there isn't relevant party ready, or they're down or something like you need the redundancy. But are there other cases where an auction makes more sense, or where an order book, or a proof, like sort of proof mining model, makes more sense?


Norbert Vadas [34:09] I'd say the mining --


Anna Rose [34:12] Wouldn't it be cheap for the end user?


Norbert Vadas [34:14] No. I think actually it would be more expensive because with mining and with all those parties racing, and only one getting rewarded, they would expect those rewards to be higher.


Anna Rose [34:25] You're right. You're right.


Norbert Vadas [34:25] And at the end of the day, it would be the user who is paying those higher fees. So I think that would land with a quite high proving cost. Auctions are a good way to let the market find the optimal cost, and maybe it can be a good tool to find kind of low proving costs as well.


On the other hand, our bet is that as soon as you can design an economic model that is cheap enough, you can just bake it into a protocol which ensures that nobody can actually influence the cost of proving over time. So you bake it into the protocol, and you can just do a credibly neutral selection through randomness basically. I think, overall, I do believe that randomness would be the right long term approach, even though parties are not necessarily rational.


So let's say, you just have a cluster of GPUs, for example, and you want -- in a rational world, you would want your operational costs to be covered plus earn something. But very interesting thing is, and this is what I heard about Aleo proving, for example, when Aleo was on testnet, actually many different operators were doing proving even though they were only partially compensated for their operational costs. So the market is not necessarily, or the operators are not necessarily rational in that sense.


What we have been focusing on is to build a network that is suitable and optimal for maybe 98% of the use cases. And for that, we bake into the protocol the characteristics of decentralization and permissionlessness, all the high liveness, censorship resistance, et cetera that comes with it. And by designing user-focused economics, we ensure that they get cheap proofs. So I think that's a more long term approach rather than a quick go-to-market, and then let the market figure out the economics and the cost of proving.


Anna Rose [36:48] For the proof requester side of things though, wouldn't it be cheaper to do the auction for them?


Norbert Vadas [36:53] For the proof requesters, not necessarily. I mean, you will see as soon as some of the teams are planning to launch testnets, some of them have already launched their testnets. We are couple months ahead of the industry, I think, because we launched our version one network first. So basically, I'm looking forward to seeing all these quotes, and where the cost of proving lands on the different marketplaces versus, for example, our economics. So currently, what we see is that we are about 95% cheaper compared to for example AWS or GCP, which is great.


On the other hand, technically an auction can be cheap, but the risk of not landing with the cheapest proof is always there, because you are dependent on who bids to your request, and what those bids are. You can then wait for a cheaper offer. But then, if you are not time constrained, then maybe you can wait 10 minutes or 30 minutes more, or maybe even just one or two minutes more. But if you are time constrained, and you need to have a proof generated by some time to ensure your finality, then maybe you just need to go with what you have there as bids, and it may not be the cheapest possible option at all.


Specifically, and we haven't talked about what if a dominant entity starts emerging, and starts controlling those offers in a large portion of the incoming proof requests. And I think one important thing regarding this is predictability of the proving cost. So for example, for an L2 to have the same proof for X amount of dollars, and the same proof an hour later for double the price, is not necessarily the most sustainable approach. But that's what they are going to have to accept if in an auction there are no other bidders, or there is no cheaper bidder.


While with ZkCloud, you see, we have the hourly cost of our machines, $0.84 for a double GPU machine, double 4090s. And then, if you only need one of those GPUs, and you need it for three minutes, then that's what you pay and that's gonna be -- it's baked into the protocol. So that's what you pay today, and that's what you pay in a month's time for the same workload. So in that sense, kind of the sustainability, or the predictability of the proving costs is a bit different based on the different architectures as well, or design decisions as well.


Anna Rose [39:39] I like that you just define those different kind of options though. I want to talk a bit about hardware and, I mean, because the minute you're talking about provers, and you even mentioned sort of proof mining, hinting mining, proving, it's compute intensive. It's not proof-of-stake where it's like a game. I mean, a lot of the prover networks might have a proof-of-stake validator set underneath, which are more like an economic game with stake and what have you, but proving does seem like a bit of a throwback to mining except that you're doing useful work, right? 


You're running computation with the goal of creating a thing that is actually being used, whereas mining is you're running computation on sort of useless puzzles in order to win a competition that secures a network. Right? But there's no -- like the output of that computation is not used towards something, other than the security of the network. Tell me if you think like is proving -- I mean, we already hear about ASICs, and we've done actually episodes from like the ZKPrize with some of the ZKPrize winners where we talked about accelerating FPGAs, and getting ASICS in the works. So how does hardware play a role, and how is this related, or is it similar to mining? Are we kind of recreating a mining situation?


Norbert Vadas [40:58] I think you're right there, because proof generation is kind of like a hybrid workload, where you do need a large amount of compute, for most of the large workloads, obviously, for tiny ones, not necessarily, but for proving an L2, proving a block or a batch of blocks, you need quite significant hardware. Sometimes tens or maybe at higher TPSs, even hundreds of GPUs or machines. So in that sense, it does resemble mining. And some people also started calling proof generation kind of proof mining. Both require substantial compute resources, but these resources, and the compute itself serves very different purposes.


So for Bitcoin mining, the mining uses proof-of-work to secure the network, and to achieve consensus. So redundancy is kind of an integral part of network security, basically. And you pointed that out very well, I think. For proof mining, we generate these validity proofs or zero-knowledge proofs to make any data or computation verifiable. And redundancy there increases the cost significantly. I don't want to repeat what we've talked about earlier, but it actually comes back to proof racing ending up being one of the most expensive ways to do proofing, even though it could make sense for certain use cases, where you cannot even afford the time of a secondary prover to come in and do the proving for you in case the first one fails.


So if you want to have redundancy, and actually customizable redundancy is also something that we have in the network, meaning if you want to be super sure, and receive the proof at first shot, then you could ask two or maybe even three different nodes to do the same job. And if some of them don't deliver, that's absolutely fine, the others will serve you the proof that you need. So that's something that we also offer in our network. I think that's one of the execution guarantees for those very sensitive cases.


So proof racing or proof mining would be quite an expensive approach. I think, and also then a very interesting thing would be redundancy versus economic security, because if you don't have redundancy, then how do you know or how do you ensure that the selected prover, the one prover that is working on your job, will actually deliver? And this is like we are already -- I don't want to distract, and I'll come back to hardware in a sec, but that's also a very interesting thing. So by default you could use staking and then slash the prover, if they don't deliver. There could be a financial bond, maybe even.


But then you already have economic commitments in the form of the computer hardware, which may be actually very expensive. So the entry barrier for anyone to come in and join proving may be extensively high. If you add staking or a large financial bond on top.


And there could be ways to come about this. For example, we haven't really seen reputation systems. We haven't really seen performance or reputation based rewarding, for example, or proving capacity, and availability constantly on node operators. And if you know that they do have the capacity, and they are available, then it could make up for part of the economic security.


So a combination of these, I've been thinking a lot about this, and we've been chatting a lot about this internally as well. So how can you ensure that the proving market is accessible for people and not overly -- not get overburdened by different economic commitments that they need to make. But on the other hand, you also ensure liveness, you ensure proof delivery, because the finality of a chain may depend on that. Yeah. It's a very interesting topic. I could talk a lot about that. Sorry for distracting the topic.


Anna Rose [45:18] I want a quick clarification though, because you're saying that like the ZkCloud model, you're not doing full redundancy. It's not like everyone -- it's not proof racing. You're not all running for one proof. But how many machines then, because you -- but you also have redundancy built into it. So is it almost like a percentage of the network will be running for any particular proof? Like how do you choose how many?


Norbert Vadas [45:40] By default, we allocate one. But if your proof request is actually very sensitive, then you could configure your request in a way that there can be two or three allocated initially, and doing the same job.


Anna Rose [45:54] I see.


Norbert Vadas [45:55] So it's up to you. Obviously, it will increase the costs, but yeah, so it's something that's possible for you in the network. But it's a very good point.


Regarding hardware, I think that's also a super interesting topic, and the impact of hardware on the proving industry. Because currently, what I see is there are some teams like Polygon zkEVM or Aztec's Barretenberg prover that use purely CPUs. But we do see that most of the zkVMs and most of the other prover programs, they use GPUs. And there are, again, some teams that actually bet on GPUs being sufficient, and they do kind of software hardware co-design focusing on GPUs and the improvements coming over the next generation of GPUs. But then there are some teams I think, think Cysic or Irreducible could be one of them.


Anna Rose [46:59] They're more hardware oriented, they're coming from hardware, yeah.


Norbert Vadas [47:04] Yeah. Who have built FPGAs for example, and these types of things. On the other hand, it's not easy because you would need, I think, Aleo ASICS for Aleo mining, you cannot reuse it for other purposes basically. And that again may be a bottleneck. Like being more universal may mean that you can accept jobs from -- for very different proving workloads. So your hardware utilization could actually be much higher in a sense. So there could be advantages and tradeoffs.


Anna Rose [47:39] But there is the one in the middle, the FPGAs. Could you use those?


Norbert Vadas [47:42] You could, but then you may land with a little bit less performance than ASICs. On the other hand, one of the most interesting developments is for example Fabric's VPUs, these verifiable programming units. Those are as far as I know, they should be coming out in the next few months, maybe even less. 


And those are basically custom silicon that is purely optimized for all those arithmetic operations included or involved in ZK proving, and everything being processed on these custom chips. Because, when we talk about GPU acceleration, usually most of the operations run on the GPUs, but are some that are still requiring certain amounts of CPUs. And then the communication, and the memory requirement, there can be different constraints there.


But with these VPUs, it seems that they are able to process every single operation on these custom chips super fast. And the cost performance ratios that we are seeing, if those benchmarks that have been published do hold, they can shake up the space quite a lot, I think, because -- I can mention maybe a couple of numbers. For example, compared to 4090s, one single VPU card compared to one 4090 could increase performance by 5, 6x for many of the RISC Zero operations, like the operations that the RISC Zero zkVM uses.


And as soon as we start getting there, the operational costs, the cost versus performance, it's going to drastically change if indeed those benchmarks hold. Yeah. So I'm really curious to see when they come out, how they really perform on actual real-world workloads. I've been talking to Jeremy, CEO of RISC Zero in Denver, and he said they are making 3, 4x performance improvements purely on their software every quarter.


Anna Rose [49:52] Wow.


Norbert Vadas [49:53] Now you add a 5x or maybe, I don't know, 3x, 5x, 10x, who knows? Improvement on the hardware side.


Anna Rose [49:59] Just on the hardware.


Norbert Vadas [50:01] Exactly. Then for example, things like real time Ethereum block proving become really tangible and really a reality, even now, or maybe in the next couple of months, what we couldn't have imagined maybe a year ago or even six months ago.


Anna Rose [50:17] Are there other hardware companies? Because like I remember at ZK11, Justin Drake announced and shared, I think he had it like a ZKP ASIC, but I don't know -- it was by Cysic, I think, but I don't know what was that for? Was it for a particular proving system?


Norbert Vadas [50:35] Unfortunately, I don't remember that by heart. What I do know is that these VPUs for example, are reprogrammable, and you can add multiple different proof systems to be supported natively. So that's kind of like the performance of an ASIC with the programmability of an FPGA. And that's going to be a game changer, I think.


Anna Rose [50:58] Interesting. You sort of mentioned the different proving systems. This has kind of gotten me now curious. You said that because it's somewhat universal, you can almost like deploy the proving system of your choice, and then provers will prove the way that you need to prove for that proving system system, if I understood it correctly.


Norbert Vadas [51:15] Yes. On ZkCloud. Yeah.


Anna Rose [51:18] So what are the differences there? Like maybe just share with people, what is the difference between a Plonk -- I guess a Plonk system or Groth16 system in terms of proving? What are the different things happening? And I guess, if you're mostly using GPUs, you can do all of them because you can adjust. But the minute you get into these -- what did you call them? VPAs or something?


Norbert Vadas [51:39] VPU. The customer there --


Anna Rose [51:42] What does that stand for actually?


Norbert Vadas [51:43] Verifiable Programming Unit, if I'm not mistaken.


Anna Rose [51:47] Okay, okay. But yeah, like would that interfere with that flexibility?


Norbert Vadas [51:51] That's a very good point. So in our network basically, obviously, we integrate most of the well known and very popular provers. For example, zkVMs that are out there, or the zkVMs support for different L2s like Zksync, Polygon, Linea, Aztec and Scroll and all those others. On the other hand, we do not want to be the bottleneck ourselves in terms of do we have the capacity to integrate your custom prover if, let's say, you took some existing one, you made your optimizations, and you want that to generate proofs for you at a third party provider, because you don't want to rent the hardware, or you don't want to build the hardware yourself.


So for that purpose, basically, we built a very flexible network where as long as you can compile your prover into a Linux binary, and as long as your prover runs in a container, you can very easily deploy it on Gevulot, on -- sorry, ZkCloud, and just send proofs to be generated. So basically coming back to your point, we are going to support VPUs and obviously, when it comes to prover node selection, and workload allocation, we are going to take into consideration which prover node is able to support that workload, and generate the proof for you.


And actually, the way we work is that when you submit the workloads to ZkCloud, you need to define the amount of CPU cores, the amount of memory, the amount of GPUs that you want to be allocated to your job. So, I guess, in the future, there would be another parameter, do you want VPUs to be allocated to your job? And then, you would know as a requester that you want GPUs, and that performance is sufficient, or your prover program is supported by the VPU. So you can even have a couple of cards thrown onto the job, and have it generated even faster.


Anna Rose [54:03] And maybe you pay a little more for those particular jobs.


Norbert Vadas [54:06] Yeah. It's going to be interesting to see where we land in the cost because -- 


Anna Rose [54:11] Oh, it's faster.


Norbert Vadas [54:12] Exactly. So we might think that it costs more because those are custom silicon, but then, if you only need 1/5 of the time, or 1/10 of the time, you actually may end up very cost effective there.


Anna Rose [54:27] Interesting. But are there other teams doing these things?


Norbert Vadas [54:31] I think as far as I know, Irreducible is planning to build a custom silicon. If I'm not mistaken, they already have FPGAs, but kind of like a programmable ASIC-type chip for ZK. And also, I heard about Ponos Technology, a team based in Switzerland who are looking to build something like that. But they are super early, if I'm not mistaken.


Anna Rose [54:57] Interesting. There's one part of the sort of proof journey that we haven't really talked about, and I don't think it's something that your project really deals with, but I do want to just sort of wrap up that journey. We talked about sort of the proof requester. So this is an entity that needs proofs made. Could be an L2, could be a zkVM, could be an application that uses ZK. They need proofs. We have the proof provider as you've defined, which could be a prover network, prover marketplace, or it could be an individual prover machine going directly to create that proof. But then what happens to the proof? Like somewhere, it has to be verified, I guess. We've sort of talked about the first half of this journey, but where do they go? And is it actually the prover network or prover machine that sends it there?


Norbert Vadas [55:50] I think there can be different approaches there. So if you take the example of an L2, then the sequencer of an L2, who builds a certain block and has to have that proven, so they commit the block to the L2's smart contract on Ethereum, for example. And then, they send this workload to a marketplace or a network. They get a proof generated, and then the proof has to be sent to the verifier contract also on L1.


If it verifies correct, and the state transitions in the proof match the state transitions in the initial commitment, then it gets settled, and finality is reached. Now, there could be different ways that L2s want to interact. Maybe some of them want the proof to be sent back to the sequencer, and then they send it to the L2. Other teams may want the proof to be directly sent to L1 to be settled.


In that way, basically, our initial design is you send a proof request and as soon as it's ready, you can download the proof. But we are also working to be able to support sending the proof directly to, for example, a smart contract on Ethereum. I mean, that could be also done through an adapter maybe. But there are different approaches, and that's basically the last step. And I'm not very deep into verification and the proof aggregation part, because many of the teams say --


Anna Rose [57:32] There are teams that focus purely on that.


Norbert Vadas [57:33] Exactly.


Anna Rose [57:34] But I'm sort of trying to figure out like, would you then send proofs their way and then they aggregate it into single proofs and then verify it as a batch. Is that sort of how one of these flows could work?


Norbert Vadas [57:47] Yes. I think that that could be one of the flows, where they receive multiple proofs. So these proofs don't go to L1 directly, they receive multiple proofs at the proof aggregator. Maybe they aggregate 10, 20 of those different types of proofs into a single one, and then have it settled on Ethereum, or some other network.


Now the interesting thing for aggregation is that, you may also need compute capacity for those jobs. So a proof aggregator in theory, and we are also exploring this with a few of them, they could actually outsource the aggregation workload to some network, like for example, ZkCloud. We are very well fit to do that. So in that case, the proofs to be aggregated would be the inputs to our network.


Anna Rose [58:39] Oh, weird.


Norbert Vadas [58:40] We do the aggregation with all the compute resources in the network, send the final one to them, which they can settle. But I think there are other approaches as well, whereas some networks may actually verify the proofs themselves kind of off-chain, and then just send a commitment to Ethereum saying yes, there exists a valid proof for this state transition, and sending the commitment might be much cheaper than actually sending the proof and having that verified on L1. So, there are different approaches, but I don't know a lot about that aspect.


Anna Rose [59:19] I feel like I'm due to dive back in with one of those teams, one of the proof verification teams, because the first time I heard about it was when NEBRA came on, and originally Shumo and NEBRA had been more of a prover network, and then had moved sort of up the stack to be more of the verification aggregator. And that was sort of the first time we started talking about that.


He also was, I think, the person who coined the ZK supply chain that I've sort of hinted at here with the different actors. But his focus is more just like proof requester, proof generator verification and focusing on that third step. But it seems like there's a lot of teams doing that right now. I mean, I remember for zkSummit -- for one of the zkSummits, we got a bunch of applications from -- and they were almost identical and I was like, wow, what's this boom of verification, folks. So it's probably due to -- I'm probably due to jump back in.


Norbert Vadas [01:00:14] Yeah. There are a few very good teams that are building that.


Anna Rose [01:00:17] Cool. So I think we have covered this through the episode, but I do want to just sort of wrap on the why of the prover marketplace, sort of summarizing what we've shared. Because, I think you have actually said what the benefits of it are, but why decentralize this proof provider role? What are all the benefits that one can get from them?


Norbert Vadas [01:00:40] I think we might have talked about this partially earlier, but usually decentralization serves two purposes, liveness and also security. But in case of ZK, security is basically a given because of the math, the underlying math. So you don't necessarily need decentralization for that aspect, but to strengthen liveness and to strengthen properties such as for example, censorship resistance, or I could imagine a situation where some block of some L2 includes transactions touching certain accounts on different sanction lists, whatever.


So there could be a censorship element there. Even though may not be very likely, it may rather happen on the sequencer side as soon as it's decentralized, because they are the ones building the blocks. But as we have seen with Ethereum's block production, sometimes it's the builder's censoring, sometimes it's the relay that's censoring, sometimes it's actually the block proposer, the Ethereum validator that is censoring.


So in that aspect, I could imagine something like this happen. And because of that -- decentralization is not important for the sake of decentralization, but really for what additional benefits it brings in terms of high liveness censorship resistance. We've talked about these.


It's a very important decision for any project that wants to outsource proving what kind of provider, or what kind of way, or path they choose when they are going out selecting a provider. I think Zac from Aztec has been vocal about this maybe a month or two ago, saying that you build a nicely decentralized network, but then any proof provider out there should actually have the same properties, permissionlessness, decentralization, censorship resistance, or else they are going to introduce tradeoffs to the network that you have built. And I think this is a key element that has not been talked about a lot.


We've recently released a blog post on that, because exactly for that reason did we build ZkCloud as an independent L1 as a permissionless network with the most neutral workload allocation mechanism in the form of randomness, with all the properties for minimizing the risk of any type of centralization as much as possible, minimizing the risk of control over approving fees through baking economics into the protocol.


So it's kind of it was very good to hear him talking about this, and being vocal about this. And a year and a half ago when the initial design came out, it was actually designed in a way that it facilitates what he was talking about to ensure that networks don't need to accept tradeoffs, actually. There exists a solution, and there may be others as well later on. I don't know, where you don't need to accept tradeoffs, in terms of your decentralization or the other properties. I don't know if I could summarize that properly, but that would be my -- kind of like my answer.


Anna Rose [01:04:03] Cool. Well, thank you so much for coming on the show, Norbert, and exploring this evolving prover marketplace space. It's been really fun to explore that, to tease it out. You can tell in talking about it this year that there have been findings, there have been some understanding, some experiments have been run, or at least have been proposed, and now people are really thinking about these setups at a deeper level. So it's been really fun to talk about it.


Norbert Vadas [01:04:31] Thank you very much for having me. I really enjoyed the chat.


Anna Rose [01:04:35] Cool. I want to say thank you to the podcast team, Henrik, Rachel and Tanya. And to our listeners, thanks for listening.