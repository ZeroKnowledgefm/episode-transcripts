Anna Rose (00:00:05):
Welcome to zero knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralized web, as well as new paradigms that promised to change the way we interact and transact online

Anna Rose (00:00:27):
This week, Tarun and I chat with Sreeram Kannan associate professor at University of Washington, where he runs the UW blockchain lab. In this conversation, we look at how information theory and blockchain intersect and talk Sreeram's realms early work on P2P, mobile networks, mapping Bitcoin security, and how this evolved to work on consensus, fair sequencing and more. We couldn't get through everything in this episode. So hope that we can have Sreeram join us again in the future. Before we start in, I wanna let you know about a section on the zero knowledge.fm website. That might be interesting for you. If you're looking to jump into ZK professionally, it's called the ZK jobs board, and there you can find jobs posted from some of the top teams working in ZK, find the project or team you wanna work with next, over on the ZK jobs board. I also wanna make sure that the upcoming ZK Hack mini is on your radar. Last fall we did a seven week event. We with puzzle hacking and ZK tool workshops. Now we bring it back for a mini version of this event. It's set to last two weeks and starts on March 1st, 2022. If you missed the first edition or if you attended and just wanna join again, be sure to sign up today for the newsletter. You can also already sign up for the March 1st kickoff event. There you can get updates on partners and schedule. Speaking of partners, I'm very excited that our puzzle building partner for this edition is Polygon. We are very much looking forward to once again, bringing the ZK community together around ZK Hack mini. We hope to see you there. So now I wanna hand over the mic to Tanya, the podcast producer to tell us a little bit about this week's sponsor.

Tanya (00:02:01):
Today's episode is sponsored by Electric Coin Company, the inventors of Zcash and Halo cryptography. They're building the next generation of zero knowledge tech with network upgrade 5 happening this April, Zcash moves to the halo zero knowledge proving system, removing trusted setup and becoming shielded by default. They believe privacy is essential to delivering on the promise of a thriving and equitable Web3. Visit electriccoin.co to learn more about the relationship between privacy, self sovereignty and creative freedom. That's electriccoin.co. So thank you again, Electric Coin Company. Now here's Anna's interview with Sreeram.

Anna Rose (00:02:43):
So in today's at episode, we're here with Sreeram Kannan associate professor at university of Washington, where he runs the UW blockchain lab. Welcome to the show Sreeram.

Sreeram Kannan (00:02:53):
Thanks Anna.

Anna Rose (00:02:54):
I actually wanted to first start this episode with a question to Tarun. Tarun really brought this episode together and introduced me to Sreeram. So Tarun, can you share with us what we have planned for today's episode?

Tarun (00:03:05):
Yeah, so, you know, one interesting thing I think about the cryptocurrency in blockchain space is that, you know, it a little bit like AI is able to draw talent and ideas from a lot of different backgrounds and, you know, one interesting sort of view from an academic lens is that, you know, cryptocurrencies really started mainly with cryptographers and then sort of with distributed systems people. But over time it sort of branched out into other academic fields over time. And you know, personally, I came from sort of this kind of like mixed background as well. And I I've always, you know, when I enter the space, I, I kind of observed like, Hey, there are all these cryptographers who are really bad at probability. Like there's gotta be something that like someone who knows how to, how, what a measure is can do. And you know, that sort of is actually what nerd sniped me into space. And around the same time I started observing Sreeram and his advisor and some other folks at different institutions, like starting to write papers that were a lot more rigorous from sort of the probability standpoint than had been in the space prior. So I think the, the idea that, you know, you can take tools from other fields and apply them to kind of like subsets of problems that are actually really interesting and widely applicable is sort of the main theme that we'll go for. And I think hopefully with this episode, you'll get to see kind of the variety of work that Sreeram has done. And also if you're, so if you're say someone who's not in the crypto, but you like studied math or something, or, or you kind of know, you know, these other fields you'll get an idea for like, Hey, there's actually a ton of open problems here where you can like apply tools from other parts of the world. And I think, yeah, kind of getting a little, I think Sreeram's background and history will give you kind of an idea of like how, you know, in tools and applied math and, and probability can be applied to a really large variety of fields. If you take kind of this very generalist lens instead of being a super specialist.

Anna Rose (00:05:13):
So Sreeram, I now really wanna hear a little bit about your background and maybe what nerd sniped you to jump into this.

Sreeram Kannan (00:05:22):
Absolutely. thanks Anna, thanks Tarun for the kind introduction. So I, I got into blockchain in four years back, but the story of my interest in this field or peer to peer network actually dates way back to my PhD thesis. My PhD thesis was actually thinking about how to construct peer-to-peer wireless networks. What are the fundamental limits? Today when we look at blockchains like Bitcoin, Ethereum, and so on, one of the big questions we look at is what is the maximum throughput that you can achieve with these kind of like peer-to-peer networks? What is the latency that we can achieve with these networks? These were exactly the same type of questions we studied, but in the context of peer-to-peer wireless networks, they used to be called ad hoc networks at that time. And there was a massive interest in figuring out, can we build wireless networks in the absence of any infrastructure, no base stations, no wifi access points. Can we just create self organizing wireless networks, peer to peer, they just talk to each other and then organize themselves into a network. And the kind of math problems we were interested in are, what are the fundamental limits of such network? Can you achieve, like, you know, what's the maximum throughput capacity that you can achieve? What is the minimal latency? And it was in my PhD thesis. My advisor professor Pramod Viswanath at Illinois, Urbana-Champaign. And I worked on some of these problems, which were basically theoretical limit of peer-to-peer networks, particularly peer-to-peer wireless networks. And so our interest in this started already at that point. However, what happened was peer-to-peer wireless networks did not take off it's it's because the massive deployment of infrastructure in wireless including access points, base stations, optic fibers, the massive availability of infrastructure, obviated the need for constructing peer-to-peer networks. And at the end of the end of my thesis, even though we solve some open math problems inside these fields, the real world application was lacking. So I was sitting around rethinking, what should we do now that, you know, we, we solve this math problem. It all looks pretty amazing. But in reality, nobody's actually interested in deploying a peer-to-peer wireless network.

Anna Rose (00:07:52):
So you had the solution without the problem.

Sreeram Kannan (00:07:54):
That's right. We definitely had a theoretical problem, which is how would you go about constructing it? But there was really nobody who was actually particularly interested. Indeed, actually Qualcomm at that time had a big project, which it shelved right around the time when I finished my PhD thesis called FlashLink, which was trying to build peer-to-peer wireless networks. My advisor used to work with them and so on. So we were thinking that could become a thing, but it didn't become a thing because of the massive deployment of infrastructure. So it gives me a lot of satisfaction to see Helium today, which is basically brought about that idea back into wireless. Peer-To-Peer networks instead of, I think a paradigm shift that Helium brought in is instead of building just a peer-to-peer network, they built a peer to pool to peer network. Oh. And the pool is basically, I think this is a kind of fundamental transformation that we are seeing in decentralized exchanges, for example, order book exchanges, peer to peer, right. Are peer trades with another peer, but fundamentally the new paradigm, you know, we are seeing in AMM dexes and so on is peer to pool to peer. So that's exactly the same paradigm shift, I think Helium brought in, they said, well, it is infrastructure based networks, but why should only like monolithic companies build these infrastructures? Can we get coordinate regular people to put up infrastructure, put the right incentives and actually create these pools from which we can start providing service? So I, I feel like, you know, world has come a full circle full starting from peer to peer wireless networks back in 2008 I think 2002 to 2008, 2012, there was a lot of academic interest in that. And then that died down. And I think now for example, my advisor Pramod Viswanath, he's actually studying how to build these kind of peer to pool wireless networks. What are the right incentives? What are the mathematical perimeters that need to go in again, a lot of interesting random processes and analysis showing up there.

Anna Rose (00:10:05):
Very cool. You're talking a little bit about this sort of lull in the P2P research or P2P interest. Would you put that firmly in that P2P winter that we've kind of heard to talked about? Absolutely. So what, this is 2008, would you say 2010 when it started to just die out?

Sreeram Kannan (00:10:23):
Yeah. Okay. Yeah. So 2008, I started my PhD and I finished my PhD around 2011, 2012. And the, the interest was just waining. So there was, so the interest in P2P wireless networks was already went by 2008, but we were thinking because wireless is infrastructure was not built out, at least all over the world. There was still a possibility that peer-to-peer wireless networks could take off, but that, that didn't turn out to be case.

Tarun (00:10:51):
I, I, I think one of the most important things to realize from listening to this, and it took me a long time to understand this is that a PhD is really like a venture bet. It's like investing in something and then like wait, waiting for five years and then seeing if the thing is still alive after five years, and choose wisely.

Sreeram Kannan (00:11:13):
Absolutely agree

Anna Rose (00:11:14):
Sounds like Sreeram, yours has worked out, at least the, the field has come back. A lot of the work that you did, I guess, becomes relevant again. So let's, let's hear about that move maybe into blockchain for you. What, what was it, where did that happen?

Sreeram Kannan (00:11:29):
So, so after peer to peer wireless appeared to be a while, I didn't, at that time, it was 2011. So I, I, and I hadn't had the same exposure to blockchain at that time. I was looking around and seeing what were, what are some of the most important things happening and potentially, how can we bring this understanding of information theory and networks and graphs into this field? So just for viewers who may not be familiar with information theory, one of the most celebrated icons in computer science is Alan Turing. I would say equally important was Claude Shannon, who fundamentally pioneered this view that all information can be represented digitally. Without any basic loss of performance, it was not at all obvious during the time that he pioneered these ideas back in nine, around 1945, 48. At that time, you know, communication was all analog, you know, radio stations used to use frequency modulation and things like that, which were methods. And he pioneered the idea that almost all information can be represented by bits. He pioneered the idea that there is a way to communicate bits without error. He pioneered the idea that you, you can bring up very sophisticated tools from mathematics to actually solve some of the real engineering problems in communication compression information processing. And I think it has laid the round work on which much of our modern society is built and definitely modern information technology. So coming from that field the inspiration that we had was can we bring these ideas into other emerging important fields? One of the things that we took a bet on several years, again, like Tarun said, another venture bet was in computational biology. So we, we spent several years, five years working on computational genomics, trying to understand DNA, RNA sequencing, things like that. And it was in 2018 January, that my former advisor at the time Pramod called me and said, Hey, do you know about Bitcoin? I said, yeah, I've heard of it. Why are you asking? And he said, no, you know, there is this thing called Bitcoin. It's like super interesting. And you know, the two basic problems in Bitcoin, one is Bitcoin as low throughput only can process a few transactions per second. And it has very high latency. It takes like several hours to confirm a transaction with high reliability, is that all our life, or rather at least my PhD, we definitely spent all our time thinking about throughput and latency in peer to peer networks. And the question was, can we redesign Bitcoin to actually be very high throughput and very low latency? And this kind of immediately caught my attention. And so that's, that's really the, the technical in for me into the area.

Anna Rose (00:14:35):
And what was the work that came out of that problem? Like, did you end up solving that? Is there something that we can like point to a paper or anything that, that describes it?

Sreeram Kannan (00:14:44):
Yeah, absolutely. I'll explain what we ended up doing. So before that, I, I think, you know, I was going through a more a philosophical problem in my head. You know, again like Tarun alluded. These are like venture bets and to take a venture bet it's to get into any area and do something useful. It's gonna take at least three to five years. So that's a basic minimum compounding required to get any kind of results. So I was considering, wow, we are working on all this cool genomic stuff. Should we give that up to work on on blockchain? Is this equally important? One of the, one of my reasons of getting into genomics is, you know, this, all this crazy things happening in synthetic biology, we are gonna be able to engineer genomes, reengineer ourselves seemed like a massive paradigm shift, and I wanted a part of a smaller part, but a part of it. And I was wondering whether blockchains are anywhere near the same scale of interest. And I wasn't convinced for a few months I was doling around looking through it and then what it clicked one day. And here's what clicked meant for me. One of my favorite authors is Yuval Noah Harari his famous book Sapiens, and there, he makes this thesis that humans have taken over this planet. He, he summarizes the humans. Evolutionary advantage in one line is as humans took over the planet because we cooperate flexibly in large numbers.

Anna Rose (00:16:13):
And we believe in myth, we're, we're able to believe in myths,

Sreeram Kannan (00:16:16):
Absolutely. In fact myths help us cooperate flexibly in large numbers, but the fundamental primitive he lays out is this ability to cooperate flexibly in large numbers is what has led us to this evolutionary advantage. As I started chewing on at, it became obvious that any mechanism which helps humans cooperate flexibly in large numbers more efficiently will have a very basic, fundamental evolutionary advantage to us as a species and worth investing deeply in whether Bitcoin crashes or not. So until I got to that conviction, I could not jump all in, on this new venture bet.

Anna Rose (00:17:00):
Yeah. Wow. Because that is such a shift like you're working on when you were saying kind of genomic is that mapping genes and this like deep, deep biology, like super important medical, like knowledge and research into knowing kind of how we're built and how it works, but, and you made a shift. Yeah. But you actually could see that even. I mean, when you think of blockchain, sometimes you do think of like the speculative nature of it. And it's a little bit like of a shame. If you move from medical to like, oh, I'm gonna make money over here. But I think the way you just put that, is really cool. This is this idea of the, the organization of society. And you can't even have that deep medical research if you can't organize society correctly.

Sreeram Kannan (00:17:44):
Absolutely. Absolutely. I think just like the internet built information, super highways, I think blockchain is building the cooperation super, super highways.

Anna Rose (00:17:53):
Cool. And I guess, is this what led you to look at consensus? Because that's like the deepest cooperation side of things.

Sreeram Kannan (00:18:04):
On, on the one side. Yes. On the other side, that's also our, our distinguishing advantage that we've actually thought deeply about how to organize peer to peer networks. How do you get information across these networks at high throughput, low latency? So for people coming from distributed systems already come with a deep understanding of the consensus problem, we did not. We came with a deep understanding of network problems. How do you get maximum throughput, but how do you get low latency? Also, we come up with a come with a very deep understanding of the random processes that Tarun highlighted, which played a massive role in information theory and, and all these wireless networks, because one of the most important things you're dealing with when you are dealing with a wireless network is nice noise, noise is random, and you have to understand the randomness quite carefully.

Sreeram Kannan (00:18:54):
So the first thing that struck me is Bitcoin mining is random. Okay. Mine, a block who gets to minor block, there is some randomness. And if you view the Bitcoin blockchain as like a chain or even a tree, potentially because there are forks because two people can mine blocks near simultaneously sounded like, Hey, that's a random growing tree. Can we bring a, a lens of probability theory and information theory to actually sharply understand this problem? So once we understand why these things are actually secure, then we can maybe expand it to say, how do we get both security and high throughput security and low latency. So that was the direction in which we actually came in. And I I'd like to give a shout out to both my advisors, you know, Pramod Viswanath at Illinois Urbana Champaign, who got me into this area and a professor David Tse Stanford, who was my post-doctoral advisor. And, and the three of us got into this in 2018. And we used to meet like every day, several hours, because we didn't understand head or tail of what is going on here. And we dropped almost everything else to just study is area from like a beginner.

Anna Rose (00:20:14):
That sounds so cool. And we were one of the questions I just asked before was like, is there work that came out of this first problem? So yeah, what's, what's it called?

Sreeram Kannan (00:20:23):
So the, the paper is called Prism. The idea that we were actually trying to pursue was how do you get low latency as well as high throughput with Bitcoin? And we realized, so just delving a little bit into what, what we did there. So first to understand why Bitcoin takes a long latency to confirm. So the idea is blocks are made every 10 minutes, right? So blocks are made every 10 minutes. And just because a block is inside the Bitcoin, blockchain does not mean you should accept any transaction that went into that block. Why? Because if there was an adversity read, the adversity could potentially mine a competing block at the same D and take over the chain and your transaction may get double spend on the other chain. So the basic fact of Bitcoin is the closer the block is to the tip of the blockchain. This, the more likely it is to get reverted, the more deeply embedded a block is into the blockchain. The less likely it is to get reverted. And so this is a basic fact. And so what this implies is if you want to confirm a transaction at a certain level of reliability, then you need to wait for a certain amount of time. So this is all there in Satoshi Nakamoto's, original Bitcoin white paper. And we went through all of this and found that the latencies accruing fundamentally, because to think of this in probabilistic terms, what you're doing is let's say Tarun is an attacker. And Tarun has like 20% mining power and the rest of the Bitcoin miners have 80% mining power. Now, if you just ask, what is the probability that Tarun will get the next block first, that's like 20% because you know, it's a race between an 80% mining power and a 20% mining power. But if you ask what is the probability that Tarun will get the next 10 blocks, as opposed to the rest of the network, then the probability is much smaller because you know, the other guys are winning the race every time, and they're kind of compounding on their wins and over. So what is called the law of large numbers, which basically says, if you average enough, then you would always see that the 80% wins over the 20%. So that's the basic reason you wait for a long enough time in order to confirm a block in Bitcoin. And the question we asked is, can we get this averaging this law of large numbers to work in a completely different way, which does not require latency? So here I wanna highlight that there any other methods, people have thought about how to reduce latency in proof of work system. And one of the simplest is to say that take the last few proof of work miners and then form a committee and let this committee vote on blocks, right? Because once you have a committee, you just send the thing to the committee, they all sign, and then you're done. But to us, this felt highly unsatisfactory. And the reason it's highly unsatisfactory is one of the really beautiful things about Bitcoin is nobody's power. Everybody's power is evanescent. The power is transient and vanishes immediately. So for example, when I'm trying to make a block, you know, I'm racing, racing, racing to make a block. Once I made the block, I don't know when I'm gonna make the block. And then once I made the block, I have no further power in the network, special power in the network. Everybody's power is evanescent. So what that means is even for a short time scale, nobody has special powers. Everybody's competing fairly all the time, but if you elect the miners from the last thousand blocks to go ahead and form a committee, and then let them run a blockchain or just sign blocks. Now you have a vested powerful intermediary, this committee, which can just collude and take down the network and do what so mathematically the way we think about it is we think of this as a dynamic adversity or an adversity that can corrupt people after knowing what their identities are. And a dynamic adversity cannot corrupt Bitcoin. It's really magical in that sense, in the sense that, you know, you have power proportional to your actual computational power and you don't get any further power in the network. And so this was a very pivotal property for us. When we thought about how do you get low latency without giving up this absolutely fundamental property of Bitcoin?

Tarun (00:25:00):
One thing I always appreciated about Sreeram research is I, I also, at one point was doing computational biology and thinking about things in terms of like probabilistic processes, generating graphs and graphical models and generative models is kind of the way that, you know, I spent probably my first five years of doing research. And I think one interesting thing is that in crypto, it's actually really hard sometimes to see that those things map as cleanly, like things in ML and AI are almost always stated directly in terms of like graph processes and things that like, look like these types of well studied statistical objects. But I think a lot of the hard part in the cryptocurrency industry is A first filtering through the bullshit, cuz there's a lot of like nonsense B filtering through the wrong math. So like, you know, the Bitcoin paper, their distribution assumption was wrong, right? Like for, and a lot of what Sreeram, you know, I I'd say like there's three lines of work that have really tried to fix all of the really math in the original Bitcoin paper and don't get me wrong. Like clearly it worked and, and whatever, but like, you know, it, it should would be told the truth should be told that the math is wrong. Like a lot of the assumptions made don't work. And like this kind of push on expectation kind of assumes sort of like some extra smoothness on the, on the kind of state space that you're looking at. But there's sort of three lines of work. The first was this paper like and this was the thing that kind of nerd sniped me, which was Bitcoin backbone protocol, which is paper in kind of overly complicated, but like it did get the right probabilistic idea of, of like how to analyze these probabilistic blockchains and get kind of cap theorum like guarantees that from traditional databases that people kind of wanted, the second was sort of like the Elaine Shi and Rafael Pass like improvements on that and simplifying it. And then the third was really, I feel like Sreeram's work, which said like, Hey, instead of like trying to analyze a specific consensus protocols, what if we took the space of all consensus protocol and then said, what properties do you need to make things work? And I think a lot of the kind of famous cryptography professors who made, you know, blockchains that went live, like kind of ignored this problem when they made their chains, they, they really focused on like, here's a single consensus protocol and here's why it works. And I think like the beauty of kind of the Sreeram and collaborators have thought of it is like they went the other way around, right? They're like, what is the set of all of these things? And like, how do we distill them down? And I think maybe talking a little bit about how you got into that, how you, you guys flipped the problem in that way, like going, you know, kind of top down versus bottom up would be awesome.

Anna Rose (00:27:41):
But before you do that, what's the name of the paper you just mentioned?

Sreeram Kannan (00:27:44):
There are two papers. One is actually doing the low low latency stuff. That's called Prism, deconstructing blockchains. The other one is called where we actually look at the fundamental properties of Bitcoin and many other of large family of consensus protocols. That's called "Everything is a race and Nakamoto always wins". So...

Anna Rose (00:28:07):
Okay, everything is a race and Nakamoto always wins.

Sreeram Kannan (00:28:12):
The name was chosen by our collaborator David Tse, who just has this enormous respect, of course, for the deep math stuff, but also for somebody like Nakamoto, who, who had an intuitive sense of actually how this thing works, even without actually formalizing it in the same rigorous ways. But we were so shocked in the number of ways in which the intusions were spot on while the, the math was far lacking was actually amazing. Know you had to have a very deep, intuitive understanding of what all ways in which things can go wrong and had a mechanism to fix almost all of them. I I'll highlight a couple of them as, as we go on. But I think to reinforce what Tarun said, I think when we got into this area, we were looking around all kinds of papers. They're all looking at very narrow formulations. One of the things that we like in information theory, you know, this really Claude Shannon's school, what, what Shannon did is instead of saying, Hey, I'm gonna communicate using this method. And then let me just try to analyze it, which is what everybody was doing at that time. Instead he says, what is the set of all possible potential communication algorithms? And then can we find the absolute best among all of them? What does it even mean? Right. It seems like a simply mindboggling thing to say that there's a space of all possible communication algorithms. And then, you know, what is the absolute best among that family? So this is really a information theoretic thinking on, on blockchains. So we just brought the same fundamental question, which is what is the space of all consensus algorithms and what is the absolute limits on what you can do if you optimize across all of them, like what would be the best one? Can you actually find something which is close to the best one? And this is really this paper, "everything is a race". And David went on and brought on some really strong probabilist Amir Dembo, Ofer Zeitouni, who I we had the privilege of working with absolutely a fantastic probabilist is and great students, you know, David's student Nusret and Xuechao, who was from Pramod's student, from the big group who actually ended up working on this problem. The, the basic thing just to summarize what kinds of problems need to be thought about. So when I state at the simple heuristic about how Nakamoto did the calculation, he's saying publicly everybody's mining, 80% of the mining power is doing a public mining and the remaining 20% the attackers mining, a private chain, and then trying to displace the private chain because in, in the blockchain, the longest chain wins. So, this is the attack that he considered. But then when we looked at it, it was not at all clear why this is the only attack. What if I, as an attacker, tried to create blocks on both sides, try to balance the chain. Now there are two chains, both of them roughly equal length all the time. And you know, the blockchain just gets confused. People get confused. What is the longest chain and leave you under perpetual confusion? Is it possible? Or why is it not possible? So even modeling what an attacker can do, seemed pretty mind boggling. And this is where the, I would say it's an absolutely brilliant paper. The Bitcoin backbone protocol just came in and distilled it out to its very essence. They said, an attacker can do four things. They can decide when to mine a block, they can decide when to release that block. They can decide what to put into that block, all these degrees of freedom. And all of these of course can now depend on the particular state of the blockchain, or if one chain is winning, then you do this. If another chain is winning, then you do that. So it's a very, very, very complex state space of what an attacker can potentially do. And now you wanna prove security against this massive potential that an adversity has. And this is really what was already done in the Bitcoin backbone protocol. What they showed is if your mining rate is slow enough, if your mining rate is slow enough, then you can be secure. As long as the adversary does not control more than 50%, then the blockchain will be secure. So it's an absolutely fantastic general result, which says that you cannot execute all these other complex attacks. Even if you tried, you'll not be able to succeed as long I, as you have less than 50%. And so we use that as our baseline to build on our work. And what we said in this paper is can we think about this problem even more sharply, instead of saying that if your mining rate is really slow, then you can actually solve this problem. Can you find what is the maximum mining rate at which you will continue to be secure?

Anna Rose (00:33:22):
Was it sort of like, were you testing out sort of the parameters of the security of the system? Were you just like trying to break it? Did you break it? Did you find those limits or

Tarun (00:33:34):
So this is, this is like theoretical, right? Like it it's, I think like one, one important, I think piece of information maybe here that I think to listeners who maybe have more of a, a background in like traditional distributed systems is traditional distributed systems work has these properties that like are, you know, safety and liveness of like, Hey, if I send a transaction, does it stay in the network? And Hey, if I keep sending transactions, can the network keep processing transactions? And one of the, the kind of fundamental results of sort of like the 1990s was that you kind of can't have your cake and eat it too. You sort of can't have safety and liveness perfectly. You have to make some trade off in terms of like how much the network can partition or things like that. And blockchain sort of promised this like holy grail of like, actually you can get all three. It's just that one of them is gonna only hold with like 99% probability. One of these properties you want and the rest will be maybe deterministic or also high probability. And I think the hard part in that is that adding in this probability thing blows up your state space to Sreeram's point to be this like infinite dimensional sort of search space, right? You go from this very finite dimensional. I can work with this stuff, thing to this infinite dimensional thing. But in some sense, the infinite dimensional thing has a lot of constraints on it. It has some kind of decay properties, some types of like, you can't really like predict the future more than a certain amount of time and that all adversaries kind of somewhat have that property. And so in, in some sense that if you can formulate that correctly you kind of can get around these sort of traditional database problems and the Bitcoin Bitcoin and backbone protocol is the first to really show that, Hey, there are different types, it's a different sort of like rotating your way of framing. The problem such that you can, you can write this simply. So I think the context is like, that was sort of like the 1990s, early two thousands Bitcoin happened, but then like no one gave a good reason for why it happened. And then, you know, Bitcoin backbone protocol came out and then people started being like, I can build better than Bitcoin. Right? Like everyone and their mom's consensus algorithm. Right. So I think then it became this problem of like, who's the taxonomist. And I think that's where our story goes next.

Anna Rose (00:35:58):
So you mentioned it was like a taxonomy move, but like, isn't it like, I guess the question I would have now is like, from the work you did, were you also developing a new protocol from that? Did you actually want to build something else? Because what it sounds more like is that you were mapping the space.

Sreeram Kannan (00:36:16):
Yep. So part of engineering or, or at least principled engineering is understanding and seething through what is happening. So we did both to summarize briefly is we actually first map the space, try to understand when things are secure, when things break, and now you have these atomic units that you can combine into new protocols that can actually give you different kinds of performance metrics. We did end up the, the protocol prism some of our collaborators and, you know, people at MIT, they ended up building it and actually showed that you can get, you know insane throughputs, for example, 60,000 transactions per second, you know low latency confirmation within one or two block times, things like that actually ended up getting built. So it's not just a, a taxonomy it's, it's kind of uncovering the principles that underpinned the design of these systems. We absolutely felt what Tarun said, which is, it seemed like a simple enough thing. You just have a longest chain, why not have a longest graph or longest tree, or make it a directed graph? Why not play with that? The, the space with which you were playing seemed intuitive and simple, but the properties were extraordinarily non-intuitive. I'll give you some examples. Little bit later when we started getting into proof of stake and proof of space where some extremely non-intuitive things happened, where if I tell you the protocol, it seems like it should work. It should be even better, but actually it turns out far worse in terms of security. So the, there are all these very non-intuitive things. In fact, you know, one of, one of our starting point was there was this really interesting protocol called the Ghost protocol by Avi Zohar and, and collaborators, which said, you know, you can get much higher throughput by instead of just looking at how many blocks are in your chain, you look at the entire tree and calculate weights and stuff recursively. So that protocol claim to improve the throughput and in fact, was adopted in Ethereum among other things. And when we did a much more formal analysis, what we found is the protocol does not actually improve the throughput. It gives you the same throughput as the longest chain, but it does not improve the throughput. And it will never be obvious without actually going into the security analysis, because there was this balancing attack, which was in Ghost. If you try to improve the throughput, by making more blocks, simultaneously, what will happen is you can try to you will not be able to figure out which one of the chains actually got confirmed because I'll keep flipping between the two chains. So you never get any agreement onto what the real chain is. So there are all these very interesting things that can happen with these security things that many, many, many people were overlooking. And it, so that was the taxonomy part as kind of understanding, when a protocol is secure, what is the limits of security, Anna, as you phrased it, can you say, in this parameter regime, it is secure and in this parameter regime, it is simply insecure because this in, in, in statistical physics and in information theory, sharp thresholds. Sharp thresholds are saying basically exactly in this regime, it's all safe. And exactly outside of this regime, it's not at all safe. So don't go, don't venture into these regimes

Anna Rose (00:39:40):
And no gray zone. It sounds like, and this is like no gray zone.

Sreeram Kannan (00:39:44):
That's exactly. That's very sharp. That's why it's called a sharp. Absolutely. Because you know, what happens is as things go on for infinite time, any bad thing that can happen will happen.

Tarun (00:39:54):
I think one thing that's funny is the first time I was on the ZK Podcast in 2018 or whatever, I, I remember talking about face transitions then and which, which is the same as, as this notion of sharp threshold, like different fields, we'll call it different things like com torics and other places will say sharp thresholds. Other people will, you know, physics you'll say face transitions. But you know, it's like ice turning into water right. It's a very sharp change in, in some sort of macroscopic property that averages over all sort of participants, particles, whatever. I, I think like people didn't have as much of an appreciation for this until maybe like 2020, 2019, when like people actually were writing these and code and like the network Testnet would always keep falling over and they couldn't explain why.

Anna Rose (00:40:44):
I kind of wanna go back to this idea though, that a lot of different people were looking at these problems. Would it be that the, the way that they defined the security would actually all like that would then define the type of protocol that they would develop, but they might not be looking at all of the axis, like who's to say.

Sreeram Kannan (00:41:01):
Yeah that's exactly. That's exactly correct.

Anna Rose (00:41:02):
But who's to say that, like, why was your, what a, like the axis you were looking at, were they broader, better? Like, why would you also have the same kind of blind spots in the work you were doing? I guess no, no researchers should should say yes,

Sreeram Kannan (00:41:17):
No, no, absolutely. Absolutely. Right. Like self-doubt is very good if you're, if you're a researcher. And so here is here is one thing one reason why, at least what we were looking at is to summarize what other people were looking at. They were mainly focusing on because Satoshi Nakamoto looked at only the private attack, which is I grow a chain in private and then I tried to overtake your public chain. Everybody else thought that that is enough. So everybody else were building protocols, which were fundamentally secure against that at attack. But what it turned out is what Satoshi Nakamoto did is uniquely correct in that, that attack was the absolute worst thing that could happen for that protocol, but not at all true for most of the protocols that people were designing. So, so that's why, you know, the paper is called. Everything is a race and Nakamoto always wins, because there's something like absolutely stunning. I I'll give you another example, which, which which is also a line of work we did. And, you know, actually pioneered by Aggelos Kiayias and the people who did the Bitcoin backbone protocol. So one of the things going on in Bitcoin is variable difficulty, which means the difficulty gets adjusted every 10,024 blocks. So people are not looking at what is the longest chain, but what is the most difficult chain or what, in which chain has most work accrued. So if you look at the mining power in Bitcoin, right. When it started, and now it has gone up like 10 to 13 times or something, some absolutely insane time. So even after all of this, the system works and is fully secure because the system keeps recalibrating the difficulty that is required to make a block. So blocks are made roughly every 10 minutes and the system every 10,024 blocks has a recalibration. Okay. So the, this is called difficulty adjustment. Instead of following the longest chain, you follow the most difficult chain or the most worked chain. Okay. And this was already kind of thought in until this point it's intuitive. Okay. You should follow the most difficult chain that seems like. Correct. Okay. And then it turns out that there is an attack on this system. If you to find it like this, and the attack is, again, let's bring Tarun as an attacker. Tarun comes in and he has 25% mining power, and Tarun wants to overtake the longest chain. So what does he do? What he does is he says, I'm going to raise my own difficulty. How do you raise, does Tarun raise his own difficulty by creating fake timestamps? You can artificially, you know, show that blocks are arriving too frequently in his, his chain. And because blocks are arriving too frequently, the algorithm will say, you should increase your difficulty. You should increase your difficulty. And if he keeps increasing difficulty, then what he can do is if his difficulty is so much higher, that one block that Tarun makes can overtake an entire chain that the honest guys are making. So what this does is there was a law of large numbers, which said that by the time the honest guys make thousand blocks, they have 80% mining power. So they're going to be much ahead of Tarun who only has 20% mining power. Okay. But now he has claimed back his fair odds. His far odds was like, he should win 20% of the time by saying that one block he makes, which is a highly stochastic event can completely take over the chain. So he's, instead of saying that he has to make 1000 blocks, he's just making one block. So he doesn't operate with the law of large numbers. He's operating in a random regime. So the law of large numbers is, is what makes things deterministic or roughly deterministic. And Tarun is by raising his difficulty. He's bringing it back into the random regime where he, his chance of winning is now 25%. That's completely unacceptable to run a system, but you can't execute this. Tarun can't execute this attack on Bitcoin because Satoshi Nakamoto had an absolutely brilliant fix to it. I don't know if they understood it or not, but there was this fix, which is basically difficulty will not be raised by arbitrary amounts, but it can only raise two X or four X, every interval. And, that just makes everything completely different. It's one of the most amazing things that I found that, you know, if just go ahead and design a protocol, you would absolutely not think of anything like this. This is an insane kind of attack. And, you know, and the fact that it has withstood attacks like this speaks a lot to the amount of engineering insight that went in into the design of this protocol.

Anna Rose (00:46:09):
So far, I feel like a lot of what we've been talking to about is very, very proof of work focused, but I know that your work also starts to touch on proof of stake. Does that change any of this? Like yeah. I'm, I'm just curious about, like, where did you evolve into almost like, where did you start on the proof of stake train too? Cuz that's a whole other kind of realm of research.

Sreeram Kannan (00:46:31):
That's right. So all, all of the stuff was proof of work. And this was 2018 when we started and end of 2018, there was too much buzz about how this proof of stake is going to be the real thing. And, you know, proof of work is gonna go away and all of this stuff and having studied the longest chain so deeply our natural impetus was to actually follow the train of work in proof of stake, which worked based on longest chain. Much later, again, going back to the taxonomist hat, we synthesized the space much more clearly by kind of understanding what the fundamental like trade offs are when you design a proof of stake protocol. So I'll, I'll start with that. So then we can delve into what the longest chain offers and doesn't offer. So fundamentally there are two different kinds of proof stake protocols. Looking at it practically, you will say that there are longest chain protocols, and then there are BFT protocols, but fundamentally what we reverse engineer and other people like Tim Roughgarden and, and others have also pointed this out. That what is actually going on is there is a fundamental tension or a trade off between two sides of algorithms. One class of algorithms offers you availability. What does availability mean? Availably or dynamic availability? Dynamic availability means even when a small fraction of participants are present, the protocol will remain and continue to make progress. The protocol continues to be running even when only a small fraction of participants are present. So these are called dynamically available protocols.

Anna Rose (00:48:10):
Would this be like when there's like 30% active or like what what's the...

Sreeram Kannan (00:48:13):
Yeah, 30% or even 3% active. The protocol needs to continue to make progress. For example, Bitcoin of course continues to make progress. Even if the mining power, like the China thing happened, you know, miners moved out of China and Bitcoin is running effortlessly because it's a completely dynamic available protocol as the mining power increases or decreases. There is no denominator on what the world's mining power is. So nobody knows what to normalize it by. And so it's self normalizing, so it just keeps adjusting itself and moving on. So we call these family of protocols, dynamically available protocols.

Anna Rose (00:48:47):
If that's one side of a spectrum, is its sort of like, are there some POS chains which are more dynamic have like how does that work actually like they have a lower threshold needed in order to remain dynamically available or something?

Sreeram Kannan (00:49:01):
Right. That's right. So there are two, two, absolutely like it sets at the very core of when you design a proof of stake protocol, do you want to be dynamically available or do you want to be finalizing? Okay. So that's the dilemma is either you're dynamically available or you are finalizing. What is a finalizing protocol is you get safety, even, even when the network is asynchronous. When you know, some nodes are when the network latency is really, really high, you still have the system to be safe. So those are called finalizing protocols. Usually the class of BFT protocols, asynchronous BFT protocols will fall under finalizing, which is, you know, Tendermint, which is HotStuff. So I'll give you examples of real world blockchains, which took one or two, one side of the fork or the other side Aggelos and others who were pioneered the longest chain, Bitcoin backbone protocol. And, they went on to build a whole family of probably secure proof of stake protocols called Ouroboros underpinned. Much of the rigorous longest chain protocols. Cardano is one of the, the bigger ones following that, but also Polkadot uses, longest chain protocols. There are smaller blockchains, which use proof of stake, which use these longest chain protocols. On the other side of the spectrum are protocols, which are finalizing Dapper labs, flow, the flow blockchain uses HotStuff, which is a finalizing protocol. Tendermint cosmos is a finalizing protocol. Ah, and there are others like Ethereum 2, which want to be both.

Anna Rose (00:50:41):
Okay. Where does Ava fall in that? I just had a,

Sreeram Kannan (00:50:43):
Ava was more on the finalizing side.

Anna Rose (00:50:45):
Finalizing side. So it it's closer to HotStuff, Flow, Tendermint.

Sreeram Kannan (00:50:47):
That's right.

Tarun (00:50:50):
But with, with, with some weaker guarantees, right? Like it, it takes more liberty with like its voting mechanisms. So there's more variance in like the expected latency for instance, and stuff like that. But it also means you get to use fewer resources and potentially lower communication complexity. So it anyway, maybe that's like too much detail, but I feel like they're kind of in the middle they're, they have some things that are a little bit unfortunate.

Anna Rose (00:51:15):
So I wanna just like state the, the, the sort of spectrum again. So it's from the, do we say this it's like dynamically available or finalizing and there's sort of on either side. I, I had an interview some time ago that I've cited a few times with Etay Abraham and there he mentioned synchronous and asynchronous. Are those the equivalent terms or those different.

Sreeram Kannan (00:51:39):
Yeah they're related but not equivalent. Okay. So synchronous means network network has a guaranteed time to deliver all messages. You're making a very strong assumption on the network that it will deliver all messages within a certain synchrony bound, which is a, a latency let's say like in 10 seconds, all messages will be delivered a protocol, which makes such an assumption will be called a synchronous protocol. An asynchronous protocol is one that will not make such an assumption. The family of asynchronous BFT protocols would be finalizing because you know, they are you cannot have a protocol which is safe on asynchrony and also available because you don't know whether you got enough messages to make progress, right. Because when you say, oh, only 10% of the nodes have signed and still gonna go, go ahead and make progress. You don't know is it, because the remaining 90% were honest nodes and they were trying to sign something and their messages got delayed. How are you gonna go make progress? You need synchrony in order to get dynamic availability. So that's the, that's the thing. But I think as a fundamental property for the end user operational property, the much better to think about, are we dynamically available? Are, are we finalizing? Okay. And, and the benefit of dynamic availability is obvious as users come and go, you know, it's a come as you please kind of like a model. And the benefit of finality is also there, which is if somebody does something wrong, you can penalize them because you, it is usually based on signatures when you know, the committee or size or whatever, they sign something. And if you double sign something, then you can be penalized. So, so the, the taxonomist us is basically trying to categorize the space and say, our protocols falling this space, our protocols falling in that space. Is it possible that a single protocol can be both? That was another question, actually, this is another one of those things, you know, in academics, actually David Tse and his students started working on this and Pramod and I started working on this simultaneously independently. And we were both working on exactly the same problem. Can you have a single protocol that is both finalizing and dynamically available? And there are some basic theorems which tell you that it's not possible. It's called the cap theorem. And I think Tarun alluded to that. So the cap theorems says you can't be both in, in modeled in this blockchain language. It says you can't be both dynamically available and finalizing.

Anna Rose (00:54:09):
I wanted to ask you since, since you had just mapped out where certain protocols fall, I had a few others. I kind of wanted to ask you about, yes, please. Where does Solana fall? I never know what it is. Is it Tendermint?

Tarun (00:54:22):
No, no. It's, it's just PBFT. What, I'm the proof faster thing is kind of like a, a heuristic. I wouldn't call it like, you know

Sreeram Kannan (00:54:30):
Yeah, so it's, it's basically a, a finalizing protocol with some protection against this thing called the long range attack. Okay. So, and, and I'll come to that when we explain some of our work in, in that space, basically. So if you look at the tree of all protocols, you have proof work and then you proof of stake. In proof of stake, you have either dynamically available or finalizing. So, by the way, you can't have a finalizing proof of work protocol because you never know what the size is. That is enough to finalize something. Because you, the total amount of mining power in the world is unknown and unbounded. So you can't have a finalizing proof of work protocols, proof of work protocols are all dynamically available. On the other side, you proof of stake protocols and proof of stake protocols could either be dynamically available or finalizing, but, you know, one difference between proof of work and proof of stake is proof of work protocols do not have the same level of long range attacks, which is, is history, reversion attacks that are quite possible in proof of stake protocols. So that, that's another like meta thing. And Solana has some defense against this kind of long range attack using this proof of history.

Tarun (00:55:40):
But, but it is heuristic. It's not like using a VDF where you have a guarantee. It's, it's, it's much better from an standpoint than a VDF, but it is that's right. It, you can't really prove that it works. It, it has, it makes a lot of weird sort of assumptions when it's used.

Anna Rose (00:55:57):
Hmm. Let's now take that step towards answering the question. Can it be both dynamically available and finalizing?

Sreeram Kannan (00:56:07):
I, I actually, we, we, it was not our original idea. It was actually Vitalik's original idea. For trying to figure out if you can get both and the way he was thinking about it, you know, I don't know if people remember back in 2018, there was this thing called Casper. Casper was supposed to be something built on top of top of Ethereum proof of work, which gives finality. So this was called Casper the final. The friendly finality gadget. Exactly. And, and we looked at it. I said, man, he's trying to have his cake and eat it too. Is it possible or not possible? Or is there like some trade off? They were not complete proofs for these things. You know, there were hints that this may work may not work what's going on. Okay. And actually the, the Casper is kind of built into Ethereum 2, now called Gasper. It's a combination of Ghost and Casper called Gasper. And I, I think it's not called Ethereum 2.0 anymore of yesterday, but foundation, I think the Ethreum foundation made an announcement.

Anna Rose (00:57:08):
Okay. Yeah. I have heard before that, that they were distancing themselves from the word from the name.

Sreeram Kannan (00:57:15):
Yeah. No, I think it's called just consensus update or consensus layer or something, but the consensus update, which is basically called Gasper, what happened was David Tse found that there are many attacks on Gasper. So this was a paper they wrote early this year and early in 2021 that got a lot of attention. And so David's working with Ethereum Foundation on how to fix several of those things.

Anna Rose (00:57:42):
We actually, we did an, so I think Tarun you and I did this like years ago or two years ago, maybe or one year ago. I think it was like the beginning of the pandemic. Yeah. I think it, and it was with one of the co-authors of Gasper. Yes. We did an episode on Gasper. So I'll try to, I'll actually try to dig that up if people are curious, but I didn't, I haven't since heard about this, this work finding problems.

Sreeram Kannan (00:58:05):
Yeah. So the thing is Gasper was trying to get both basically the way Ethereum 2 works is there's kind of like a longest chain, but people eventually vote on the chain enough and then that becomes final. So it's kind of like having the features of both longest chain and finalizing, so that that's the basic class of things. And the fundamental questions we as is can you be both finalizing and dynamically available? And the way you can be both is you have a common underlying protocol, but different ways to confirm things inside that protocol. So Anna, you are a very, let's say conservative person. You wanna only make very safe claims. Let's say you're running a bank or whatever. So have a common underlying protocol. And you'll only accept somebody's payment if it has gone through a very safe rule, whereas Tarun is running a gaming engine and he's like saying, Hey, you know, let's get things moving, man.

Sreeram Kannan (00:58:59):
So what Tarun is doing is it's the same underlying protocol, but he's coming at it and applying a different confirmation rule on the protocol, which keeps the system running live. And the, the theorems that, you know, David showed and we showed in separate papers, by the way, the end of the story was David beat us to the race. Oh. And we wrote our paper like two weeks later with Pramod Viswanath and Surya. And we ended up basically bringing more properties to this kind of a protocol. Basically, can we build a protocol which is both dynamically available and finalizing fundamental blockchain is run by the same protocol. Anna can use a different confirmation rule and Tarun can use a different confirmation rule.

Anna Rose (00:59:44):
At the same, but at the same time, like, can you do at the same...

Sreeram Kannan (00:59:47):
At the same time. And so you are, so the safe rule will always be lagging the live rule because you know, you are only making things that have been like signed off very, very safe,

Anna Rose (00:59:59):
Very finalized somehow.

Sreeram Kannan (01:00:00):
Yeah. Very finalize. So the, the way you can think of it is already in Bitcoin, you know, you can use a confirmation rule, which is like 10 blocks deep. Yeah. And Tarun can use a confirmation rule, which is only one block deep. Okay. And so it's, it's like that, but much more complex and much more general. And so that's the space of things we figured out. And so of course now David is working with the Ethereum Foundation, trying to understand how you can make sure the pro protocol has both properties with different confirmation.

Anna Rose (01:00:28):
What was that work? You just mentioned when you said you released this paper, but later.

Sreeram Kannan (01:00:32):
Yeah. So the paper is called Blockchain CAP Theorem Allows User-Dependent Adaptivity and Finality. It's basically user dependent, adaptivity, and finality. So either you are adaptive, which is dynamically available or finalizing depends on the user, but the protocol itself is the same underlying engine. And actually other blockchains have also been thinking about this. Polkadot had something called grandpa, which is their finality gadget. And so we kind of integrated all of this into like a common picture. Which is again, the taxonomist.

Tarun (01:01:06):
One, one thing that's also interesting to note here is that a lot of layer 2s effectively offer this sort of similar promise of you choose what level of quality of service you want. And so you probably can generalize this to some of the layer 2s although of course you need to make different latency assumptions, cuz there's two different chains and sequencers and stuff like, but, but like you could argue that layer twos were already trying to go in this direction, but like didn't know why they were doing it other than it was easy engineering wise.

Anna Rose (01:01:36):
I think it was because it was so slow to actually write the main chain. Right. You needed a faster approach.

Tarun (01:01:41):
Well I guess I meant in terms of like their security guarantees, right? Like how, how people design layer twos based on security guarantees is still quite heuristic, right? There's no, there hasn't been the Bitcoin backbone protocol paper for layer 2s in a lot of ways, a lot of layer twos are driven by engineering needs and not like mathematical soundness needs, which is why, you know, I think they've been somewhat hard to, to production so far.

Sreeram Kannan (01:02:07):
Absolutely. I think so here I've, you know, at this point will also ask something Anna raised earlier, which is what are you not doing? You know, you're saying, you know, you're modeling all the adversity thing and all this, what are you not doing? What are you sneaking under the carpet? I'll tell you exactly what we know. We are sneaking under the carpet. We may not know other things. What we do know is fundamentally when you try to mathematically model these complex blockchain systems, again, there is, there are two fundamentally different views. One view says, I'm gonna ssume that X fraction of nodes are protocol obeying. You can call them honest. But I think maybe the right thing to call them is altruist. They'll just do, or you can say obedient if you wanna appear differently. But basically these nodes just simply tow the party line.

Sreeram Kannan (01:02:55):
Basically they're gonna follow whatever the protocol, founder, whatever says they'll just run that particular instance of the node. And there's a fraction of nodes, which are non protocol following and they can deviate arbitrarily. They can do whatever they want. So this is one kind of model we can think of this as an adversarial model of blockchain, some group of nodes are adversarial. The remaining are altruists. They call honest, but basically are interest. They're just doing what they're told and they're doing it because it's good for the network. They're doing it because they're, they're already happy with the revenue. They're getting whatever we don't care. We don't model why some nodes are honest and some nodes are advers. We just plop them into those groups and say, either you're honest or you're adversarial. Okay. So that's one kind of modeling. There is a completely different kind of modeling, which is saying nodes are rational.

Sreeram Kannan (01:03:47):
Okay. Now, when you say nodes are rational, they're trying to maximize kind of some utility, which is how much maybe money they're making. It's it maybe factors in some of their real world reputation, we don't know what a utility function is. So the problem with the second class is it's much, much more difficult to state things, very sharply, everything that you state is under somewhat weak models, because it's very difficult to model the full space of what utilities are and what rationality is. So what we had done for several years let's say 2018 to 2021 is completely focus on the honest versus adversity or altruist versus adversity models. But over the last one year, or I've been thinking a lot about the incentives, which seem to drive a lot of user behavior, that's a fundamental disclaimer. The fundamental disclaimer is when you see a paper and it says, Hey, you know, some nodes are honest, some nodes are adversarial.

Sreeram Kannan (01:04:51):
Okay, it's good. In some sense, because it, you can, other, the adversaries can do arbitrarily bad things and still it's gonna work. But the honest nodes have to absolutely follow protocol. and then you can model it rationally, for example Emin Gn Sirer and Ittay Eyal their selfish mining attack is basically rational deviation, right? It's not a honest adversity thing. It's basically I'm gonna maximize my like revenue source. So when you look at a mathematical model you're to first calibrate is the assumptions, what you want or will these assumptions hold, are there reasons promoting the holding of these reason, all of these assumptions or not. And so that, that's a, that's a basic fork in the model link choice.

Anna Rose (01:05:37):
When you talk about this rational though, like, is there not still sort of the like irrational following their orders, honest in that, like, do you really believe that all will act purely based on incentives?

Sreeram Kannan (01:05:53):
No, no, no. I'm not. I'm not, not at all saying that everybody's incentive to and also incentive have a meta layer, which is super hard to model. And the, the metal layer may be the following in Bitcoin, there is a very easy, there is a very easy bribing attack, but if you execute this bribing attack, you may collapse the value of Bitcoin measured in Bitcoin. Maybe you make more money, but measured in US dollar, you make less us money. Yeah. So there are all these meta things that drive into incentives, which are very hard to formally model. So I'm, I'm not at all suggesting that this is the absolutely correct modeling or anything like that. But the reality is there are byzantine nodes who don't care, they just wanna take your, maybe there are a competing protocol they're coming and shilling onto twitter, Hey, your protocol is, is bad and I'm gonna take it out. So there are byzantine nodes. There are altruist nodes and there are rational nodes. So the fundamental correct overarching mathematical paradigm will be what I'll call a BAR model, Byzantine, Altruist, Rational, all mixing to the other in like one big party and trying to characterize what fraction of byzantine, what fraction of altruist, what fraction of rational can your system endure? And can you really slug it out? This was actually our original formulation when, when David and promo and I were trying to formulate the mathematical problem of blockchain. Oh, but we said, this is way too complex for our heads and the maths to handle simple it down to just, let's say there is byzantine, can we solve it. Now let's say there is rational. Can we solve it? Let's combine everything together. Can we solve it? So, and, and we see this kind of playing out in, in practice in like various ways, for example, Ethereum, one of the, the main things that Ethereum 2 now called just a Consensus layer is thinking through is the crypto economics and crypto economics to me is just how to make sure it's rationally incentive, compatible to just follow what people are telling you to follow. And can you make it more and more compatible to actually follow it? Maybe if you attack, you're gonna get slashed. This is one of the basic principles that Casper introduced and like many others followed suit. The idea of slashing, which is that if you deviate, you're gonna lose your money.

Anna Rose (01:08:12):
And that's the economic game there...

Sreeram Kannan (01:08:13):
That's the economic game there. Exactly. And, by the way, you know, even though in theory, people are talking about all this, I don't know of any real large scale protocol that actually does slashing.

Anna Rose (01:08:24):
It has appened. I mean, in Cosmos, it definitely happened. I remember that somebody got slashed, but it was usually accidental. That's the problem, it wasn't really, yeah. So it, like,

Sreeram Kannan (01:08:33):
That's the reason that people are worried when they wanna bring slashing. Is, are you getting slash for a real reason? Or, you know, it's just some kind of like discordance in the version you're running and you can get slashed. So absolutely. That's the risk with building slashing. And so it's gonna take time before, like slashing becomes like an automated thing rather than, you know, it's it's a social consensus thing that if somebody gets slash I can go and unslash them because, you know, they were all just honest nodes out of sync.

Tarun (01:09:03):
Yeah. Yeah. I mean, I mean, so people do a variety of very weird kind of heuristics avoid slashing right now, for instance, I think in Solana, they have this like thing where they keep track of like validators statistics. And I think it's actually like central. I'm not actually sure who, how they get collected and disseminated, but there's some notion of like what the distribution of validator times is. And then you kind of get penalized based on like how far you are in that distribution. In general. Yeah. Designing the slashing conditions are, is really hard because like, at some level, right, it has to be just as dynamic as like the fastest type of attack in your system. But like the system is trying to also slow things down. And so like, it becomes this like very like multi timescale optimization problem, which sucks

Sreeram Kannan (01:09:50):
Absolutely. I think we've, we observed this kind of central difficulty in slashing and the, I is this, you know, slashing is applied for safety failures when you have two blocks, you know, which are simultaneously confirmed. Actually we have a paper on, when can you slash this paper is called BFT Protocol Forensics. This appeared in this CCS and was selected as a best paper runner runner up. But the, the core idea here is the, when can a protocol have slashing. And what we found is that people normally think, you know, if you have enough signatures and people double sign, you can slash what we found is that's not absolutely true because there are scenarios where you can create safety failures, two things getting confirmed without actually double signing. And for example, Algorand is not slashable fundamentally the way they have designed it, you even though it's, it has signatures and quorum and all this it's notable. So looking deeply into the mechanics of what makes a protocol slashable is another line of work that we kind of embarked upon in this, in this taxonomy of consensus protocols.

Tarun (01:11:03):
So, so like, I, I like to view a lot of this as, as like the natural maturation of this space, right? Like if you think about things in 2017, 18, there was a lot of wild west. Like, Hey, I wrote this paper, I wrote some code, like, like as a prototype, like, Hey, it's gonna work put hundreds of millions of dollars into it. And I think, you know, at that time, everyone was like, the math doesn't matter. Like who cares, whatever the engineering matters more. And like, you know, there's always this interplay in this industry, I think in general, where like the engineering can oftentimes get ahead of the theory, but then the engineering hits a roadblock, as we're seeing with layer twos, where I actually think like the theory is actually gonna be very important for ensuring that they actually can get to production, unlike a lot of layer ones.

Tarun (01:11:48):
And, you know, there's some roadblock and then it takes a while for people from other fields, like provide theory. You know, like if you, some of the people mentioned earlier, like Kia, I can never say his last name correctly Kiayias, Kiayias thank you. And Elaine Shi itself, you know, they were kind of working in related fields, but like tangentially and then brought in you thoughts. And I think in general, you know, it's like each kind of wave sees like this improvement in theory. I think like the moral of the story is there's kind of this like meta meta game between the engineers and the a, like, you sort more mathematical researchers in the space competing over who's whose design actually works in prog better.

Sreeram Kannan (01:12:31):
Yeah. So, so just a thing the are, when we look at security, right, performance can be determined by engineering. You just run things and check, you know, you, you know, if Solana is running 120,000 transactions per second, or not by just running it, but you don't know if Solana is secure, or if you don't know, Ethereum is secure by running it because the space of all possible attacks is simply too mind boggling for like people to even realize this is the basic thing in cryptography was, you know, you cannot just say that your a signature scheme or like a zero knowledge scheme is secular because nobody has attacked it yet. Instead you rely on very basic axiomatic assumptions saying, okay, calculating the discrete algorithm is hard. Therefore, you know, this scheme is secure. So this principle somehow did not yet fully manifest itself in the blockchain world because protocols seemed like engineering things. They are engineering in one dimension, but the dimension that they are engineering is just performance. There's no, you know, to prove that one thing is more performant than other is not that useful. I mean, it's useful to some extent to understand the taxonomy and principles, but I'm running it and I'm actually getting better performance. Nobody cares about your theory in some sense, but to prove that something is secure, you cannot just run it and show, you know, what did you do? You know, did, did your friends attack it? Did your enemies attack it? Did you put a hundred billion dollar on it and let it sit for 10 years? And did you move the entire wall street into your application and see if it's secure, it's not possible, right? It's simply not possible to do any of this. You have to have mathematical methods to think through security in a way that you do not necessarily need for performance. So when people come in from an applied systems view and say, Hey, I've built this system. It's great in engineering. We can absolutely give them credence for building high performance systems, but whether their system is secure has to be saved through a theoretical lens. And here, I, I wanna give like a big shout out to like Aggelos, Elaine Shi, Rafael Pass all of these people who broaden this lens, you know, and, and in particular, this was a very important

Tarun (01:14:52):
They were like cowboy academics, you know, at that time, a lot of their peers were like, you're going into this scam thing.

Sreeram Kannan (01:14:59):
Yeah, yeah, exactly. And, they stood there and wrote paper after paper, after paper - sieving through the, you know, core principles in this. So it's absolutely amazing.

Anna Rose (01:15:12):
As you're analyzing a lot of this work. I mean, I think a question that inevitably will come up is what happens when you think about MEV like this other vertical, this other, I mean, maybe it was taken into account and I missed it, but I feel like it hasn't been fully kind of chat, like brought into any of your modeling. And obviously it's a very big topic right now. So what, what are you thinking around that? What work are you doing?

Sreeram Kannan (01:15:38):
Absolutely. So when we started seeing this whole MEV thing explode, I remember first time, I think Ari Juels was giving a talk on flashbots 2.0 how these things were happening and how they blogged. And more people started just like front running and gas, you know, arbitrage, bots, all of this stuff. And thinking that this, there must be like a fundamental mathematical problem underpinning why MEV is happening. And one way to think about MEV is it's really an incentive thing, right? If you are a miner, you are obviously going to prioritize transactions that pay you more fees and put them ahead than transactions that don't pay you that much. So that leads to this whole idea that you can front run transactions by just increasing the fee that you're paying the miner. Okay. This is an incentive driven way of thinking about it. And of course, we were much more familiar or deeply embedded in the honest and adversarial way of thinking about it. And the, the adversarial way of characterizing this problem is the following saying that if you are an adversity and you are mining a block, you can unilaterally determine the order of transactions in this block. If you're just running the geth node, you're going order transactions in a certain way. But if you're running some other version of the node, you could order transactions in the way that you want. So that's an adversarial behavior. And the point is anybody can mine blocks and an adversary can mine bloks and their mined bloks will have a very different order. So then we went back to the basics of like distributed systems, and we see that there are really two pillars of distributed systems. Safety to security. One is called safety, which says that a confirmed transaction should not be reverted. Another is liveness, which is a honest action should eventually get confirmed. But what we are looking at here is a fundamentally third dimension. And the dimension is, is the order of transactions in the final ledger. How is that related to the order of transactions entering the network? So it's a concordance property between transactions flooding into the network and transactions entering into the ledger. So people had thought about it a little, but no deep work had gone on to create systems which give strong guarantees on these on this concordance between the arrived order and the finalized order.

Anna Rose (01:18:10):
And would you say like high concurrence, or, I don't know if I'm saying that word. Right. But like, if it's high in that dimension, does that mean that it's more similar? So there's less kind of like funny business happening in the MEV camp?

Sreeram Kannan (01:18:21):
Exactly, exactly. Right. Imagine. So, in fact, this is one of the big properties that a centralized exchange office today, like a New York stock change will take enormous efforts to make sure that transactions are processed in the order in which they're arrived. Going so far as trying to synchronize their servers across different cities to millisecond, resolutions, and so on. Okay. So this ordering fair ordering is also a mandated property by something like the SEC, because you wanna make sure that the big guy who can pay for order or does not outrun the small guys who cannot pay for order. Yeah. And we were seeing, you know, ripples of this, in things like Robinhood, doing auto flow, things like that.

Anna Rose (01:19:09):
I'm just realizing like early stock market probably had like the MEV bot equivalent, which was like the person who ran the node from one place.

Tarun (01:19:18):
And I kind of all still happens. That was, you know, I having worked in HFT it's the same thing. It's just that it's more competitive on a hardware level than it was before.

Anna Rose (01:19:28):
I'm picturing, like, when orders were like written on papers and like run across the room, I just wanna like, have a vision of what a, like MEV bot would look like if they were corporeal and I'm like, picturing it's like, that's why I'm saying like early, early, like pre-digital, but yeah, I guess it's, it's something that's always been there.

Sreeram Kannan (01:19:49):
It's always that, but I, once it arrives at the exchange, there is no unfair ordering, but to arrive at the exchange, you can always kind of try to get things close to the exchange or whatever. Okay. But coming back to the blockchain, I think the guarantees offered by today's blockchains are far worse than the guarantees offered by a centralized exchange. Yeah. Because, because you can front-run just by like putting more transaction fee and that's just like a zero bar on like what is needed to do front running. So what happens in this world is if you're doing large trades, you can put in a higher transaction fee. If you're small trades, you cannot put a higher transaction fee. So you're obviously going to get front-run. And to me, this goes against the spirit of decentralized finance. What are we decentralizing if we cannot offer at least some protections for people whose orders are small?

Sreeram Kannan (01:20:43):
Okay. So what we are saying is can we create a system which does fair ordering and fair ordering means if the timing in which transactions entered into the system is something, then those transactions, if a transaction, A arrived into the system earlier than transaction B, and you have to define it and so on, then it should appear ahead. And in fact, our, our work shows is that on both types of protocol, longest chain protocols, as well as confirming protocols, finalizing protocols, like BFT you can actually modify the protocols to have very little overhead, but still have fair ordering. So that's the line of work.

Tarun (01:21:26):
Although one thing, one, one caveat here, right. Still is in the Themis result. You still have some level of trust in this aggregator where there's like some it's like somewhat permissioned, right?

Sreeram Kannan (01:21:38):
Oh yeah. So, so suggest, give a, to, to give a quick overview of what that is. So normally in these BFT protocols, there's a leader and the leader makes a block and send it to everybody. Instead, the leader does not make a block or is not supposed to make a block unilaterally. Instead, the leader has to collect everybody's viewed orders because, you know, the order arrivals are different at different nodes. It's a distributed system. So what the leader has to do is to collect all the different people's orders of different transactions, and then come up with an aggregate order. And that's what they have to put it on the, on the chain. And if they did something wrong, it's a slashable offense because anybody can challenge them. And then if they don't show it, then they can get slashed. So the point that Tarun is mentioning is of course, now it's still dependent on one node, but if the node does do something wrong, they can get slashed.

Sreeram Kannan (01:22:31):
And what you can do is you can increase the scale of slashing by saying that not just one node needs to sign on it by 10 nodes need to sign on it. So 10X, the collateral can be slashed. If you want the entire network to be slashed, you can say that everybody needs to sign on it. So there's a kind of natural trade off between complexity, which is many people need to sign on stuff versus like the guarantees that you're deriving out of it, or the, the slashing that you wanna derive out of it. So that's the finalizing protocol work called Themis.

Anna Rose (01:23:02):
Themis. That was the, that was what I was looking for. What's the name of the Themis?

Sreeram Kannan (01:23:05):
That's called Themis and Themis is build on top of some of the work done by Ari Juels and a student Mahimna Kelkar earlier in collaboration with them, my student Soubhik and I worked with them to actually come up with this protocol called Themis which is a way more efficient. They'd already come up with a basic principles of what it means that, that protocol was called Equias Equias is a BFT protocol which is offers some kind of fair ordering. What we did is both improve the scale of fair ordering as well as reduce the complexity back to what it would've been without adding fair ordering. So that's one line of work. The other line of work is on the longest chains, and this is something also another thing happening in, in things like Ethereum two is short reorgs, which is, can miners kind of reorg the last few blocks and, you know, our protocol called Hammurabi, you know, going back to a ancient Babylon

Tarun (01:24:03):
Mean you have, you have Babylon, Themis and Hammurabi you're, you're just like you're really going

Anna Rose (01:24:10):
Archeology protocols.

Tarun (01:24:13):
It's kind of ironic to like name the newer protocols after older things like it's like you're going backwards in time to make future things, you know, further back.

Sreeram Kannan (01:24:24):
No, but, but I think some of these things actually organized and created law and order and organization and society at that time and potentially newer protocols

Tarun (01:24:35):
That's an alpha leak. I don't think anyone ever explained to me your choice of paper names until this moment.

Sreeram Kannan (01:24:44):
Absolutely. So we all also have longest chain protocols where you do not have a miner make ordering unilaterally. The ordering comes out of an emergent understanding or a consensus among many people presenting their views of how the order should be and then that gets synthesized into the finalized ordering. So that's the line of work we have on this direction.

Anna Rose (01:25:11):
All of this stuff, like in just an interview, I just did recently, it was actually talking to a team building the bots that like exploits MEV. And I'm wondering like, you know, hear you talk mainly about the miners and like the work for the miner, but like, do you actually remove the ability of these bots to use strategies to actually get something? Maybe it's not all the attacks, but are there still some like attacks that get through?

Sreeram Kannan (01:25:39):
Absolutely. I think the scope of attacks is something, you know, if you, you can broadly categorize front running as targeted front running with is, you know, I wanna kind of sandwich your transaction, Anna and, you know, extract some MEV from it that I would call a targeted front running. And then you can say, I wanna do non-targeted front running. Imagine the price fall of Bitcoin, like 2, 3 days back. What I could do is I say, I don't, I don't want to particularly front run anybody. I just want my transaction to go before other people's transaction. Because I know the price is crashing. I just wanna get out. So there are a scope of methods which can defend against targeted front running, basically, by saying that I obfuscate or just send a commitment of what I'm going to send and then reveal it.

Anna Rose (01:26:32):
This is like the threshold decryption.

Sreeram Kannan (01:26:35):
Yeah. This is exactly threshold encryption. Absolutely. So that'll solve targeted front running because I don't know your transaction to target it. It still, there is a massive scope of non-targeted front running, which is prices crashing. I wanna take all the price advantage. I'm a big trader. I'm gonna put up a big transaction fee and go ahead of all other small traders, even though they actually sold ahead of me. This is basically like creating an insight system mechanism for auto flow, which, you know people don't like about Robinhood and Citadel and all these guys coming together. But what we feel is that the protocol should offer the absolute strongest guarantees on timing. Yeah. The protocol should offer the best standards of timing on top of it. When you build your DeFi applications, you could absolutely do things like you don't respond to each transaction separately. You batch them up because, you know, you don't want just because, you know, thought and sent an order early. I don't wanna give him like an advantage. I'm gonna batch things at a slower resolution. In fact, this has been suggested over and over for things like the stock exchange. And you could absolutely built it. Once you have a fair ordered system, you could build these batching things at a higher layer, but you have a non-fair ordered system. You can never retrieve the people who got screwed in the unfair ordering.

Anna Rose (01:27:59):
So Sreeram, I know that you have a lot of work, a lot of papers. Lot of I like kind of various works happening and we probably won't be able to, you know, get everything into this episode. We're really at time at this point, but I'd love to hear what are you working on right now? Like after all of this work behind you, like, what is, what are the focuses? Maybe you can highlight a few things and maybe we can bring you back some time to go deeper on these.

Sreeram Kannan (01:28:27):
Absolutely. So, one thing is you, when we talked about slashing, you know, how do you build a system? You know, even more basically you think about proof of stake on the one hand has certain kinds of properties, proof of work has some kinds of properties. Can you build a system which is better than both? So this is a question that we've been kind of rooming over. And one particular direction that's culminated in is a protocol called Babylon, which lets you derive security from Bitcoin to protect yourself, you know, your proof of stake network against long range attacks. So suppose you wanna build a Cosmos chain, but you wanna also be resilient to like long range attacks. You also wanna be resilient to let's say your own token market cap is only like a hundred million, but you don't want your security to be bounded by your own token market cap. Instead you wanna borrow security from like a security powerhouse, Bitcoin, how can you do that? That is a theoretical project we did called Babylon and David Tse. Who's a professor at Stanford and my former advisor has taken it up and actually building a project around it.

Anna Rose (01:29:29):
Another archeology project, just a note.

Sreeram Kannan (01:29:31):
No actually building it. Engineering project, actually, they're trying to, to borrow Bitcoin security. On the other hand I myself have been on leave over the last eight months, building a company called Layer Labs and Layer Labs is trying to solve a problem, which I goes, which goes fundamentally to the root of governance in blockchains. If you look at blockchains today, like Ethereum or any of the other blockchains, how does innovation get in and move the practical blockchain industry? It's very difficult for innovation to find a place in existing blockchains because blockchains upgrade through unanimous or near unanimous agreement. Which is a very rare thing in society. What do we all as a society agree on unanimously, it'll be almost nothing, but you know, blockchains have, have unanimous agreement or near unanimous agreement to get new innovation in. And this has bugged me a lot. Why? Because we come up with these cool algorithms. We don't want to necessarily start companies or anything. We just want to have other people adopted. If Bitcoin were a company. Absolutely. They would've adopted ideas like Prism inside it, but Bitcoin is not a company. And that is to its credit because it's a decentralized network. Yeah. So how do we build decentralized networks that do not have unanimous dynamics? We need to build decentralized networks, which have opt-in dynamics. For example, can we take the Ethereum network and say, oh, 20% of Ethereum stakers, like my new protocol or like my new idea, they can provide service on this. What does it mean? How do you build systems like that? I'm just trying to solve the problem that we faced as innovators in this space again and again. When you come up with an innovation, you have to build your own trust network. Let's say Solana comes up with a new virtual machine. Why do they have to build a new network? Why not just run the virtual machine on Ethereum? Because there is no way to run a new virtual machine on Ethereum. So this we face again and again and again. And I think this is massively slowing down the pace of innovation and blockchain because new innovation needs to build new trust. And by decoupling innovation and trust, you can actually, if you can get Ethereum, have opt-in dynamics, if you can have Solana have opt-in dynamics on new innovations, you completely change the equations of adoption. So that's what we are building in Layer Labs.

Anna Rose (01:31:56):
It also sounds like you're almost like rethinking the role, the agents that actually do a lot of this kind of consensus building, like the validators would potentially change or the miners, I guess in some cases like, would they dramatically be like a different beast in this?

Sreeram Kannan (01:32:12):
I, I think so. I think we have to give credence to the free will of these validators. So today say, oh, I'm everybody uses Ethereum virtual machine or whatever, you know, this consensus protocol because it's universally accepted by all of us is good, but why not? Each of these Ethereum staker also has a free will to say, I am putting my ETH at double jeopardy, not only providing this service, but also running like a new UV 3.0 or like, you know, a Solana C level virtual machine on top Ethereum. What does it mean? So basically addressing the, the scale of what is agency here in blockchains. Validator is not being passive. Yeah, no, we just like run the thing. You tell them to run instead having active agency in deciding their own risk reward trade-offs and potentially earning additional yield by actually providing much more furtive of services

Anna Rose (01:33:09):
Crazy. You also start to create almost like subsets in the chain itself, almost sub communities, sub applications. This is very interesting.

Sreeram Kannan (01:33:19):
This also goes something that Tarun asked about - L2s. How do you build L2s which have strong value alignment and full security of L1s is, is something I think we can address then we'll, we'll hopefully talk about all this at some point.

Anna Rose (01:33:32):
Yeah. So I think we're gonna have to invite you back maybe in the next few months to continue this conversation. We've, we've covered amazing ground so far, I think in this episode and a lot of your background and what led you to a lot of this work. And yeah, I guess you've kind of just teased the audience a little bit here with what we can talk about next.

Sreeram Kannan (01:33:51):
Absolutely. I'd love to come back. I really enjoyed the chat. Thanks Tarun and Anna for bringing me here.

Tarun (01:33:58):
Yeah. I mean, you know, I just want in a lot of ways, you know, know, I think a lot of the, there are a lot of engineers who listen to this podcast who, you know, find ideas or things to implement or, or things like that from listening to this. And maybe aren't so deep in the theory. So I think sometimes it's really good to give some exposition of what's been happening, especially since there haven't been conferences in two years or like really any conferences, like, like live conferences. So I feel like it's always good to hear, hear about these types things.

Sreeram Kannan (01:34:30):
Absolutely.

Anna Rose (01:34:31):
Yeah. Thank you so much for coming on the show. And I wanna say a big thank you to the podcast producer, Tanya, the podcast editor Henrik, and to our listeners. Thanks for listening.

