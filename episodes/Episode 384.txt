[Anna Rose] (0:05 - 0:21)
Welcome to Zero Knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralised web, as well as new paradigms that promise to change the way we interact and transact online.


[Anna Rose] (0:28 - 2:57)
This week, Tarun and I chat with Sora and Enrico from Machina iO, a team within the Ethereum Foundation. In this episode, we explore the details of their iO implementation work.


iO stands for indistinguishability obfuscation. It's a type of programme obfuscation where for any two equivalent programmes, once obfuscated would seem the same. They would be indistinguishable from one another.


It's very much in the realm of theoretical cryptography today. But as we learn in this episode, Machina is attempting to do some of the early implementation work around this technology.


This is our second episode on the topic of iO. And for a full explanation of what iO is, I would recommend listening to the episode with Rachel Lin.


In that episode, we go over what are the theoretical primitives, what are the assumptions, what is the state of the art, what have they tried and decided to move away from, what are they actually moving towards. And I think it gives a more comprehensive definition of what iO is.


I'll add the link in the show notes to this.


In this episode, we dive quite deep into the challenges and opportunities of this cutting-edge privacy cryptography.


Our takeaway remains that iO is still far from truly practical. However, as with these frontier technologies, having both attempts at implementation alongside the theory can offer interesting insights into how this actually works.


Now before we kick off, I just want to share that we've launched Season 3 of the ZK Whiteboard Sessions. The ZK Whiteboard Sessions are produced by ZK Hack in collaboration with Bain Capital Crypto. This series goes deep on some of the key concepts in ZK.


I've added a link in the show notes. Be sure to check out these videos. They are a great way to learn about the fundamentals and techniques powering the ZK systems we often discuss on this show.


Also, if you're looking to jump into ZK professionally, or just looking for a new role, I wanted to point you towards the ZK Jobs Board.


There you can find job postings from top teams working in ZK. And if you're a team looking to hire, you can also post your job there today. We've heard great things from teams who found their perfect hire through this platform, and we hope it can help you as well.


Find out more over at jobsboard.zeroknowledge.fm. You can find this on our website, and I've added it to the show notes.


Now, here is our episode with Sora and Enrico from Machina iO.


Today, Tarun and I are here with Sora and Enrico from Machina iO, a team within the Ethereum Foundation.


Welcome to the show, Sora and Enrico.


[Sora Suegami] (2:58 - 3:02)
Hi, everyone. Thank you for inviting us to this ZK Podcast.


[Enrico Bottazzi] (3:03 - 3:04)
Thanks, Sora. Thanks, Anna, for having us.


[Anna Rose] (3:05 - 3:07)
Cool.


Hey, Tarun. How are you?


[Tarun Chitra] (3:07 - 3:07)
Good.


[Anna Rose] (3:08 - 3:09)
Excited to jump back into iO?


[Tarun Chitra] (3:09 - 3:13)
Well, yeah. Somehow, I don't know how I became the person who's the co-host on all the iO episodes.


[Anna Rose] (3:13 - 3:14)
The iO guy.


[Tarun Chitra] (3:14 - 3:18)
But I guess that's somehow what's happened.


[Anna Rose] (3:19 - 4:21)
Nice. Yeah. So a few months ago, Tarun, you and I spoke with Rachel Lin from University of Washington. And in that, we discussed the state of theoretical iO.


It was one of my favourite episodes of the year, I think. I think I can say that now. iO had been teased on the show for years, but that was the first episode where we actually jumped into it.


And in that episode, we discussed how she and her colleagues had in 2021 proved that iO was at least theoretically possible if it was built on well-founded assumptions, which they kind of proved in their paper. I very much recommend listening to that one to the audience if you haven't already heard it.


Today, I feel like we're switching more into the realm of "practical" iO, but I'm putting practical in quotes here because as far as I understand, iO isn't usable yet. But I know that's what you guys have been trying to do, so I'm very excited to hear how it's going, how far you've gotten, what you think is possible, what still needs to be figured out.


Yeah. I'm very excited to learn more.


[Sora Suegami] (4:22 - 5:02)
Yeah. Okay. Okay. So I think as far as we know, most of the iO... all of the iO construction we know, including ours, is still far from practical.


So, for example, even obfuscating some circuit with just 4 input bits still takes more than 6 hours.


However, I don't feel we are still in the dark forest of the practical iO. Rather than that, we found some of the long but concrete paths toward practical iO. And we see practical iO as a concrete technical challenge rather than some miscellaneous like a dream.


[Anna Rose] (5:02 - 5:18)
Okay. I would love to get to know both of you before we jump into Machina and the project.


So why don't we start with you, Sora? Tell us a bit about what got you excited about iO. Yeah, when did you start working on this? Were you working on something else before? Have you ever done ZK or...? 


[Sora Suegami] (5:18 - 5:49)
Yeah. So my story with iO start from 2021. So, exactly from the paper from Professor Aayush Jain, or Rachel Lin, and Amit Sahai about the iO construction from standard well-founded assumptions.


So it starts from the WIRED article a friend of mine shared with me, and this article was exactly about this like a paper of iO from standard assumptions.


[Anna Rose] (5:50 - 5:52)
Wait, this was a WIRED article? Is that what you just said?


[Sora Suegami] (5:52 - 5:56)
Yeah. So after this paper was published, I think they are...


[Anna Rose] (5:56 - 5:57)
WIRED picked it up.


[Sora Suegami] (5:58 - 6:57)
Yeah. But they became the WIRED article, and I read that paper. And this was the first time for me to know the concept of the iO.


And at that time, I had already worked on some blockchain technologies and some ZKP application developments. And I knew some of the concept of using cryptography for smart contracts, like using MPC or FHE.


But I felt if we use such MPC or FHE, this data decreases the security of smart contract applications, because you additionally need to write on some ad hoc or threshold committees.


So that's when I learned the concept of iO, I was really surprised and fascinated by iO, because it was an ideal solution for enabling smart contracts to process some secrets, such that no human can ever learn, but anyone can use in some predefined logics.


[Anna Rose] (6:57 - 7:13)
So it's almost like it's the optimal. It's like if it worked, it would be amazing. There would be so much potential. It would actually do a lot of the things that ZK and FHE promised to do, but in better form.


But, I mean, did you learn quickly that it's still a ways off?


[Sora Suegami] (7:14 - 7:41)
Yeah. Of course, yes. But when I read that article, I was really surprised, because this universe allows the existence of such secure iO construction. And I thought I found a new kind of dream, concrete dream and goal to achieve, which is big enough and difficult enough.


So this is how I first met iO.


[Anna Rose] (7:42 - 7:47)
Nice. So I'd also like to hear from you, Enrico.


What's your story? What got you excited about this? 


[Enrico Bottazzi] (7:47 - 8:23)
Yeah. I think in my case, I came to know iO more recently. So I think it was around one year and a half ago, two years ago.


And at the time, I was working on an application. I was writing a paper related to multilateral credit setup, which is a very practical application in which you have an agency that is collecting invoices and IOUs from different businesses, different companies.


And they kind of create a network of IOUs after collecting this information.


[Anna Rose] (8:23 - 8:26)
And you're saying IOUs here, not iO.


[Enrico Bottazzi] (8:26 - 8:30)
Yeah, yeah, yeah. No. No. It has nothing to do with iO at this point.


[Anna Rose] (8:30 - 8:31)
IOU as in like credit.


[Enrico Bottazzi] (8:31 - 8:33)
Yes, yes. Exactly. iOU.


[Anna Rose] (8:33 - 8:33)
Got it.


[Enrico Bottazzi] (8:34 - 10:31)
And since this agency, this single party is able to construct this network, they can then identify cycles of debit and credit, and they can cancel them out from this network. And this yields some liquidity saving for the company.


So I think it was a very compelling application for privacy, because I thought that if companies would be basically guaranteed that their personal information wouldn't be revealed to this agency, they would be more encouraged to submit their invoices. And therefore, this would yield bigger liquidity savings for the whole system.


And I started looking into technology to enable that. And for example, I was working with FHE, but I soon realised that if you want to use FHE, you need somehow to... you cannot distribute the trust across the whole firms, because let's say you have like thousands of firms, you might end up with a very kind of weak system which just a single firm dropping off the whole process will kind of destroy everything.


So what we ended up doing in this paper was just, which is kind of what happens in most of production-ready application today, is they kind of delegate the security of the FHE, the security of the decryption key to a committee, to a very small committee that is kind of trusted to not collude, to not misbehave.


I wasn't kind of satisfied by this solution. And this is actually when I already knew that Sora was kind of interested in this iO thing, even without fully working on that.


But we decided to undergo this challenge last year. I think it was exactly one year ago at Devcon. And for me, it was more to kind of a frustration from an application developer point of view.


[Anna Rose] (10:31 - 10:41)
But it's weird because you were experimenting with FHE, which is also not quite practical. And instead of going backwards to the things, the libraries that exist, you went further.


[Enrico Bottazzi] (10:42 - 10:46)
Yeah. I think it was too practical for me. So it wasn't satisfying.


[Anna Rose] (10:46 - 11:05)
Okay. So Machina, I've sort of said this in the intro, that it's a team within the Ethereum Foundation. Bt were you both working already at the Ethereum Foundation, and then you sort of formed this... I don't know, subcommittee... I don't know what it is exactly.


But tell us a little bit about what the connection point is there, and what Machina is.


[Sora Suegami] (11:05 - 11:24)
Yeah. So our Machina iO is a team or a project within PSE of the Ethereum Foundation. And we have worked on PSE even before forming this Machina iO project.


So this is exactly the project we started to research practical iO.


[Anna Rose] (11:24 - 11:41)
Yeah. But I was kind of curious, why iO? Why Ethereum?


Like is Ethereum... is it in the roadmap? Is it part of the plan to explore this stuff? Or is it more just like experimental? You had the green field, and so you decided to explore this.


[Sora Suegami] (11:42 - 12:37)
Yeah. So I think this is more kind of experimental things. And of course, iO is not in the official roadmap published by Ethereum Foundation, because iO is still not yet practical.


But I think iO is aligned with, especially the privacy goal of the Ethereum Foundation, Ethereum. And this is because when we want to protect privacy of applications, which involves multiple parties secrets, data, such as a private form of the DEX, or some kind of a mixing applications, with enhanced security or a cross-chain bridge with Bitcoins, they need to write. So they cannot be implemented only using ZKP.


But in this case, without iO, we have to write on either like MPC or TEE, or multi-key FHE and so on.


[Anna Rose] (12:38 - 12:43)
What do you mean? Like if you have iO, if you want to work with iO, you're actually using these other tech stacks?


[Sora Suegami] (12:44 - 13:55)
Yeah. So if we have iO, we can use iO to protect our data, such that no one should not be able to know.


So for example, if you want to build some cross-chain bridge systems with Bitcoins, we'll need some system such that this system uses some Bitcoin private key, no one knows, for like a signing transaction to withdraw Bitcoin, if and only if a user burns the corresponding wrapped Bitcoin or Ethereum blockchain.


But to manage such a private key, and use it only for some valid cases, we usually need to write on some MPC or threshold signature or TEEs.


But if we have iO, we can just make one obfuscated programme, which hard-codes this Bitcoin private key. And if anywhere inputs some ZKP proof to say, my wrapped Bitcoin is honestly like burned on Ethereum blockchain, with some validators' consensus, then this obfuscated circuit will output the signature to withdraw Bitcoin for your address.


[Anna Rose] (13:55 - 14:04)
Are you talking about replacing existing ZK systems and architectures? Or are you saying you're adding iO to some sort of architecture?


[Sora Suegami] (14:04 - 14:19)
Yeah. I think this will replace some of the systems based on like MPC or like a threshold-based cryptography, such as threshold signature, MPC and so on, or maybe some of the TEE-based applications.


[Anna Rose] (14:19 - 14:24)
Your research is about replacing it. It's not about working with it.


[Enrico Bottazzi] (14:24 - 16:12)
I think, actually, this is something that we've been discussing very recently, that I think we fit very well with, for example, existing FHE application. And especially, for example, I know that you guys had Zama people as guests of the podcast, that they are building this FHEVM, which is basically an equivalent VM in which all the smart contract states are encrypted under FHE.


And in this scenario, I think, we can kind of plug into this FHEVM and only replace the decryption part, actually making the decryption part more secure.


So the whole FHEVM, let's say the onchain logic of an FHEVM remains the same, but the thing that we can actually provide as an add-on with iO is actually replacing their current system that is handling the decryption part, which, similar to what I was describing before, is relying on a committee of people, which can always kind of, I don't know, just decide not to cooperate or even worse, behave maliciously.


And we can kind of embed this conditional decryption logic inside an obfuscated programme such that once obfuscated, this can be just released in the public and everyone can consume this. Everyone can use this programme, everyone can pass ciphertext to this programme, if they are allowed to obtain plain text as a result.


But the point is that for this decryption process, there's no longer the reliance on the committee members. So I think this actually kind of fits very nicely with some existing architectures.


[Sora Suegami] (16:13 - 16:31)
So in summary, we can say iO will replace the trust assumption, in particular threshold assumptions, from the existing cryptography-based applications. But at the same time, iO will be used combined with other cryptographic techniques, including ZKP and FHE.


[Anna Rose] (16:31 - 16:31)
Got it.


[Tarun Chitra] (16:32 - 18:26)
Maybe a place to kind of go more into the technical details. I think if you think about the original construction that we talked about during the Rachel Lin episode, where you have this circuit, and you divide it into a public and private bit.


And then the private bit is a very low degree quadratic. And then the public bit... but potentially large, but still low degree.


And then the public bit is unbounded degree, and you kind of separate the calculations with this sort of projecting back into the secret space.


Obviously, that is very cool. But obviously, there's all this kind of exponential space blow-up that can happen as you're doing the computation.


To make things practical, of course, you have to effectively choose a different set of assumptions that underlie the security. Like instead of kind of relying on PRG and degree-2, and things like that, you have to rely on something that's more linear algebra in LWE.


So maybe, I guess, let's talk about the assumptions you're making here. You know, why LWE? Why ring LWE? Obviously, that shows up in FHE as well, but just sort of just to get an idea of why did you choose the assumptions you chose?


And then let's talk about how those enable you to get something more practical. And before we describe the iO, just more like the core assumptions you make, how those assumptions differ from sort of the theoretical constructions, and what those assumptions buy you.


Because I think that's something that is hard to kind of define sometimes from these papers, where it's like, there's a theoretical thing that uses very simple assumptions, but impossible to construct in practise. And there's a practical thing that makes a different assumption, which is not equivalent in many cases, but seems like it should be equivalent, and in the sense that we don't know of any polytime reduction.


But yeah, just would love to kind of go down that path with you guys.


[Sora Suegami] (18:27 - 18:47)
So first of all our iO constructions rely on only lattice assumptions, but not only standard LWE assumptions, but also some two non-standard assumptions. And one of which is actually a new assumption we proposed, we introduced for our iO construction, called all-product LWE.


[Anna Rose] (18:47 - 18:48)
All-product LWE.


[Sora Suegami] (18:48 - 19:32)
So this is our product because we consider a variant of our LWE instances for corresponding to all of the input patterns.


So if you have 3 input bits, like we have 3 input bits, we have 2 power of 3 equals 8 patterns of the input bits. And we define LWE kind of different, but a variant of the LWE instances corresponding to each input bit. So in total, 8 bits, if we have 3 bits.


And our assumption claims that even if we reduce our LWE, such kind of LWE instances, for all of the product patterns...


[Anna Rose] (19:32 - 19:32)
All products. Okay.


[Sora Suegami] (18:3 - 19:43)
All of these instances remain pseudorandom, and there's no kind of relation which breaks the security of such LWE instances.


[Anna Rose] (19:43 - 19:55)
But you are creating your own assumptions here. So you are deferring from the initial, at least for sure, the 2021 work. Are you taking any of their assumptions along with you in these implementations, though?


[Sora Suegami] (19:55 - 20:00)
From 2021 paper, we only took standard LWE assumption.


[Anna Rose] (20:01 - 20:05)
Okay. And then you added this all-product LWE assumption?


[Sora Suegami] (20:05 - 20:13)
Yes. And also, we have also used one more non-linear... non-standard assumption called evasive LWE assumption. 


[Anna Rose] (20:13 - 20:14)
Evasive. Okay.


[Sora Suegami] (20:14 - 20:38)
Yeah. Actually, this is also a super strong assumption.


And I think in the last episode, Rachel Lin I think she mentioned new assumption as a LWE assumption with hint. And she said recently, some counterexamples, I guess, these assumptions have been proposed.


And this evasive LWE is actually one of such assumptions.


[Anna Rose] (20:38 - 20:44)
Evasive LWE. But is that something that existed outside of this, and you've kind of brought it in?


[Sora Suegami] (20:45 - 21:14)
Evasive LWE is kind of new assumption. This has been introduced from 2022, and this has been used in many of the lattice-based construction, and we also use that assumption.


But I think, in the beginning of this year, just right before and after we published the Diamond iO paper, I think there are four papers which has proposed counterexamples against this evasive LWE assumption.


[Anna Rose] (21:15 - 21:15)
Oh, no!


[Sora Suegami] (21:15 - 21:15)
Yeah.


[Anna Rose] (21:16 - 21:19)
So was it like you released some work and then it got broke right away?


[Sora Suegami] (21:20 - 22:04)
Actually, but this is slightly... situation is slightly different from usual assumptions. Because in most of the assumptions, cryptographic assumptions, these assumptions are falsifiable, which means if you provide some counterexamples this assumption is basically completely broken.


But in the case of evasive LWE, you can consider many variants of this assumption. Because it has a lot of variants of the sampler definition used in the definition of the assumption.


And this attacking paper proposes just a counterexample of some specific samplers. So, this does not mean this assumption does not fall, or for all of the cases.


[Anna Rose] (22:05 - 22:05)
I see, I see.


[Sora Suegami] (21:05 - 22:10)
So, we are still not sure if our use of the assumption is secure or not.


[Anna Rose] (22:10 - 22:20)
But it does sound like you guys are building on, to use Rachel Lin's metaphor, like moving lava, that may or may not hold.


[Sora Suegami] (22:20 - 22:22)
Yeah, yeah, yeah, yeah. [?] three, yeah. 


[Anna Rose] (22:22 - 22:29)
Got it. Tarun, your question, I feel like, was broader, though. I know we just kind of dug into the actual assumptions here, so.


[Tarun Chitra] (22:30 - 25:01)
Yeah, to maybe give a little background. Learning with errors is, I take a plaintext, I embed it into a lattice with the ciphertext, I add some error, and the goal is that, hey, this linear system, like the generator matrix, is not easy to invert.


So, by adding the error, it becomes too hard to exactly invert. And the hope is that I can add a small amount of error, small in the sense of the noise or the magnitude, and also in the support, like the number of coordinates I have to add.


And one reason that's very appealing is linear algebra is something we know how to do very easily and do it efficiently everywhere.


So, for FHE, [?] LWE is more or less sufficient to get pretty far. But for iO, you have to really deal with this idea that at each step, your LWE assumption is sort of implicitly leaking something about the circuit you're using. It's sort of like there's some correlation between the way you're constructing the generator matrix and the circuit.


And so you have to keep refreshing, and that refreshing is very slow or expensive. Again, this is my heuristic explanation, so you guys can correct me if I'm wrong. It's like I'm trying to simplify as much as I can of what I understand of this.


So, you have to keep refreshing, and that refreshing is expensive. And so, you need to find some other shortcut where it's like, I can apply another LWE-like transformation, another projection, maybe to a different lattice, whatever, maybe with different error, but something where it looks like it's matrix multiplication. But such that I hope that the matrix multiplication whitens or adds so much noise to the circuit definition without removing the effect of the circuit computation.


And so, somehow, this all-product assumption and the evasive assumption effectively do that. The hint is sort of a trapdoor to allow you to, kind of like in FHE where you project and then de-noise, and you can only de-noise if you have the secret key. There's sort of some sense in which the hint lets you de-noise the real answer if you knew enough information.


So, maybe walk us through those assumptions. How do they map to that? Again, I tried to simplify it a lot. I know it probably misses a lot of the nuance, but jus walk us through that.


Just like, why do you need these assumptions? There's a sense in which there's a gap between FHE and iO that's very non-trivial from a complexity theory standpoint. But somehow, that also maps into this assumption space.


And so, I slightly changed my question, but I think hopefully that gives a little more context into it.


[Sora Suegami] (25:02 - 25:09)
Okay. So your question is like how each assumption is necessary and important in the iO constructions, right?


[Tarun Chitra] (25:09 - 25:24)
Yeah. I think I just wanted to... I think one really big difference between FHE and iO from just the standpoint of a user is the circuit has to get obfuscated. But somehow, the normal LWE is not sufficient. So why do I need these extra assumptions?


[Sora Suegami] (25:25 - 27:38)
I see, I see. So basically, as you said, we need new assumptions because while we want to hide most of the secrets about the obfuscated circuit, we also need to reduce only the circuit output as a plaintext in a non-interactive way so that we have to prepare some like hints which reveals such corresponding output.


And we need new assumptions to say that this hint does not reveal information beyond what we want to actually reveal to the evaluator of the obfuscated circuit.


For example, we need evasive LWE assumption because evasive LWE provides some kind of similar role as a pairing, but it's not necessarily restricted to just a degree-2 multiplication and some constraints because this provides a similar role to the [?] pairings, but it's not restricted to degree-2 multiplication.


Like a similar technique, so a key building block regarding this evasive LWE assumption is a lattice trapdoor. So if you have some LWE instance which encodes some secret and some secret data, and if you have a preimage which also encodes another secret, you can do multiplication between them.


And this makes [?] LWE instances which encodes some like a product of these two secrets. And you can do the similar things multiple time. So this like a preimage, lattice preimage was originally proposed to make the kind of multilinear maps.


But finally, people found multilinear maps for general use cases based on such techniques are insecure.


But evasive LWE rule adds some use cases of such preimage, lattice preimage multiplications which allows the same type of attacks against the multilinear maps.


In particular, if the products remains pseudorandom for adversaries, this assumption says these like a preimages will not reveal the internal secrets and it only reveals the pseudorandom value obtained by multiplications.


[Enrico Bottazzi] (27:38 - 28:49)
I think we're relying on a different line of work compared to the 2021 paper that Rachel Lin talked about in the previous episode.


And I think the core of our construction of iO relies on a paper that was published in October last year, which is called Compact Pseudorandom Functional Encryption, authors AKY. And basically this paper proposes a new, more algebraic construction of functional encryption.


This is also where the evasive LWE assumption comes from. And in our work, in the Diamond iO paper that we published earlier this year, we kind of add, let's say an additional component to this AKY24 paper in which we allow basically a user, an evaluator, let's say a consumer of this  obfuscated programme to dynamically add their input bits. 


And this is where the all-product LWE assumption that Sora was describing comes into play. So we are more, let's say, on a different line of work.


[Anna Rose] (28:50 - 28:52)
Turn, did that answer your question?


[Tarun Chitra] (28:52 - 30:23)
Yeah. I mean, it kind of gets at this, but I guess the real question is just more like, why should I kind of trust that these assumptions will be equivalent or at least give me some comparable amount of security?


I think the other thing I'm a little bit... and maybe this is actually worth going from now the assumption to the implementation, is I think of a lot of LWE things and it's like, I do a single matrix multiplication, I get a ciphertext, and then I do apply the circuit, and then I project back for FHE.


But then for iO, I do something sort of different where I embed the ciphertext and then I look at the difference between the ciphertext and the true circuit evaluation and then project that again.


And that naturally sort of seems like why you would need this all-product thing, because I'm doing two nested LWEs. I'm doing an LWE on the raw input, and I'm doing an LWE on the difference between the ciphertext and the sum embedding and the true value.


Maybe let's talk through, if it's sort of these types of matrix multiplications, why are they so large for a particular circuit? Where does it blow up?


Is it like nested multiplications blow up really quickly? Is it actually the fact that addition and multiplication blow up the polynomial degree in some more nuanced way? What makes it really large? What makes these matrices large? What makes it kind of slow?


And we'll talk about optimisations next, but just maybe, why does it have this reputation of being such a large kind of programme for small circuits?


[Sora Suegami] (30:24 - 31:21)
Basically, the main bottleneck we have is that when you have a larger input size, you need to prepare more matrices as lattice preimages, and you need more number of matrix multiplications to insert each input bit.


And this finally blow up the accumulated error size, because the initial error is merged by this matrix so that the size of the error increases. But at the same time, we need to ensure that the modulus of the matrix is sufficiently larger than this like the maximum bound of the error.


So finally, if we have more input size, we need more modulus. And since the modulus is large, each of the matrix size also increases by the increase of this modulus.


This is the main reason that why the obfuscation size, matrix size, increases so quickly, even for simple functions.


[Anna Rose] (31:22 - 31:35)
Everything we're talking about, is it completely theoretical and you guys are making kind of toy implementations, but it's so unusable at this point? Or do you actually see like a glimmer in what you're doing that it could potentially be used?


[Enrico Bottazzi] (31:36 - 34:10)
Yeah. So I actually have in front of me our benchmark results that we shared earlier this year based on this Diamond iO paper that we published.


And I think we can definitely say that this is not practical and this is not usable. So we have, for example, as Sora was mentioning, we think in terms of input bit size because our construction is basically what allows users, allows evaluator...


So, again, let's go back, maybe make a step back. And when we talk about obfuscation, we always have these two different phases. One is the obfuscation phase in which... basically the compiler phase in which you take a programme, you pass it through this compiler, and you then get the obfuscated equivalent.


And then there's the evaluation phase in which users, like consumer of an application, will pass their input into this obfuscated programme and obtain an output without learning the inner working of this programme.


And this input size refers to the input size of the evaluator. And for input size of just a few bits, so we tested with 1, 2, 4 bits, a programme that is doing, for example, just a bunch of multiplication, the size of the programme is huge, is around 50 gigabytes.


And the obfuscation time for4 bits, for example, is like over 300 minutes, like 5 hours.


So I'm not saying that this is a hint for practicality, but I think one interesting observation that we see is that actually there's this huge asymmetry between the obfuscation phase and the evaluation phase. So the obfuscation phase, for example, in the case of the 4 bit input size, took around 5 hours, but the evaluation took only, let's say, 18 minutes.


And the cool thing is that the obfuscation is something that needs to be done only once. So we can potentially admit that this takes a long time, and what we really want to optimise for in order to make it practical is the evaluation phase.


So I think our implementation, at least, gave some numbers. They put some numbers into something that everyone says, yeah, it's very inefficient, but they didn't have the numbers. So I think this is a good point to start.


And we also have a lot of idea, and we also have a lot of... let's say, we have a rather clear roadmap in order to make it practical.


[Tarun Chitra] (34:11 - 35:48)
Actually, one kind of interesting question that maybe this brings up, this asymmetry, is I think about ZK, how ZK went from very impractical because everyone wanted to... until 2012 Gentry paper, everyone was like, we have to do a PCP, versus then people were like, okay, find a way to write a constraint set that's small. And then in that constraint set, all the polynomials are small, so then you can kind of like optimise things before we got to Groth16.


It seems like you need a step like that where you find some lower dimensional reduction that allows you to embed how you describe the circuit efficiently.


But there's also a question of pipelining. Can I separate? Is there any ability to parallelise some of the obfuscation versus evaluation components such that they can run in tandem?


So I'm just curious, what are the optimisations you see that are possible and which things are impossible, which things would not work? Because I do think for ZK, in theory, people could have made really inefficient implementations of ZK in the 90s, but then it just took 20 years to find the right representation such that things were small and then it was off to the races towards optimisation.


And that's not to take away from all of the things, lookups and whatever, that obviously made things faster, but I do really feel like it was changing how people thought about representing the circuit that changed things.


So how do you foresee that for iO? How do you think about optimisation within iO?


[Enrico Bottazzi] (35:49 - 38:04)
Okay. So I will take my favourite, let's say, optimisation potential, but then I will also let Sora add his opinion.


For me, it's actually, I would make an analogy closer to the FHE space rather than the ZK space, which is when Gentry introduced the bootstrapping technique in FHE that allow basically to, let's say, allow computation for unbounded circuits.


And I think this is also the case for iO, or in general, it's not strictly for iO, but it's more for, let's say, the underlying primitive that we're using for iO is something called BGG+ encoding, which actually one of the G is actually Gentry and the B is Dan Boneh.


And this was a construction that was published over more than 10 years ago for attribute-based encryption and is still the main building block for this whole category of non-interactive cryptography that, for example, involves also witness encryption, functional encryption and iO.


And we actually need a way to do noise refreshing, a sort of bootstrapping on this BGG+ encoding that would allow, similar to what Sora was mentioning before, to increase the input bit size such that, for example, we add some input bits, then the error blows up, but we are able to do this noise refreshing to kind of move the error back to zero and keep doing it again without having to, let's say, allow from the very beginning this error space to handle this gigantic input size because this wouldn't be practical.


So I think this is one way, one promising optimisation.


And we know that actually Rachel Lin published a paper last year on bootstrapping, which I think it proves a theoretical point that you can do bootstrapping on BGG+ encoding, but I think it's not practical yet. So maybe we need a more practical way to do bootstrapping on BGG+ encoding.


And I think this would kind of bring us closer to practicality.


[Tarun Chitra] (38:04 - 38:35)
Actually, I do think a kind of important part about at least what I understand of your construction is actually this encoding, which seems like you take a bunch of small matrices to select which input bits are needed and then evaluate, then add noise.


So maybe walk us through what the encoding is, and what the difficulties are, because I do think there is some nuance to that that's unique to your construction that the other iO papers maybe don't kind of cover as much.


So it would be great to kind of understand how the selector matrices work.


[Sora Suegami] (38:36 - 38:41)
Yeah. Yeah. So like a selector means a section for each input bit, right?


[Tarun Chitra] (38:41 - 38:44)
Yeah. I just mean like, yeah, the small, yeah.


[Sora Suegami] (38:44 - 40:27)
Yeah. So actually, this is a new technique we introduced, and the selection means that... so you can first imagine that if we have encoding, which involves both of the encryption of the obfuscated circuit and your input bits, you can do the similar technique to the functional encryption proposed in the Compact Pseudorandom Functional Encryption paper to evaluate this private circuit on your input and only recovers the plaintext of the circuit output.


And we use this selection idea to insert our evaluators input bits to this encoding, which only contains the obfuscated circuit. And this selection means that you first have the encodings, which does not have any of the input bits, which contains only obfuscated circuit, but you can also access to these preimage matrix, each of which correspond to for example, you have two metrics for first input bit.


One is corresponding to input bit 0, and the second one is corresponding to input bit 1. And if your first input bit is 0, you choose the 0s one and multiply this matrix. Then you can insert your input bit 0 to the given encoding.


And the obfuscator provides such matrix for each input index so that evaluator will choose one of the two matrices for each input index to insert each of the evaluators input bit one by one.


And finally, they can obtain the encoding, which contains both the obfuscated circuit and all of the evaluators input bit.


[Tarun Chitra] (40:27 - 41:07)
So one thing I think I was kind of curious about is the choice in which... the permutation in which you evaluate the order of the input sequencing can change the embedding. But then you add this Gaussian noise, which is invariant to the permutation.


So I'm kind of curious is the ordering of the input bit constructions part of the obfuscation? Or does it actually not matter? Because in distribution, it's equivalent once you re-add the noise at the end.


Because that was sort of my thing. I think about a lot of obfuscation methods that are draw a random secret permutation and permute. Like is that involved in the security here? Or...


[Sora Suegami] (41:07 - 41:23)
So in our case, in our construction, I don't think we involve some of the kind of random permutation metrics, because this selection part only depends on the evaluators input bit. So we don't need to randomly permutate like such orders.


[Tarun Chitra] (41:24 - 41:26)
Oh, I see, I see. Okay, okay.


[Sora Suegami] (41:26 - 41:45)
Maybe you may think, is it like iO candidates proposed probably in 2015 or 2018, these things are real... converts your circuit to the blanching problem, and which is permutation metrics, and you obfuscate each of the metrics.


Yeah, I understand.


[Tarun Chitra] (41:45 - 41:50)
Yeah. I sort of was reading this and thinking Barrington's theorem, like that exactly that.


[Sora Suegami] (41:50 - 41:50)
I see, I see.


[Tarun Chitra] (41:51 - 42:24)
And that was... but because when someone is introducing a new set of assumptions, it changes the tools you use to prove the same... you have two different assumptions.


You're trying to prove basically a similar or same security property that something isn't corruptible or it's indistinguishable against other adversaries.


But because there's one set of proof techniques in the theoretical construction, another set in the practical construction, understanding where they are similar and different is kind of important.


So that's sort of the lens in which I'm asking this.


[Sora Suegami] (42:24 - 45:34)
The high-level idea of our security proof is that, first, when we use evasive LWE, we have to prove that all patterns of the product between the LWE instances and all of the provided lattice preimages remains pseudorandom.


And in your case, a part of the lattice preimage corresponds to the matrix, which insert some input bit, each of the input bit, and a secret corresponding to each input bit to the given encoding.


To prove that these preimages will not make our initial encoding insecure, they're building some secret. We consider two-path n LWE instances such that we try all of the multiplication patterns of the n preimages where n is the input size. We have to prove that combinations all of the two-path L instances remain pseudorandom.


And actually, we finally need to rely on our all-product LWE to finally claim that these of the instances remain pseudorandom.


And the essence of the all-product LWE is that if we just reuse the same secret, but with changing some public part, it's easy to attack these LWE instances and easy to distinguish between the two of the LWE instances with some random ones.


So the precondition for evasive LWE will not hold. But our assumption introduced in all-product LWE is that if we insert input-dependent secrets between the initial secret part and the public part, then it might be possible to say these LWE instances will be... looks independent and pseudorandom.


So for example, let's say the initial secret is just s, and we prepare some secret corresponding to the bit 0 for the first input bit and the bit 1 for the first input bit. And also s and the other two secrets corresponding to 0 and 1 for second input bit.


And the public part will be different for every input path, like {0, 0} or {0, 1}, {1, 0} and {1, 1}. And we will reuse the encoding which depends on four patterns.


So first pattern corresponding to input bits {0, 0} will involve some common secret times secret for bit 0 at the first input bit and secret for bit 0 at the second input bit and the public matrix corresponding to {0, 0}. And we do the same things for {0, 1}, {1, 0}, {1, 1}.


And we finally rely on our assumption to say that all of these four patterns will be pseudorandom, so that the precondition to use this evasive LWE assumption will hold.


And finally, we can say that the initial encoding along with these lattice preimages, each of which holds each of the secret, piece of the secret will remain pseudorandom.


[Tarun Chitra] (45:35 - 47:10)
I do think it's actually kind of, it's interesting, right? Because here you care about this... I think the oversimplified by a long shot and dumbest version of iO is like I take a Boolean function that has some Boolean expansion, and then I take the set of circuits that describe that function up to some constraint, and then I randomly sample on those circuits and evaluate it partially.


And that is sort of not at all obvious that the sequence level thing is equivalent to that sort of very high level view of how people think of iO.


So if we zoom out a little bit, I actually think another kind of important question, this is a question I think that's still open in every field from sort of ZK all the way to AI in different ways, which is what is the correct set of benchmarks for iO?


What programmes should we think about as this is a good benchmark that we should be targeting? That, oh, given this benchmark, as we do optimisations, we're improving performance or whatever on that.


I think for ZK for better or worse, in the beginning, all the benchmarks were this Fibonacci stuff, but then eventually, once people had VMs, they started targeting real programmes.


And for AI, obviously they have like...  there's obviously all this fight over what evals are correct.


So in a world where iO becomes practical what are the programmes you think that are the benchmarks? What are the benchmarks you're doing now? You talked about... you gave wall clock times. Well, that had to have been for some circuit. So what circuit were you using?


[Enrico Bottazzi] (47:11 - 49:16)
Yeah, yeah. I think, actually, we have a pretty clear answer to this. Because in this future that I kind of hinted to before, in which the whole application logic is run by an FHE processor and only the decryption logic, the conditional decryption logic is handled by an obfuscated programme, well, in this world, actually, we only need to obfuscate a single programme and we only need to obfuscate this programme once and then this can handle all this private computation application forever.


So really what we need to obfuscate is not... I think this is also a common misconception when it comes to iO that for each application that you want to, let's say, have some privacy or you want to hide secrets, you need to obfuscate a specific programme.


But actually what... And this is also another paper from Gentry, GGH13, in which he introduced this concept of... Maybe it didn't call it this way, but we call it like universal iO. So as long as we are able to obfuscate this programme, we can kind of handle the security of any type of application, really.


And the logic of this programme is as follows.


So it should take as input a SNARK, a proof, together with a ciphertext. And there's some verification logic that is encoded in the obfuscated programme that will, let's say, in a kind of very simple words, is saying, am I allowed to decrypt this ciphertext? And this will be based, for example, on some onchain smart contract logic.


And if the verification passes, the decryption proceeds. So it's like, I think we can simplify the programme as SNARK verification plus FHE decryption.


And as soon as we get to obfuscate this programme, we are basically, let's say, done.


[Anna Rose] (49:17 - 49:18)
You just have to do that.


[Sora Suegami] (49:19 - 50:04)
Yes, yes. So I also have another [?] about how to benchmark iO, especially for more short time. So I think a more concrete way for benchmarks in short time is obfuscating your circuit with, for example, 32 input bits or more.


Because if input bits is, I think, more than 16 or 18, it's basically impossible in practise to prepare some function as a truth table.


So if we can make practical obfuscation for input size like 32 bits, this is a non-trivial result, such that we cannot do the same things without iO in a practical way, unless p is equal to lv.


[Tarun Chitra] (50:04 - 52:04)
Yeah. So actually, that's a very salient point. Because to me, another kind of, again, I'm oversimplifying in all these descriptions, but mainly just so... because I do think iO, if you haven't spent a lot of time trying to understand the assumptions, it can be kind of confusing, is iO is very good at taking anything that has a really large gap between the average case complexity of an instance and the worst case complexity of an instance, and it makes everything the worst case complexity.


So it's like, take these SAT problems where the worst case SAT problem is very hard exponential time to find the answer, but the average case is very low-degree polynomial.


iO in those problems has to convert the low-degree thing to close to exponential or at least super polynomial.


And so I guess the natural question is like, there are a bunch of problems like that that are natural. The SAT is a little bit annoying, probably, I imagine, as your benchmark, but you could imagine other benchmarks that are like the simplex algorithm in optimisation, where it's well known what the worst case instances are, and you basically can run an average instance, which is known to be fast versus the worst case instance and show that it's like the same sort of reduction.


Another question that's sort of related to this is the power of iO, and at least from my perspective, is the fact that it's sort of universal in the sense that you can reduce it to, if you have it as a black box, you can construct all the other cryptographic primitives you want, functional encryption, FHE, ZK, whatever.


All of them are sort of this is the big boss, and if you solve the big boss, all of them are sort of corollaries without that much work.


And so I'm curious how you think about benchmarking the reductions. If iO was sort of a universal compute, you had a computer that had a universal iO thing that was fast, how do I benchmark how good the iO is on how well it does at implementing the reductions to these applications?


Because I actually think the reductions are in some sense, the more practical use case than raw iO itself, or that was more a question, sorry.


[Sora Suegami] (52:05 - 52:10)
Reduction means like reduction, so like using iO for more like simpler applications.


[Tarun Chitra] (52:11 - 52:20)
Yeah, yeah, exactly. Like all of kind of the known, hey, these are known cryptographic applications that if you had iO as a black box, you could implement quickly or implement efficiently.


[Sora Suegami] (52:21 - 52:21)
Yeah, yeah, yeah, yeah, I see.


[Tarun Chitra] (52:21 - 52:27)
Yeah what are those benefits? Because I feel like those are the practical applications that people want, to some extent.


[Sora Suegami] (52:28 - 53:31)
I see. So I understand at least for cryptographers, this reduction to the... like using iO as a black box to make a simpler applications or extends like a symmetric-key encryption to public-key encryption can be like one good applications.


But I'm not sure if this is how we eventually use iO for practical applications, because I believe even if we can make ZK or FHE by iO, like a direct construction of them should be more practical than iO reduction.


And at the same time, I think as Enrico said, we will use iO part only for some specific small parts, like doing conditional decryption so that... I'm not sure if like, for example, measuring benchmark of iO-based FHE construction or iO-based ZKP construction makes sense.


If our like a wide range of the iO, which also rely on ZK and FHE is like a used for this benchmark.


[Tarun Chitra] (53:32 - 53:35)
I was just curious, have you tried implementing any of these reductions?


[Sora Suegami] (53:36 - 53:42)
No. Because at this time, we can support only a few input bits.


[Tarun Chitra] (53:40 -  54:01)
Okay. Okay, yeah, yeah, fair, fair.


I mean, I actually, I'm curious, while you're writing the software here, what gates do you use as your unit tests?


What are your... is it just like add and multiply for three bit integers? Or are you doing something? Yeah, kind of curious, where do you start when you're doing a project like this?


[Enrico Bottazzi] (54:01 - 55:31)
Yeah. I think like most of the circuits are really just, for example, multiplying a polynomial by a secret bit. So we have a non-polynomial and our secret is either a zero bit or a one bit and we want just to make sure that the output is what we expect.


So it's not really any meaningful programme that we are testing out, but it's just some really dummy programme that is testing out the functionality that we describe in our paper.


I think also something that is worth mentioning is that, before I referred to this BGG+ encoding as the core building block for our construction... and again, we are not trying to reinvent the wheel here. There are like known libraries, for example, like OpenFHE library, which is a very popular C++ library, already had all the primitives that we needed to handle this BGG+ encoding.


And they tried in the past some sort of, let's say, similar construction to iO, and therefore that's how they built this. But it's also very similar to what all the FHE people, all the... let's say, lattice people are more familiar with. So I think what we also want to convince people is that the jump from FHE to iO is not a huge jump. Most of the idea, the concept and the actual code that we are using is also shared across these two domains.


[Sora Suegami] (55:31 - 57:14)
In the iO construction level, we only evaluated some dummy circuits as Enrico explained. But after publishing our iO implementation, instead of focussing on the directory or improving the efficiency of our iO construction, we are focussing on improving the efficiency of this encoding layer. Because this encoding has not been researched to make them really practical, similar to ZKP or FHE.


And they are researched mainly in the theoretical area. So we first need to make this encoding practical. And for this encoding layer, we try to implement our FHE evaluation over encodings.


In particular, doing some multiplication, some nonlinear operation over like large modulus and so on.


And we introduce lookup table evaluation over this encoding to make them more practical, but we still have some challenges to make them concretely efficient.


[Anna Rose] (56:31 - 56:33)
Oh, interesting. You're actually using lookup tables.


[Sora Suegami] (56:34 - 57:14)
So after making our iO implementation, we next focused on making a lookup table evaluation over this encoding. Because when we see the history of the ZKP or FHE, lookup table evaluation played a significant role to make them practical.


So we considered if we can make a similar native lookup evaluation technique for this encoding so that we can do nonlinear operations more efficiently. And we actually make such ideas and we confirmed the efficiency has been improved, but we need more improvements in the case of this BGG+ encoding.


[Anna Rose] (57:14 - 57:31)
I have just a quick clarification question about something that was said I don't know, like 20 minutes ago. You mentioned the Gentry paper of 2012. Tarun, I think you had actually mentioned that.


But I always think of Gentry as like FHE, but you used it in the context of ZK. So now I'm confused.


[Tarun Chitra] (57:31 - 57:35)
A lot of ZK stuff, like he's just done stuff and everything.


[Anna Rose] (57:35 - 57:37)
Okay. It is actually Gentry ZK.


[Tarun Chitra] (57:37 - 58:03)
Yeah, yeah. There is an old paper of his arguably Groth16 probably owes a lot of the original framing from. Groth16 made this paper practical, but from a theoretical standpoint, I think he figured out the right representation or encoding that made it efficient.


So this is like, but he's done something and everything. The best part about Gentry is he wasn't even like trying to be a cryptographer. He was like a lawyer for 10 years before.


[Anna Rose] (58:05 - 58:06)
Amazing. Do you know his story?


[Tarun Chitra] (58:06 - 58:08)
Yeah. His story is pretty crazy.


[Anna Rose] (58:08 - 58:45)
We met at zkSummit13. He popped over and I definitely pitched him to come on the show and I hope I get to invite him sometime. I think it would be a lot of fun to have him on.


But yeah, I think his background is amazing, but I actually didn't realise that. I always thought of him very much in the FHE world, but that's cool.


I do want to say one other thing before we sign off.


I just want to say a quick shout out to one of your colleagues, Pia Park, who kind of brought us together. I had reached out to her. She, I think, had published something through the Machina Project and then she introduced me to the two of you.


I'm just curious, what was her work there? And just since she did that intro.


[Enrico Bottazzi] (58:45 - 59:42)
Yeah, actually, we met with Pia at a residency that we did earlier this year in Taiwan. And I actually already had a chance to work with Pia before because she audited one of the projects that I was working on in the past.


And it was very actually fun because probably in two days, I think I'm not wrong saying that, after two days that she spoke to us for the first time and learned about iO, she was already contributing to the repo.


So we already had... probably she's still the main contributor and she was definitely like the main driving force at the very early stage in which we were kind of building this from scratch. And I think actually all the results that we mentioned before are really thanks to Pia and her work on the engineering side and especially the Rust side of our library.


[Anna Rose] (59:43 - 59:53)
Nice. How big is the group that's contributing actually? Is it two of you and then Pia was part of it? Or is it bigger than that? Is it open source and there's lots of people that are kind of adding?


[Enrico Bottazzi] (59:54 - 1:01:05)
Yeah, no. I think, so the main group is me, Sora and Pia.


But I think a big shout out, as I mentioned before, is on the OpenFHE guys, which our whole library relies on and which we kind of, let's say, we relied a lot on them on trying to understand how this BGG+ thing works and also on the optimisation side.


But we're also very welcome to have new contributors and we probably should also be more active on the documentation and on the communication side to make it easier for other developers to jump into that.


But I think what I want to make sure that passes as a message from this podcast is that, iO sounds this kind of crazy thing that is inaccessible, but it's literally just a jump from something that already exists.


So if you already have some skill in lattices and in FHE, I think that the jump is not too long. You will be very, very equipped with everything you need to start contributing and to start helping us optimising this BGG+ encoding that we spoke a lot about.


[Anna Rose] (1:01:06 - 1:01:23)
This is a very off, like left field question, but you mentioned it's sort of... I mean, you've said this throughout the episode that lattices are kind of at the heart of this. 


Does this make iO post-quantum secure? Or is it like one part is lattices, but actually there's all sorts of cryptography that could be broken?


[Enrico Bottazzi] (1:01:23 - 1:01:36)
Yeah. I think as long as this security assumption that we added are secure and as long as the learning with error assumption is not broken, I think we can claim that iO is also quantum safe. Yes.


[Anna Rose] (1:01:36 - 1:01:36)
Okay.


[Sora Suegami] (1:01:36 - 1:02:18)
I think I should mention that at this time, as far as we know, we only have like a iO construction from standard assumption, which is not post-quantum because these kinds of things, all of the standard assumption constructions are right on a bilinear map [?] pairings.


And there are many purposes of lattice-based iO constructions. But all of them rely on some kind of new lattice assumption. So as Enrico says, if these new lattice assumptions are really secure, both against classical and quantum adversaries, we can say we will have, eventually have post-quantum iO potentially.


[Anna Rose] (1:02:19 - 1:02:23)
Sort of, answer kind of, sort of depending. Okay.


[Sora Suegami] (1:02:23 - 1:02:24)
Yeah, yeah, yeah.


[Anna Rose] (1:02:24 - 1:02:37)
Got it. I want to say thank you to both of you for coming on, sharing the story of Machina iO and your interest in iO, the work you're doing to implement it, where we're at in practical iO.


Yeah. Thanks so much.


[Sora Suegami] (1:02:37 - 1:02:37)
Yeah. Thank you very much.


[Enrico Bottazzi] (1:02:38 - 1:02:40)
Thanks, Anna. Thanks, Tarun.


[Tarun Chitra] (1:02:40 - 1:02:42)
Good to be here. Thanks for coming on.


[Anna Rose] (1:02:43 - 1:02:50)
I want to say thank you to the podcast team, Rachel, Henrik, Tanya, and Hector/


And to our listeners, thanks for listening.