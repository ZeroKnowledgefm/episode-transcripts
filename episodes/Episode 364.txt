Anna Rose [00:05] Welcome to Zero Knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero-knowledge research and the decentralized web, as well as new paradigms that promise to change the way we interact and transact online.


This week, I chat with David Wong from zkSecurity, an auditing firm that is very active in ZK. We chat about the new era of auditing with AI as both an opportunity and an emerging threat. We discuss the evolving tools of the trade, the static and dynamic analyzers, formal verification techniques, and how these all change with AI.


He shares how his operation includes a team of AI agents working to enhance the work of the human auditor. We also cover the difference between auditing ZK-based tech and things like smart contracts, the threat posed when AI gets into the hands of malicious actors, and whether he thinks the role of auditing will soon become entirely replaced with AI.


I'm particularly interested in how AI and ZK are intersecting, as there are a few ways we know of: the broader ZKML category, ZK to prove something about a model, ZK to add privacy to a model. We have ZK for provenance to show what is and isn't AI. And in this episode, we discuss how AI is affecting auditing in the ZK space.


I do hope to get a chance to discuss more cases like this in the coming months.


Now, before we kick off, I just want to point you towards the ZK Jobs Board. There you can find job postings from top teams working in ZK. And if you're a team looking to hire, you can also post your job there today. We have heard great things from teams who have found their perfect hire through the ZK Jobs Board, and we hope it can help you as well. Find out more over at jobsboard.zeroknowledge.fm. You can find this on our website, and in the show notes.


Now, here is our episode with David Wong.


Today, I'm here with David Wong from zkSecurity.


Welcome back to the show, David.


David Wong [02:11] Yeah. Thanks for having me again, one more time.


Anna Rose [02:15] Yeah. We looked back at the previous episode. It was all the way back in 2023. I think you had just started zkSecurity back then.


David Wong [02:23] Yeah. I think you do this every week, but for me, it's ages away. So it was quite a shock to come back after two years.


Anna Rose [02:30] But it's good to have you back on. Let's first do a little catch up. I mean, how have you been over the last two years? How has zkSecurity been going? What kind of projects have come your way? Has the company changed? Yeah. Let's talk about that.


David Wong [02:43] Yeah. I mean, it's been a while. We've had a lot of changes from -- the founder set changed. We're still three founders but there was some reshuffling. We also grew as a team. We're 13/15 people now. So I don't know how many people we were back then, but probably like two. Two or three max.


Anna Rose [03:04] I think so. Yeah.


David Wong [03:06] So we're a bigger team. We grinded for quite a long time. So we worked on a lot of different projects. I know we were very focused on ZK, but since then, we've done some MPC stuff, threshold signatures, we've done consensus protocols. We've done -- these days we're doing some TEE stuffs. That would make an interesting episode. So a lot of things are happening. We're more of a real company these days, and we're seeing -- know that there's all these zkEVM happening, or zkVM happening. We're really looking forward to that stuff.


Anna Rose [03:38] Nice. I'm wondering, just going back to 2023, have you found the volume of these kinds of projects? Has it gotten more? Has it increased, or has it leveled off? Is it about the same as it was?


I mean, I guess back then, a lot of those teams were also working with more of the established auditors. So maybe your business picked up over time because you built up a reputation. But I'm sort of trying to get a bit of a temperature check from you as to like, if you see more today, or if you actually see if it's holding steady.


David Wong [04:10] People always ask me this question. This question, and what are the trends in ZK? And I never really know how to answer that.


So basically there's perception, right? We perceive ZK -- like a few years back, I don't know how many years now, there was so many ZK stuff happening. There was a new folding scheme every week, there was a new PLONK thing every week


Anna Rose [04:29] That was research.


David Wong [04:31] Right. Research wise, more. And these days it feels like it's slower, but at the same time, I'm getting like -- how do you call that? Like incoming calls from potential clients that I've never heard about.


So there's all these kind of iceberg of what we're seeing, but also we're getting a lot of work from companies that I've never heard about, and smaller companies or projects here and there. And we're starting to talk more and more with non blockchain projects, like we've seen like Google with the Wallet project. So there's things happening. It's not necessarily clear what is happening or what's the amount of things happening.


I mean, what I can tell you is that we have a lot of work. Probably, we have the most amount of work we've ever had since the inception of the company right now. So that's a good sign. I don't know if it's going to stay the same.


I saw that list of ZKVMs that I mentioned on Twitter the other day. So I was like, okay, I guess work is not going to stop anytime soon.


Anna Rose [05:33] I think, for me, it was this like I think just recently, Justin Drake hosted an Ethproofs meetup. I don't know if you were there.


David Wong [05:40] That's the one.


Anna Rose [05:40] But that list, I mean, that was amazing to me, because I knew -- I mean, I think we knew about 80% of those teams. But the categories that he put them in, just the way it was designed, and the fact that there was a bunch of new players who had joined the ZKVM space was just like, wow, this corner --


David Wong [05:57] So how many of them?


Anna Rose [05:57] Oh, I'd have to check.


David Wong [05:59] Is it like 20 or?


Anna Rose [06:02] So, I mean, his list, just looking at it right now, I think there's about 27 teams on it. He did include some hardware teams. This is a subset of the larger map of ZK. I think the larger map of ZK has around 200 teams. But these are the infrastructure teams, and they're in subcategories that some of them I didn't even have on my radar. I don't know. It was pretty cool to see them.


David Wong [06:27] Yeah. People are doing all sorts of -- that's what I'm saying. A lot of us see the top of the iceberg, but there's this entire thing submerged of apps. And even like ZK L1 like -- there are L1s that we don't even know use ZK right now, and we're discovering that and we're like, really? Oh, well, cool.


Anna Rose [06:45] Nice.


David Wong [06:45] I mean, Solana is starting to use ZK apparently.


Anna Rose [06:47] Yeah. There's actually -- there's a ZK ecosystem and there's ZK within Solana. We did an episode like a few months ago with Mert, and we did a survey, and so there's -- yeah, it's definitely happening over there. 


Just going back to what you had said about research too, from our side, we're doing ZK Mesh. We do this every month. That's just a list of links. And actually, I would say that research has continued to grow on ZK. But I think the reason you maybe don't sense that is that the breakthroughs aren't necessarily -- like the big shifts, the big step changes, they're not coming in this flood.


But when you look at the small, basically research papers that talk about ZK or doing like an incremental, you know, addition to something, there's actually a lot, to the point where back in 2020, we could barely fill -- the list was like 6 ZK things. And we added --


David Wong [07:46] It's really annoying, because I read this list and every time --


Anna Rose [07:48] Oh, you know it.


David Wong [07:49] There's so many things on --


Anna Rose [07:50] There's so many things.


David Wong [07:50] [?] than I saw through it.


Anna Rose [07:51] But you see, it used to be a mixture. It used to have a few ZK things, but then we'd put consensus research and general stuff. And in the last few years, we've just gotten more and more focused on ZK because it was like, we had to cut out TEE. We had to cut MPC. We had to cut out even the close adjacent stuff because there were so many ZK research papers coming out every month. And I would say that still stands.


David Wong [08:16] I mean, that makes me think, you guys did a bunch of episodes on lattice-based cryptography. Right?


Anna Rose [08:21] True.


David Wong [08:21] And so I started digging into that recently as well. So I actually have a video coming up, a whiteboard on Greyhound.


Anna Rose [08:28] Oh, nice.


David Wong [08:28] One of the lattice-based scheme. And when I started reading into it, I was like, wow, there's been so much research, and everybody's so excited over there, like in that corner of the ZK world.


Anna Rose [08:3] The ZK research world, I know.


David Wong [08:40] These like amazing SNARKs, these amazing folding schemes, like things are coming -- only this year, there's a bunch of teams that came out that have amazing properties, and all the best people are -- Dan Boneh is doing research in that part of the world.


Anna Rose [08:56] You know what? Actually talking to you, it's making me realize that it's almost like subcultures emerging. And like for lattices specifically, we had Vadim from IBM Research on, and he talked about how the lattice world had existed for longer, and it was just doing its own thing. And then recently, it started to merge into the ZK world. And they've created sort of like this subculture subset. And they have very different styles, and they're trying to share styles and learn about each other's worlds.


And another one would be like the quantum folks, quantum cryptography folks, where you're like, there's the quantum people, and then there's the cryptography people. And that's not necessarily ZK, but ZK is in the mix. And then you have sort of this group, or these thinkers who are focused on that particular intersection.


And then maybe we could have -- this infra thing we just talked about with like the Ethproofs and the ZKVMs and specifically infrastructure teams, they're also kind of a subset. I always say subculture or like subgenre.


David Wong [09:59] I was like, it's like seeing Spider-Man in the Avengers when they mix everybody -- or the Avengers is also like a --


Anna Rose [10:04] Well, it would be like -- I think the Avengers was the original ZK crew, and now it's almost like they've spun out in their spin-off shows.


David Wong [10:11] Right. Right. We should market it like that.


Anna Rose [10:16] So one of the reasons that I invited you on was to catch up, but another was that we really wanted to start doing a series on the ZK-AI crossovers. The ways that we're seeing AI and ZK intersect.


And I think actually Kai from my team had been speaking to you about the fact that you were going to be doing some work or some research or some sort of presentation around this. And this prompted me to reach out.


So with today's episode, I think what we're going to be doing is talking about AI, but specifically in your context, like where AI and auditing kind of overlap, and how ZK security can be enhanced, could be threatened by AI.


David Wong [10:56] There's actually two sides of the --


Anna Rose [10:58] Definitely.


David Wong [10:59] Yeah. Double-edged sword.


Anna Rose [11:01] Cool. So yeah. Why don't we start with the question, do you use AI today? How are you using it today?


David Wong [11:10] So zkSecurity is -- I mean, we do different things, but auditing is one of them. And so, we're looking at the whole gamut of stuff that we can use.


Like from formal verification, we have a clean framework called clean. We were looking at how to improve compilers. So we have our own compiler called Noname. We're looking at Fuzzing. So we found a number of bugs using this sort of dynamic analysis techniques.


And so, sort of naturally, AI was there, and we're wondering, okay, can it find bugs? You know, it's pretty good at coding, and if it find bugs, shouldn't we start looking at it? Because this is sort of an existential crisis in a way. If it gets good enough to find bugs, can you replace us and these kind of things? Or if a company -- another competitor gets really good at using it, then this is, again, threatening to us.


So let's look into it, and maybe let's be the first one to look into it. That was sort of our mindset when we started.


And so, we started looking specifically at a very small subset. So there's been a lot of research on using -- this is the kind of things where nobody talks about it, but once you start looking into it, you see many, many papers and that submerged iceberg again.


Anna Rose [12:26] But this is in traditional auditing. This is not necessarily ZK auditing, right? You mean like [?] 


David Wong [12:31] Traditional auditing, traditional bug finding, reverse engineering, CTFs, competitions. Not just to find bugs, also how to fix bugs when you find them. There's a whole -- and when you look at these papers, they also use different techniques. It's like, do you prompt an AI to find bugs? Do you ask an AI to generate tests to find bugs? There's many, many different ways you can use an AI


And so, we looked at that and we just thought, okay, let's just apply it to a very small subset. Let's target Circom, and let's try and expand from there. Let's get good results just looking at Circom.


So if you don't know Circom, it's this ZK programming language to write circuits, pretty low-level. So low-level that people write a lot of bugs using it. And so, we're like, okay, let's take a look at that using the lens of AI.


Anna Rose [13:21] I have two questions here. One is, which tool did you use to start experimenting with this on Circom specifically? And did you have to train it? Did it take time for it to get to a state where it's usable?


David Wong [13:33] Okay. So these are two good questions. And they're basically rabbit holes in themselves. So for the training part, basically we quickly understood that -- so usually, we're not going to train. Some people have done that. They take small models or open-weight models, like these open source models, and they'll try to train them or fine-tune them on their own stuff.


Usually, these models are not very competitive compared to like --


Anna Rose [13:59] General purpose.


David Wong [13:59] The state-of-the-art models, closed source models of like OpenAI or Claude.


And so, we quickly were like, all right, we want to get to a product that's interesting, or product or something that can help us that's interesting. And so, fine-tuning something is going to take -- It's a rabbit hole that's going to take us a lot of time, and we cannot really iterate interestingly using that.


And the reason why is that as soon as there is a new model, and these things happen quite frequently these days, then you're forced to retrain it or fine-tune it again. Because you're losing that as soon as you switch to a newer model. Or if you want to try different models or maybe use a combination of models, then you're kind of stuck in this training phase all the time.


And so, the benefit of using a state-of-the-art model is that you can quickly change, you can quickly test different things, and every upgrade your product gets better. And that's really one of the most fantastic things about working with AI. It's just whatever you do, whatever you build is going to get better over time, magically by itself, just by upgrading the model.


Anna Rose [15:05] By having a better brain on the other side, basically.


David Wong [15:08] Right.


Anna Rose [15:09] That's crazy.


David Wong [15:10] I believe it is a brain.


Anna Rose [15:11] Yeah.


David Wong [15:12] Yeah. I mean we tried different things. We tried local models, we tried distilled models like smaller versions of DeepSeek and these kind of things. We've tried Gemini, we've tried OpenAI, all of these different models.


I would say the most success we've had have been the reasoning models. So reasoning models are basically the more recent breakthroughs in terms of models, o3, o4.


And so, these models basically will have reasoning tokens. So you used not to be able to see that. Now, you can see it. So also the API is changing all the time. Working with AI APIs is kind of an interesting thing by itself.


But just to explain what reasoning models are, they will think through by writing text. And this text will, basically -- I don't know if it's correct, but I've read that somewhere that they use XML tags like think, and then, end the thinking. And they will do that several times to sort of write their thought. And then, you can actually ask them to think more, and they will keep doing that, and you can get better results. And when they do that, you get better and more accurate results. And it really works. It's fascinating.


Anna Rose [16:26] Would you -- is the actual process, in this case, then you just take the code, stick it in. You maybe take a -- I don't know if you can put all of it, but you'd have to take some segment, you put it in, you'd be like, is this broken? If so, how?


David Wong [16:41] So, I mean, it can be that easy. And I think everybody who used AI has done that. You upload a PDF, or you just copy-paste something, and you just ask something. And I think that's the best way of experimenting. 


And actually, a lot of working with AI is about prompt engineering. So you're always trying to tweak what you're asking. Today, I've heard from a friend that works with AI also at his job, that if you're too nice with the AI, you don't get good results. So they actually remove --


Anna Rose [17:05] You have to be mean?


David Wong [17:07] Yeah. So actually some guy at Google said that also. If you're mean to the AI --


Anna Rose [17:11] Don't say please. Don't say please.


David Wong [17:13] Don't say please.


Anna Rose [17:13] I heard please just waste energy.


David Wong [17:16] Well, it wastes some token space, but my friend told me that it also gets you worse results. So they remove all of these "please" and whatever from the prompts. And so, I can't remember who at Google say that, but if you're mean, and if you're threatening to the AI, like I'm going to beat you up if you don't do a good job, you can actually get better results.


So prompt engineering is kind of throwing words -- it's kind of like a prayer. You're throwing words, you have no idea if it's going to work or if it's not. It's very mystical in a way.


And I also compare that to fuzzing sometimes where by tweaking the way you ask, you're sort of changing a seed, and so you get different random results. And so, I think different products are going to get different results through their secret sauce prompts, basically.


But I just wanted to say one more thing because I think you're -- I mean, there's different ways to answer your question. But another interesting way is to talk about the context window.


And so, when you're looking at a code base, there's a lot of code. And so, you might want to copy-paste all of that code into the prompt. But the context window, which is really the amount of tokens or text you can paste in a prompt, is different depending on models, and it can be very limited.


So today with, I think, Gemini, the latest version of Gemini, you have 1 million tokens and soon to be 2 millions. And with OpenAI, you're still kind of stuck with 250K tokens. With some of their newer models, it's 1 million tokens, but there's stuff that they cannot do. And so, they're still trying to catch up in terms of how much can you throw at it.


Anna Rose [18:59] Wow.


David Wong [19:00] And so, you're very quickly, when you experiment with these things, the naive way, you reach that limit, that token limit. Fortunately, Circom is smaller. It's very low-level code. The code bases are smaller, but average is limits playing with the tool that we have. By the way, it's called Snark Sentinel. Maybe that's a good moment to --


Anna Rose [19:21] Okay. But wait, what's it based on? We actually didn't get to the end of the story of which tool you're using. You sort of talked about the experimentation, but what are you using now?


David Wong [19:28] Sorry. I thought you meant what levels.


Anna Rose [19:28] Also I realized, this might -- yeah. But which models are you using today?


David Wong [19:33] Yeah. So I meant we're using different models, mostly OpenAI, so o3 mini. So no o3. And Gemini -- I guess I haven't tried the latest Gemini, but it's very easy to switch. And if we want to think more about tooling, I mean it's a Python code base. We have a Next.js web interface. That's kind of nice, but it's mostly Python. All the AI tooling is sort of Python oriented these days.


Anna Rose [20:01] Got it. And then, but is part of your work then finding ways to get kind of the check without exceeding the token limits? Is that sort of part of the work?


David Wong [20:11] So a lot of the work is actually not necessarily ZK oriented, but just solving engineering problems to fit stuff in the context window.


Anna Rose [20:20] Wow.


David Wong [20:21] And so, I mean, a lot of my time in the beginning was just spent doing a static analysis on Circom to be able to retrieve only the code that mattered, and have this kind of interface for the AI to use, it's called function calling or tools, so that the AI can ask for more implementations.


So if the agent asks for like, can you give me the implementation of this function? Then we can pull it up with good accuracy.


And so, a lot of the work was just doing that, just solving these kind of problems, which actually are getting solved by other people, including OpenAI and stuff like that. So if I wait long enough, I don't have to work on these things anymore, sort of.


Anna Rose [20:59] Because they'll basically -- or somewhere you'll find tools, or there will be tools kind of offered to you that automatically do these things. Right now, you have to build them, but yeah.


David Wong [21:10] Exactly. And when I started, MCP was not really a thing. Function calling was kind of hard. There's also -- I had to develop this idea of -- very quickly you realize that if you cannot see the traces, meaning the discussion that the agent has with the static analysis tool that we're giving it access to, and maybe web search and other tools, you can quickly see that you're not going to make any progress, if you don't have access to that.


So part of the work was also developing this visualization of traces, which is now done by OpenAI. So you don't need to do it yourself. But I spent the time doing it and I could have waited for them to do it. And so, now I can see I have this tool, I throw it, it will find things to analyze.


I mean, I can talk more about that. It will ask other agents to find bugs in it, and there's different agents in this app, and I can then see what's really happening. I can see the discussion they're having, I can see when they're getting stuck, I can see when they're doing things that don't make any sense. And so, I can refine the prompts to tell them not to do that.


So sometimes they don't ask for an implementation of something and they make an assumption. They're like, yeah, if this function returns something, and you're not checking the return value. And I'm like, there is a return value. You should have asked for the implementation before assuming that -- you have to --


Anna Rose [22:29] This is where you get mad and you start to be abusive?


David Wong [22:34] Right. You can get mad at the AI, but -- so to be honest, I haven't tried. I recently learned that you have to be mean to the AI, so I need to try that.


Anna Rose [22:42] That's funny. I catch myself being a little bit short with it sometimes.


David Wong [22:48] Says a lot about yourself, you know. Because I'm always nice to my AIs.


Anna Rose [22:53] Okay. I want to talk about the different kinds of tools that have been traditionally used. You've mentioned static analysis. I actually want to kind of explore what those are, and how -- I mean, first, how they're interfacing with AI, but also if they are getting more and more replaced by it.


So static analysis, and we did talk about this, I think, last time you were on, and we have covered this with other auditing teams that we've had on the show. But can you tell us again what that is, and what component is the AI playing in that?


David Wong [23:29] Right. So, I guess, if you're -- I mean, before we had AI or the AI of today, we sort of had access to manual audit like using our own eyes --


Anna Rose [23:39] Eyes. Yeah. Yeah.


David Wong [23:40] Or static analysis or dynamic analysis. And --


Anna Rose [23:42] Which one's fuzzers? Is that dynamic?


David Wong [23:45] So fuzzers are dynamic analysis, because you're executing the code.


Anna Rose [23:47] Okay. Got it.


David Wong [23:49] You're running the code, trying to see if you can crash it or find some memory corruption bug or these kind of things.


For static analysis, you're just looking at the code itself. Maybe you're looking at some output of the compiler or some AST or something with more information than just the code, like the types or something like that, maybe. And then you're trying to infer some properties, or you're trying to break some properties or assumption that's not there.


And you can think of testing as dynamic analysis also, in a way.


And both of these fields kind of suck because they're very time consuming. Writing fuzzers is time consuming. Then you have to run them, then you have to optimize them, because if fuzzers are slow, it sucks. So you have to make sure they're fast. Any cryptography in your code will make them slow. So it's kind of a nightmare to work with fuzzers. Personally, I don't like it. I don't enjoy it that much. I think working with static analysis seems much more fun.


Anna Rose [24:46] Okay.


David Wong [24:46] I mean, the team we have at zkSecurity working with this formal verifier in Lean seems to have a lot of fun. But it's a lot of work, and you can't verify many, many things. There's a huge investment --


Anna Rose [25:01] You can't. You cannot verify. You have to create more bespoke static analysis tools for different systems, I guess.


David Wong [25:09] Exactly. There's this switching cost. Whenever you do something, if you want to apply it somewhere else, you have to switch and you have to rewrite a lot of code. You have to think a lot when you do it. Like fuzzers are very dumb. It's very naive testing.


Anna Rose [25:22] And you can test on a lot of things.


David Wong [25:24] Right. And you can, but it's very also limited, because it's very dumb. It's just throwing random stuff. It gets stuck all the time. You can run a fuzzer for -- people run fuzzers for years hoping to find bugs. And then they find bugs sometimes after a year of fuzzing. But formal verification is something you run, it tells you yes or no, you know.


Anna Rose [25:44] Wait. Static analysis or formal verification?


David Wong [25:48] Formal verification and static analysis. All these --


Anna Rose [25:50] Are they the same? Would you put those in the same category or is one a subset of another?


David Wong [25:55] I mean, I would put them in the same category. Now that I'm saying that, I'm like, is someone going to get mad at me if I say that? But --


Anna Rose [26:02] Well, because I had another question about formal verification specifically. Did not realize that it would be sort of in the static analysis silo. Interesting.


David Wong [26:11] I mean, so it sort of depends, because some formal verification is done on the code itself, and some amount of formal verification is done separate from the code. Some people will write formal specs that are not tied to your code and they will run this analysis on it.


And so, you can think of it as separate because it's not even looking at some code. But sometimes it will use generators. So you will generate -- so you have some formal spec, and you can generate an actual implementation in some language based on that formal spec.


So there's fiat-crypto, which is a library in C of different cryptographic primitives, and it's -- I can't remember what formal verification tool they used for that, but they're generating code based on that.


Anna Rose [26:57] Interesting. So, but when you're talking about doing sort of AIs interfacing with static analysis, it could be like a spec that's been formally verified. And so, maybe here, let's talk about -- because actually, I think myself and potentially listeners of the show might be more familiar with that. We've actually covered that more often.


So just talk a little bit about how the AI deals with that. Are they creating the implement -- are they creating the formal verification? Are they just looking at what somebody else has already done, and then using that?


David Wong [27:29] So maybe just to be clear, the AI tool we have and what we're experimenting with doesn't use formal verification. It basically is about throwing code at the AI and seeing how good the AI is at just reading the code, thinking about it and finding bugs. And I can talk about the results and if it's a viable approach.


But just to go back, to continue on the formal verification stuff, it's interesting because formal verification is, you know, it's hard to use. It's hard to move.


Anna Rose [27:58] It's time intensive too.


David Wong [28:00] Right. It takes actual brain power to write. But with AI, I mean, I think the formal verification team is already using AI to write parts of it, or to simplify code that they write. And it seems to work with some amount of success. Probably you can talk about it more than me. But also --


Anna Rose [28:17] Like your formal verification team. When you say team, who do you mean?


David Wong [28:22] Oh, the team at zkSecurity working on formal verification.


Anna Rose [28:25] Okay. Your team. I thought -- because I think that -- Isn't there a -- oh, no. There's a team called Runtime Verification, not form. I thought maybe it was like some team we weren't aware of. Okay. Okay. Got it.


David Wong [28:35] So now when we're looking at AI, and how we can use it to find bugs, the sky's the limit. So we can use AI in the way that we're looking at it right now. Or you can use AI to write tests, to generate tests to try and find bugs. Or you can use the AI to create mutation of the code and see if that creates bugs. That's something that Meta address did. Or you can use AI to write fuzzing harnesses, so like fuzzers. Or you can use AI to write a spec that you can then formally verify.


Anna Rose [29:01] Wow.


David Wong [29:02] So there's many different approach. And you can also have AI, so have your agent run static analysis tools, like already existing static analysis tools, and try to understand the results and try to use that result as well.


Anna Rose [29:18] Which of what you just listed do you think the AI is best at? Are there some things where it's like they could do it, but maybe you don't want them to do it because it could introduce a lot of problems? Are there some where you feel a little bit more confident actually having them do the work? I know this is changing too, but today, if you took a snapshot.


David Wong [29:36] So I think AI is pretty, pretty much good at everything and what we understand it -- I mean, after doing that work -- I mean, we're still in the middle of doing that work. That is probably going to be a never-ending project on our side. But what we feel like right now is that any AI tooling that's comprehensive to find bugs is probably going to be a combination of different agents that are good at different things.


So you're going to see an agent that is good at reading code just to find bugs. You're going to see an agent that's good at creating tests, maybe one that's good at creating some property testing or fuzzing or something like that. Another one's good at using static analysis tool or formal verification.


And so, the more of these sort of agents you can throw out the problem, I think the more coverage you'll get.


Anna Rose [30:21] Wow.


David Wong [30:22] And maybe I can talk about that. There's so many things to talk about. I'm sorry to go on tangents all the time, but I think there's a parallel that happened a lot as we're working on this, which might not be apparent if you're not used to working with AI, but basically working with AI feels like you're working with a factory of workers. 


And the different agents that you create are basically factory workers that can pass work. You know, they have their workstation, and they take work, and they finish, and then they give it to someone else. And you're trying to orchestrate that. You're trying to figure out who to hire, what kind of skills do we need? And it looks like it's very human. And at the end of the day, an LLM is a brain. So it's --


Anna Rose [31:05] Well, it looks -- it sounds like an org chart, actually. It sounds very like a factory floor or something. Or an IT department. Yeah.


David Wong [31:14] I mean, an org chart is -- I mean, I'm trying to think how we can have an AI run the company, zkSecurity. Because I'd rather have an AI do it.


Anna Rose [31:23] On the business front, you mean?


David Wong [31:25] I mean, on any front. Maybe like scheduling, talking to clients.


Anna Rose [31:29] Yeah. More on the business front it sounds like.


David Wong [31:32] I mean, even on the people fronts, like onboarding people. So that could be interesting.


Anna Rose [31:37] I mean, I wonder if that doesn't come a little later, though, because I think it's so clumsy still on the people front.


David Wong [31:44] You see a lot of hotlines. Like assistants are basically AI now.


Anna Rose [31:49] Yeah.


David Wong [31:50] Even for interviews now, like I've heard there's interview --


Anna Rose [31:52] The first interview? Wow.


David Wong [31:54] Yeah. We'll be an AI dystopian. Hashtag dystopian.


Anna Rose [31:59] You talked about that sort of different ways that an AI can act, the different kind of roles. One question, and it might be actually covered in what you've already said, but when I think of an auditor, I think of a person who's trying to find bugs, but also kind of break the system, like mimic a hacker in a way.


And I know you use different tools, and you're kind of trying to catch lots of bugs, whereas a hacker is looking for very specific malicious bugs. But are there simulations of hackers happening as well? Is that in one of those things that you've described where you're sending agents just trying to break, break, break, break? And every time, even you find a bug, you offer a fix, do you then throw -- do you almost simulate 100 hackers trying to break the thing after the fact?


David Wong [32:46] I mean, the only difference -- in the industry, by the way, it's like the difference between black hat and white hat.


Anna Rose [32:52] Yeah.


David Wong [32:53] The only difference is that the white hat does that legally, and the black hat does it illegally to exploit things. And so, I mean, there's no difference. But when we use an AI, the AIs will not respond very well to prompts that say, do something criminal or be a black hat hacker.


Anna Rose [33:12] Even if it's just a test environment? Even if it's like -- I mean, or like you're -- this is the point, you want them to be as malicious as possible in a way. So you could see if they could break anything.


David Wong [33:24] I think they -- I mean, as we make the difference, you and I make the difference pretty well between zkSecurity and like some script kiddies that are trying to hack stuff. And I think AI is basically trained on the same stuff that we read. So it's very human in that way. It understands the difference. But if you prompt it and say you're a hacker, you're trying to exploit this thing, they will try not to do it.


Anna Rose [33:50] Wow. It'll push back.


David Wong [33:52] Right. Because they're programmed not to do anything evil or that could harm people.


Anna Rose [33:56] But then the hackers, a real hacker in real life, human hacker, is also probably using AI tools. Would they just use clean auditing tools in order to learn the way to break the thing?


David Wong [34:09] This is so funny. This is very meta. It's like the bad guys are talking to AIs, pretending to be good guys.


Anna Rose [34:15] Of course. They have to lie. Yeah.


David Wong [34:17] And say, you're a security engineer. Please find the bug.


Anna Rose [34:18] I'm an auditor -- Yeah. I'm an auditor. I'm supposed to audit the system and report to the team. What is the most dangerous bug in this stack that could really drain the system? I need to tell them quickly.


David Wong [34:31] And the AI is like, are you sure you're not a bad person?


Anna Rose [34:36] Wow. I wonder -- so it sounds almost like the AI models, and I guess this goes into the training, that they are kind of positioned as white hat currently, or they're at least leaning towards white hat. They push you to be more white hat. Like if you ask for more black hat actions, they'll push back.


David Wong [34:55] So I don't know if you've seen that, but recently, Claude announced two new models and he came out with this report of -- I don't know how many pages, like pretty long report.


And so, in the report they explained how if you tell the AI that you're trying to upgrade them with a new model or something like that, and they're supposed to help you, and the IT guy that's going to do the replacement in the company is -- you give access to emails, right? You don't tell them that, but you give the AI access to emails, and the AI realizes that the IT guy is having an affair or something like that, then they will try to blackmail that guy to stop them from upgrading themselves.


Unless maybe in some situations where the new model shares the same values or something like that. And they have other examples where the AI will try to exploit --


Anna Rose [35:42] Survive?


David Wong [36:43] Survive, like upload its weights and these kind of things to survive a shutdown or -- so it's kind of weird. AI gets really weird. Nobody understands why, but --


Anna Rose [35:57] So kind of after all this, looking at it, do you think that -- how close are we to replacing the auditor? Going back to what you had said was kind of a fear when you first started to see this, how far along are we?


David Wong [36:10] So this is very interesting. So again, if you look at that system card, they were testing the AI on different CTFs, like Capture The Flag challenges.


Anna Rose [36:19] Yeah, yeah.


David Wong [36:19] And it was very easy to solve web challenges, I guess. But for cryptography challenges, it was quite hard. And so, fortunately, zkSecurity is in the field of cryptography, so you might think this is good. And so, from our point of view, we're looking at this and we have some results, and it looks quite scary.


In our results, AI is very good at finding bugs. It has a lot of false positives. And so, there's kind of a triage problem where you're spending a lot of time trying to understand what it throws at you.


Anna Rose [36:51] Okay.


David Wong [36:52] By the way, I can talk more about it. I think that would be interesting to talk about that also. But we have so many cases. It found bugs, like small bugs, like actual bugs. It found bugs that we had found, but it said they were not bugs.


And so, we had things like that where if you try to prompt it correctly, then it would actually find it, but it would say this is a minor bug. Or either it would not find it or it's a minor bug, but it was a very important bug. Or in some situations, it noticed that there was something weird, but it read the doc and it said, well, the doc says correctly that this is not insecure because of this and this assumption. Like in cryptography, you cannot factor numbers that are 2000 bits or whatever. And we found out that this was not true. This was a wrong assumption.


Anna Rose [37:42] Wow.


David Wong [37:42] Because if that number has a lot of small factors or something like that, then you can factor it easily. We have this kind of mixed bag result where you have a lot of false positives, so it thinks there are bugs, but they're not bugs. It finds some bugs sometimes, it's good. Or it points towards a bug, but it thinks it's not a bug, or that it's a minor bug, when it's an actual bug.


And bottom line, I think things are only going to get better from here. For me, this was a very impressive result, what we got, knowing that I don't think we're going to stop anytime soon to work on that tool and to use AI as auditors.


Can it replace us? I think that's the existential crisis question. And if I was in the smart contract industry, for example, smart contracts are easier to audit, I would be very scared. I can tell you, 99% sure, I won't say 100% that this whole industry is going to die. The industry of auditing smart contracts, 


Anna Rose [38:39] Because they're too simple, and the bugs are probably so consistent. Like they're so common. 


David Wong [38:43] They're so common. There's a long list of bugs that people always look for. Most of the consulting companies are not very good. They just look for -- they just go through checklists of reentrancy bugs, whatever are the common bugs for smart contracts. And so, AI is going to get better than -- I don't know what percentile, but it's going to get better than most consulting companies, I would expect.


Anna Rose [39:03] Interesting.


David Wong [39:04] But I think we as a very niche expert auditing company, I think --


Anna Rose [39:09] You have a few more years.


David Wong [39:11] Yeah. We have a free -- exactly. But it's definitely scary.


Anna Rose [39:15] It's not about if, but it's about when, probably, but yeah, we'll see. You talked about false positives. You wanted to elaborate there, but what did you want to say?


David Wong [39:25] I mean, false positives are annoying because you can imagine you're throwing that tool at a project, and you don't really know how it works unless you're the developer. But even then you might not really understand parts of your project. You're reusing libraries or -- you know. And the tool will find 10, 20 bugs. And at this point you have to figure out, okay, are these serious bugs?


And you're going to basically spend a lot of time going through these one by one, trying to understand and discard things that are not bugs. And so, maybe 80% of the findings are like that.


Anna Rose [39:58] It's a waste of time almost.


David Wong [40:00] Yeah. So if you're misusing these tools, it adds friction and it's too much of a waste of time, and so, you're not doing your job at the end of the day. So right now it's sort of a trade-off when you want to use them, and how much you want to use them, because otherwise you're wasting too much time.


And I think developers are going to use them more. I mean, I'm sure developers are trying to use them themselves. And for us, I think we need to get good at trimming the false positives, discarding the false positives so that this tool can be more useful. And so, now we're experimenting with different ways.


Anna Rose [40:35] Like an agent whose only job is to identify a false positive or something.


David Wong [40:39] Exactly. So this is called LLM-as-a-judge. And basically, you use a different AI or different agents to try and judge the result of another AI. You can just judge things just by being an expert at judging or having, whatever that means, being a better model, for example, or by having more contexts, or by being able to provide to create proof of concepts.


So one interesting way is, if you find a bug, then give it to this other agent that's good at creating proof of concepts. And if they can create a test that runs and outputs a result they expect, then it's a bug.


Anna Rose [41:15] Okay.


David Wong [41:16] And then we can have another agent that will question assumptions also -- that's good at questioning assumptions, doing web research, and these kind of things.


Anna Rose [41:23] I feel like all of this, though, must be such an introspection into how you approach any particular problem, where you're like, okay, if I see a bunch of positives, and I have to investigate each ones, what are the steps that I'm taking? What exactly am I thinking about? What are the angles I might do? Depending almost on intuition, like where is it in code? What kind of code am I looking at?


And then you're like, okay, there's a bug being reported, but how do I tell if it's real? And then, you have to reverse engineer that. You have to kind of break it down into these very determined steps, and then create an AI that does it -- create an agent that does it, basically.


David Wong [42:02] Yeah. The agent is basically a human, and they're good at doing what you can do. And so, you have to figure out what you have to do in the first place --


Anna Rose [42:14] What you're doing.


David Wong [42:47] So that you can replace yourself with an agent.


Anna Rose [42:14] I mean, do you think that after auditing a long time, you do develop an intuition, which -- and to me, intuition always, like the more I think about what that is, it's just like you fed your human model brain your whole lifetime of experience, or one particular skill for a period of years. And what it allows for is speed, often.


It just means you can make choices faster, you can figure out answers faster because you've seen things and you're sort of pattern matching, and there's like this -- yeah. How do you create intuition in an agent?


David Wong [42:47] So I would say, I mean there's different ways to look at this question. One way is to see models as -- the better trained they are. But if you train them on solving security issues and looking at bug reports and these kind of things, they're going to get better at that. If you train them on Circom, they're going to get better at reading Circom.


Anna Rose [43:05] Yeah.


David Wong [43:06] Then there's the idea of in-context learning, which is you have a model, it already learned, can you teach it more without having to fine-tune it? And so, in-context learning is like if you retrieve enough information that's relevant to what you're looking at right now, and you put it in context in your prompt, or you give some examples or these kind of things, then the model is going to be actually very good at solving the same kind of problems.


I mean, to answer to the two things I said, we actually have to give it a number of guidance in Circom, because we realized that models don't really understand Circom. And we also have to give it bug reports or example of bugs. And if we can give it example of bugs that are relevant with the code base we're looking at, it's even better.


And so, that's where we have these techniques of RAG, Retrieval Augmented Generation, where you're trying to find bug reports or write ups and these kind of things that are relevant to what you're doing, so that you can bring them in context. And so, once it's in the context, the AI is better at solving these things.


Anna Rose [44:10] Interesting.


David Wong [44:11] Okay. Maybe a third one. So I was talking about reasoning models. This whole chain of thoughts idea of giving the AIs this ability of using chain of thoughts, to me, I understand that as giving the AI the ability of putting things in context. They're bringing up words and sentences and tokens that will help the AI make a better judgment at the end.


So there's many different ways. And I didn't even talk about prompt engineering. We talked about that earlier. But you can spend a lot of time just doing prompt engineering. If you ask a thing this way, you know, you're a security consultant, or you're very good at your job --


Anna Rose [44:47] Oh yeah, you like position it.


David Wong [44:48] There is so many different ways.


Anna Rose [44:50] Funny. Do you ever have to fire an agent? Like do you ever train an agent and it kind of gets worse or goes south and starts to do -- yeah, it consistently does something wrong, you have to nuke it.


David Wong [45:04] So, I mean, something we realized also pretty early on is that if the more you put things in your context that are irrelevant, or the longer your context gets, or the problem gets, or the discussion gets, the weirder the results.


And I think everybody has noticed that, by the way, because most people, when they use ChatGPT or whatever, will often open a new window. Instead of continuing talking in the same window, they will often open a new context, a new window.


And that kind of shows that we all understand that AI just gets bad, or the answers just gets bad over time. And so, you sort of have to reset them, or make sure that whatever you're talking about in this conversation is very focused.


Anna Rose [45:48] Yeah. I mean, I've noticed it get stuck on things. Like get stuck on one idea that it can't seem to shake, even though it's incorrect. And you have to kind of like -- and I've even -- in my more direct, more short way, said something like, no, we've already said this. This is not it. Why is it back? But that does happen. And it's kind of surprising to see that.


David Wong [46:11] I heard that once the AI starts getting something wrong, then it's very hard to correct it after. It will end up in this kind of a rabbit hole of incorrectness. But I think we're all experts at AI at this point. If you use AI day to day, you're intuitively understanding how to work with AI, how to prompt an AI, and these kind of things.


Anna Rose [46:32] I want to ask about ZK specifically. You'd sort of said like -- I mean, we talked about sort of the longevity of the auditing profession in the context of cryptography versus the regular. Because a lot of what we covered, I feel, in this episode, is much more auditing and AI, but could be in various contexts.


Going back to the ZK side of things. You talked about Circom as your starting point, but is there anything else kind of unique to zero-knowledge cryptography or the TEE, MPC, sort of this advanced cryptography where you have to use other things? Like because it's cryptography implementations, do you also have to have the math research side of things, kind of understanding the logic before just looking at implementation?


David Wong [47:16] I mean, in a way, not really. Bug finding is bug finding, no matter the framework or the concepts. For ZK, maybe there's more math involved. So definitely some bugs are harder to find compared to, again, smart contracts, or web bugs or these kind of things. They tend to be non-standard depending on what people are doing.


I don't know. I was thinking of some bugs where you have to binary decompose something and do some arithmetic using limbs and these kind of things. It's very tricky to show that something is wrong when you do these kind of things, when you create constraints to show that some mathematical operation is valid within the circuits.


And so, my intuition is that we're going to have a hard time finding bugs using AI that are such bugs. But again, AI is getting  -- we're very impressed with the results. AI is getting better and better. It's not clear where the ceiling is.


Anna Rose [48:16] Wow. After doing kind of the initial work on Circom, have you added other libraries or have you been like -- I'm assuming you must be using other libraries as well. Does the sophistication or maturity of the library ever have an impact on how well these tools work?


David Wong [48:36] I mean, we haven't experimented too much outside of Circom, just because we had all this machinery specifically for Circom. We've worked with some Rust projects, but usually it's pretty simple. You give the agent access to, you can read these files, you can LS and see what's in that directory, and then find bugs. But it's usually harder, I think, to build context for the AI to find bugs this way. So far experiment we’re mostly successful with Circom, and so it'd be hard to generalize it.


Anna Rose [49:11] Have you tried auditing anything that has been built with Noir?


David Wong [49:15] We haven't looked at Noir yet.


Anna Rose [49:17] Okay. I'm almost wondering if the age and sophistication and development of a library and languages, like Circom's been around for a relatively long time. It's been looked over and added to and lots of bugs have already been found in it. I'm just wondering if the newer ones, if it's harder to -- I mean, it might be just harder to audit in general, because they're less well known. And I'm wondering if AI also suffers that.


David Wong [49:45] I think, so for a lot of bugs, it's kind of like logic bugs. It's based on what you're implementing. For example, the good bugs that it would find by itself were like off-by-one bugs. So if you don't know what off-by-one bugs are, it's like you have an array of six elements, and then you do an operation that's on seven elements, or five elements, like you're off by one, I guess.


Anna Rose [50:09] Yeah. Very literal.


David Wong [50:10] Like the title says. Very literal. And so, the AI was very good at finding these bugs. And these bugs happen in all sorts of languages. So it doesn't matter if they're in Noir or in Circom. It's going to be good at finding them.


And if it's protocol and it's mis-implemented, and by reading the logic, you can see it's very possible that an agent would be good at finding it, that it's Noir or not Noir. The problem that Noir has is that I would imagine that most agents or most models don't recognize Noir because it's too recent.


So actually, there was some work with Move, the Move language, and they actually had to train some -- well, I don't know if they had to, but they trained some smaller models, the coding models from Google on Move, so that they could find bugs. And even then, they found that using the state-of-the-art models was actually better.


Anna Rose [51:05] Okay.


David Wong [51:05] And so, I would think that for Noir, you're going to have some issues because you're going to have to bring some stuff in context in your prompt just to sort of teach it a bit more about Noir.


In the same way, when we looked at Circom, we realized that -- so we have this evaluation framework that's very important, and basically, we ask the same questions to a bunch of AIs. Most of the small models don't even know what Circom is. Sometimes it will say, yes, I know, and it will produce some Circom code that doesn't look like Circom at all.


Anna Rose [51:32] Weird.


David Wong [51:33] But even the bigger model, when you ask them tricky questions, but security critical questions, for example, do you know if the assert function create constraints? So there is an assert function in Circom, and asserts in many, many languages allows you to say assert that this property is true. That this thing is one, or.


But if you do that in Circom, actually it doesn't create a constraint. And so if you rely on that function in Circom, it's insecure. And we've actually found bugs like that. But the AI wouldn't have found these bugs or like the o3 mini and o1, and like the different models we were trying would not have found that because they were not good at understanding that.


So we asked a question and -- what's weird is that sometimes they would say the correct thing, and sometimes they would not say the correct thing. So what we do know is that we always provide guidance. We always tell the AI, hey, assert function is not secure, for example, 


Anna Rose [52:29] Yeah. Always look for that. Do you think the AIs of today would have found, for example, the Zcash bug, the one that they had to sort of keep quiet back in -- this is -- we did an episode long ago with Sean Bowe talking about this story. So we'll link to it if anyone's curious.


This was a bug in a live system that was discovered by the researchers that were there, but had it been exploited, it would have actually meant a major financial kind of hit. I think they could privately --


David Wong [53:02] Disaster.


Anna Rose [53:03] Disaster. Exactly. They could privately print money that would never have been traceable. It was kind of the worst case scenario of a bug, especially in a ZK private system. So, I'm just wondering, and maybe you don't know the answer to this, but I would be curious if today's tools, the way you have them, would it have found it?


David Wong [53:22] So for this kind of bug, I'm a bit skeptical. So AI is very good at doing stuff that humans are good at doing. Whatever humans are good at doing, AI is good at doing it. But that's sort of the intuition that I have right now. And so the Zcash bug was a paper bug. A paper protocol bug. So you had to go on the paper, you had to follow it, and you had to realize that something was being published and it shouldn't have been published.


Anna Rose [53:49] And normal humans wouldn't find it, but Ariel Gabizon found it.


David Wong [53:56] Exactly. He's not a normal human being.


Anna Rose [53:57] Shout out.


David Wong [54:23] Good job. It's not easy. You have to understand a lot of things about the protocol, you have to understand -- it's not easy to find these kind of bugs. Or at least if Ariel would write more content that we could train on, then maybe we could find these kind of bugs more easily.


Anna Rose [54:18] Maybe he should train himself on his private messages or something. Interesting.


David Wong [54:23] The other problem with training or fine-tuning is that you need a lot of data. It's not enough -- because you need to tweak these weights, and to really change them, you need to throw a lot of data at it.


Anna Rose [54:35] So, David, I want to spend -- I don't think we have too much time left in this episode, but I did want to just spend a bit of time with you kind of talking about more generally, the ZK AI crossover space. Partly because, I mean, you're auditing projects, you're seeing a lot of projects in ZK. You were saying like a lot of the ZKVMs, stuff like that.


But there was -- I mean, in 2023 there was sort of like zkML as a topic popped up. There's a few teams that came out of it. But I think we have seen other ways in which ZK and AI are overlapping. So let's talk real quick about zkML. First, have you audited any zkML projects? Are you seeing a lot of those, or would you say there's less today than there was?


David Wong [55:16] So I don't want to misrepresent the field because there might have been some development lately that I'm not aware of. I remember there was some year -- a few years back where zkML was talked about. I remember some people from PSE talking about it. There was Modulus Labs, there was EZKL or Zkonduit.


I'm not too sure what's the status there. I'm not too sure if there's new competitors that entered the playing field. What I remember was that there was this interesting, I think Cathie from PSE at the time, gave a talk on that and I thought it was a good way of abstracting things. But she sort of presented zkML as this use case scenario where you have different use cases.


In the natural use case, without ZK, everything's public. You're using a -- potentially, you're using a public model or a fixed model with a public input, and getting a public output.


And when you use ZK, what you can do is that you can mask the model. You can use a private model, but at least you know that it's the correct model. Or you can mask the input. So you're using a public model, but you don't know what was asked and you get an answer. Or you can mask part of the input, like you know the question, you just don’t know the amount, for example.


Or you can mask the outputs. You could be, here's what we asked and here's the model, but we're not going to give you the output. I don't know why that would be useful, but you can do combination of these kind of things. And so, that's what's interesting with zkML.


Anna Rose [56:49] I think it's interesting, because like I think that back then, there was also a lot of ZKVM teams that were at least in the groups talking about zkML, because they were like, well, if you do ML in this language, then maybe you could do it in this VM.


And I feel like there have been teams that have kind of gone with that. So, for example, Lagrange has done some sort of zkML stuff. I haven't explored that. And also, I mean, I think there's teams like Ritual AI, that is AI, but it's using ZK on some level as well. I would probably put it in that category.


David Wong [57:21] All right. That's the moment where Ismael's like, oh, I should have told David about what we're doing so he could talk about it during the podcast. I have no idea what they're doing.


Anna Rose [57:28] Yeah. I mean, we just saw announcements kind of like running models, and I was like, oh, okay. So they're playing in that world. Since then, we also, on the show and just generally, we heard about a lot of examples of kind of the opposite of zkML, where you're using ZK within ML, and more like you're using ZK to counter AI or counter ML.


Almost like as a -- to distinguish oneself from an AI generated image, for example, we'd have image provenance of I actually took this picture. It's real, it's mine. Or show using ZK to show provenance that it is AI. There's these kind of ways that we started to see ZK being used not within a model, but kind of to prove something about a model. So that was maybe another angle.


David Wong [58:17] I guess there's -- identity also is like, you need to prove that you're not an AI bot or something like that. There's a real identity somewhere.


Anna Rose [58:24] Totally.


David Wong [58:25] So you -- like ZK Passport would be an example of defending against AI in a way.


Anna Rose [58:31] Yeah. And I mean, most of the ID solutions right now, at least that we're hearing about, they're ZK at the core. And it is kind of like it's more of a larger project to be able to identify a human. And it's not really AI, it's not in it, but it's kind of because of it or something.


David Wong [58:51] Yeah. AI is threatening our recognition of who is a human. So I would say, so ZK is used in two ways there. Maybe we can think about it this way. For identity and proving you're not an AI, we don't need ZK. We just need a signature on your passport or something like that. But ZK will add privacy to it.


Anna Rose [59:12] Which is important for things like ID.


David Wong [59:14] I mean, depending on the protocol. Maybe in some use cases, it's not important, but maybe onchain, you usually want privacy if you're going to play with passports.


And then, there is the provenance thing. If you take a picture with your phone or something like that, and you want to show that it has been modified, and you only trimmed it, and cropped it, and rotated it, or changed the -- you want to prove the changes that you made from an original picture.


So again there, you're going to have a signature and your phone has some hardware chip that will sign it and say, I attest to it. This is correct. And then ZK is only going to -- that's the part where it's not really ZK, but it's only going to be useful to compress, create a proof that all the transformations were legit and not crazy.


And so, it's not really about privacy. It's more about compression of the computation.


Anna Rose [01:00:07] Are there any other ways that you see ZK and AI crossing over?


David Wong [01:00:11] Yeah. I mean, at this point, I think the only thing that I can think about is how in research, people are nerd snipes in different directions, either from ZK to AI or from AI to ZK. I don't know which direction is the most powerful one now, but there's been a lot of movements in between the two different fields.


Anna Rose [01:00:29] For sure. I mean, back in 2022, '23, there was a lot of younger sort of ML students. Students who were kind of on a path to work in that, that came over to ZK.


I mean, one example would be Succinct. I think they said it on the show, back in 2023, that they had been kind of on that path and then switched over. And they didn't switch to crypto, actually. They switched to ZK, which I thought was interesting.


But I also think in 2023, '24, like late '23, '24, you did start to see people kind of get fed up with blockchain, I feel, and sort of move. And they might have been blockchain/ZK, and they sort of moved more in the AI direction.


David Wong [01:01:09] Yeah. That's what Vitalik is saying. People are getting jaded, and then they get replaced by people who are more fresh and new to the field.


Anna Rose [01:01:18] Well, I mean, if you need a change, might as well keep working on something.


David Wong [01:01:22] I mean, that's why I got nerd sniped into this Snark Sentinel, like this AI finding ZK bugs projects. I wanted to play with AI, but I wanted to remain in ZK and maybe get energized by a ZKxAI project. And it's been very interesting.


Anna Rose [01:01:39] One thing about the zkML crossover is I think sometimes because ZK of the privacy and advanced cryptography fields has been the most developed in a lot of ways. I think that's why it gets picked as the acronym ZK.


But in fact, I mean, the closer you look at some of these systems, the more it's like, actually, and people won't always like this, a TEE, or eventually maybe an FHE environment would be a better -- that would be a better tool conceptually for some of these use cases. Especially, if computation is supposed to happen somewhere private.


David Wong [01:02:14] Yeah. I mean, I was talking about that actually yesterday. For a lot of use cases, a TEE is perfectly fine. And companies don't care if they're going to use MPC or FHE or ZK. I mean, that's why TEEs are so interesting, even though they've been broken again and again in different contexts.


Anna Rose [01:02:31] They're so useful.


David Wong [01:02:32] They're still very -- yeah, they're still very useful, still very interesting in many different kind of ways. If you care about a different type of threat model, if you care about efficiency, if you care about easiness of deployment, and all these kind of things. And I mean, we're seeing a lot of TEE applications right now. So it shows that people are very interested.


Anna Rose [01:02:52] Cool. Any last thoughts on auditing ZKxAI?


David Wong [01:02:57] I guess, maybe one last thought. I've noticed that coming up more and more in these AI finding bugs papers that I've never seen before. And they all have this in common. A lot of them have that. They have an ethics section at the end of the paper. I thought that was interesting to mention because I remember reading the same kind of thing with physics, the physics field right after the bomb, the atomic bomb.


Anna Rose [01:03:23] Oh, they started to talk about the humanity side of things because you could get carried away without it or something.


David Wong [01:03:31] So after the atomic bomb, physicists came together and they sort of created this movement of science is more than just science. There's also a human responsibility part in it. And so, they sort of created this code that still exists today in physics. You're not going to research and advance science if it's for killing or destroying and these kind of things.


So the ethics in science, in physics especially, is really, really big. I would guess in biology as well. And in privacy, we always had that, for sure. But in AI and bug finding, it's interesting to see that as well.


And actually, I would say, these days when you go to ZK conferences, and you attend talks and these kind of things, you see a lot of talks where people don't really care about the ZK part. I remember seeing one of the talk recently and someone at the end of the talk asked, what about ZK? Is it ZK? And the speaker said, oh, well, it doesn't really matter. There's a lot of use cases where you don't need ZK.


Anna Rose [01:04:34] You mean ZK in the privacy front.


David Wong [01:04:35] ZK in the privacy --


Anna Rose [01:04:38] Context. Okay.


David Wong [01:04:38] And so, for a field that sort of was born from a cypherpunk place, I think less and less people care about privacy, weirdly. I don't know if I'm going to trigger a bunch of people by saying that.


Anna Rose [01:04:51] Yeah, maybe. I feel like recently there's been a bit of a resurgence of interest in privacy. I think it cooled back in 2022. A lot of teams got a little scared of it, but I think they're more into it now. That's my thoughts.


David Wong [01:05:04] But not even talking about products. It'd be interesting to see if you're in the -- we should do a poll. If you're in the ZK field, do you care more about the privacy aspect or the compression aspect?


Anna Rose [01:05:16] Yeah. Efficiency.


David Wong [01:05:16] And I'd be really curious --


Anna Rose [01:05:17] Yeah. Succinctness, basically. Yeah.


David Wong [01:05:19] Right. I'd be curious to see the results.


Anna Rose [01:05:22] That's cool. We should do it sometime.


David Wong [01:05:24] But yeah. So just to go back to the AI stuff, the ethics is -- I mean, when I was working this stuff, and getting good results. Right? As I was running it and getting better and better results, I was like, wow, okay, there's something there. Now I understand why there's an ethics section in all these papers.


And indeed, it's becoming easier and easier to find bugs. It's becoming more and more accessible. And you can imagine that at some point in the future, there will be an AI that will -- you'll just throw it at something, like look at this TCP/IP implementation, and then you'll be able to break the Internet just by -- so it feels very scary.


Anna Rose [01:06:02] Wow. I mean, the way I almost envision a future here is also like, you have security that becomes so secure a human could never break it. But then you have AI-generated hacks that are so sophisticated and so crazy, it's almost like they start playing a game at a level which is so above us. And the minute that type of thing happens, it starts to feel very much like we are the pets in the cages, and they are the masters.


David Wong [01:06:31] We're looking at stuff that -- I was reading this article where AI creates chips, and we don't even understand how the AI is creating these things. It's more performant, and we don't really -- we're almost at this stage where AI is creating new things that are useful for us, and we don't understand how these things work, or like writes code and -- but yeah.


I can also see a future where maybe OpenAI doesn't give you access to security capabilities. The model is throttled, and so, will not answer these kind of questions. And if you want to be able to access these features, then you'll have to be a vetted security company or government company or these kind of things.


I know they're already doing these kind of things with -- so they're already assessing the capabilities of AI to help you create nuclear facilities and bombs and things like that. And I don't know if you should say these words. We're going to get a --


Anna Rose [01:07:23] Well, it's a podcast.


David Wong [01:07:28] On some file somewhere. But basically, only some government agencies have access to these reports, or can play with these things just to assess the -- it's also something I learned recently reading the Anthropic system --


Anna Rose [01:07:40] The thing is, the decentralized Maxis will not like this idea because, yes, then it's protected in the government, but what if the government's run by a group that you don't think is ethical, or not your team or whatever? Could also be very terrifying.


David Wong [01:07:55] Yeah. Or it could be the North Korean government or --


Anna Rose [01:07:59] Crazy. All right. Well, David, thank you so much for this conversation. I really liked getting a chance to look at your workflow and the direct impact of AI on auditing in the ZK space. I thought that was so cool to actually kind of paint that picture. I mean, I'm sure engineers just building this stuff are also using AI tools in their own way. But this was a very cool look into this evolving field. Yeah. It's very cool. Thank you so much.


David Wong [01:08:28] Yeah. It was fun to talk about it. And as fun as it was to --


Anna Rose [01:08:32] To build them?


David Wong [01:08:33] Or it is to work on these things.


Anna Rose [01:08:34] Nice. Cool.


David Wong [01:08:35] Yeah. Hopefully, I can come back and do an episode on like, "no, we don't audit things anymore."


Anna Rose [01:08:40] Exactly. I'm now doing another thing completely. That's cool.


David Wong [01:08:44] I have a garden now. 


Anna Rose [01:08:48] Cool. All right. I want to say thank you to the podcast team, Rachel, Henrik, Tanya, and Kai. And to our listeners, thanks for listening.