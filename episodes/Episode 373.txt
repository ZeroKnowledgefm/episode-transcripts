
[Anna Rose] (0:05 - 0:21)
Welcome to Zero Knowledge, I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralised web, as well as new paradigms that promise to change the way we interact and transact online.


[Anna Rose] (0:28 - 2:07)
This week, Nico and I catch up with Justin Drake from the EF. We dive into Ethproofs, which he defines as a meme, a platform, a benchmarking effort, and an emerging community. Specifically, we discuss the goals that motivated its creation, the types of collaboration being forged through this effort, the zkVM benchmarking targets and how these were chosen, as well as some of the future benchmarks that could extend this effort.


We also cover why the EF chose to develop Ethproofs, and what the Ethproofs project actually brings to the Ethereum network. For me, this was a bit of a dive back into the heart of the Ethereum L1. We got to explore how the roadmap, especially the roadmap involving ZK, has evolved, was changed at the Ethereum Foundation, and more.


The conversation touches on one of the most dynamic and well-funded parts of the ZK space, made up of the zkVM, Prover Network, and general ZK infra projects. There have been many teams moving in that direction. And the mindshare being focused on this effort at the moment is immense. So it was really great to have a look at what's going on in this part of the ecosystem.


Now before we kick off, I just want to point you towards the ZK jobs board. There you can find job postings from top teams working in ZK. And if you're a team looking to hire, you can also post your job there today. We've heard great things from teams who found their perfect hire through this platform, and we hope it can help you as well. Find out more at jobsboard.zeroknowledge.fm. You can find a link to this from our website and in the show notes.


Now here is our interview about Ethproofs with Justin Drake.


So today, Nico and I are here with Justin Drake from the EF. Welcome back to the show, Justin.


[Justin Drake] (2:07 - 2:08)
Thanks for having me, Anna and Nico.


[Anna Rose] (2:09 - 2:10)
Hey, Nico. How you doing?


[Nico Mohnblatt] (2:10 - 2:12)
Very good. Thanks. Nice to be here.


[Anna Rose] (2:12 - 3:27)
Cool. So, Justin, this is not the first time you're on the show. I'm going to do a quick throwback to some of the episodes that we've already recorded over the years.


And it goes back, I think, to 2019. We've covered very different topics pretty much every time you come on the show, which is really cool. So all the way back, the first episode I think we did together was about randomness and random beacons.


This is during the ETH2 era, where a lot of research that you were working on was focused on that. We then recorded an episode on the research efforts at the EF. We recorded this like just before COVID hit in person in right around SBC 2020.


And last time you were on, we talked about something called ultrasound money and VDFs. And actually at ZK Summit, you also spoke about ZK and hardware. Yeah, I just feel like whenever I have you on, we end up covering something very new.


Today, we're going to be talking about Ethproofs, which I do feel like is kind of a departure from some of the previous stuff, but maybe you'll be able to help us link these things together. Before we jump into Ethproofs, let's throw back to that last episode on ultrasound money. You talked about this meme, and the question here is, did it come true?


[Justin Drake] (3:28 - 5:01)
Yeah, great question. So the meme did play out for a couple of years. Basically, what happened is that the supply was decreasing because there was more burn than there was issuance.


And I guess a couple of things happened. The first one is that the amount of stakers grew quite radically, and therefore the amount of issuance also grew. And also, we enabled blobs at L1, which is basically a scaling increase, which has reduced the total amount of transaction fees.


Now, the reason why the transaction fees have gone down is because right now there's more supply than there is demand. And so we need to wait a little bit more, basically, for demand to catch up so that we can have price discovery. So in some sense, you could say we're in growth mode, where we're favouring growth over value capture.


The other thing that I'll say is that there are ideas to change the issuance curve to be much more sustainable for the long-term future. So right now, the way that the issuance curve is structured is that it incentivises every rational ETH holder to become a staker. And there are negative externalities to having the vast majority of all ETH being staked.


And so the proposal, or at least one of the proposals, is to cap the amount of staked ETH to 50 percent. And the way we would do so is taper the issuance down to zero as we approach this 50 percent cap.


[Anna Rose] (5:01 - 5:02)
OK.


[Justin Drake] (5:02 - 5:14)
And when these two things happen, we dramatically reduce issuance, we dramatically increase the burn because demand has caught up with supply, and I believe that we will reenter this ultrasound mode.


[Anna Rose] (5:14 - 5:19)
OK. So we're not in it right now. We're not in the ultrasound money mode at the moment.


[Justin Drake] (5:19 - 5:23)
Well, it depends what your definition of ultrasound is.


[Anna Rose] (5:23 - 5:29)
We haven't actually defined it. So anyone listening who didn't hear that previous episode may be quite confused.


[Justin Drake] (5:30 - 5:56)
So the strict definition is basically one where the total supply of tokens is going down over time. But really, the ultrasound meme came as this tongue-in-cheek joke almost, comparing it to Bitcoin. And as of today, still, the total growth of the ETH supply is slower than that of the Bitcoin supply.


[Anna Rose] (5:56 - 5:56)
Oh, wow.


[Justin Drake] (5:56 - 6:00)
So it's still kind of ultrasound-er or sounder than that of Bitcoin.


[Anna Rose] (6:00 - 6:20)
Ultrasound-er. Nice. Cool.


OK. So going back, like, I'm trying to map from that episode and the work you were doing at the time. What led you to work on something like Ethproofs and the work you're doing today?


So maybe share with us a bit of the journey in your research since we last spoke.


[Justin Drake] (6:21 - 7:46)
Sure. So I don't know if you remember, but Vitalik used to post these big roadmaps with, you know, the merge, the surge, the plurge. He stopped updating those.


But one of the items in those roadmaps was Snarkify everything. Really, Snarkification is this really powerful tool that can be used to enhance every single layer of Ethereum, starting from the consensus layer, the CL, through the DL, the data layer, and finally the EL, the execution layer. Now, for a very long time, the Snarkify everything was a little bit of a pie in the sky because specifically what we needed was this concept of real-time proving, where the latency of proofs was shorter than that of an Ethereum slot.


And I've been very optimistic on the advent of real-time proving, but for a long time, no one kind of believed me. But about a year ago or so, we started seeing signs that real-time proving was indeed possible. And so we went ahead and decided to build this website, Ethproofs, that would document the journey towards real-time proving and hopefully accelerate it in the process.


Ethproofs is the L2BEAT of zkVMs, if you will.


[Anna Rose] (7:46 - 8:32)
I was going to ask if it was similar to that because, you know, L2BEATs, I mean, this was done as an independent effort, but it was taking this idea that the zkVMs, the rollups, they didn't all have the same, like they were sort of trying to benchmark each other on various things, but there was no standardisation. All of a sudden it gave a roadmap and a structure for these ZK rollups to sort of work within. Ethproofs, I feel like, has a very similar thing, but its focus seems to be for zkVMs, if I'm correct.


And I am curious, like how close are we to real-time proving today? Like you said you started this about a year ago, you felt like it was coming. I'm guessing this is happening in the zkVM realm.


Yeah. How close are we?


[Justin Drake] (8:33 - 10:15)
So it depends on your definition of real-time proving. There's a lot of little caveats here and there, but broadly speaking, what we want is to be able to prove the vast majority of mainnet Ethereum blocks, so these are EVM blocks, within 12 seconds. And by vast majority, what we mean now, we've codified it, is 99% of blocks.


And we want to do so within a little bit less than a slot, so actually 10 seconds instead of the whole 12 seconds. And we also have a requirement on the prover hardware, and that's for decentralisation reasons. So if you were to throw a massive cluster of GPUs, for example, 150 GPUs, you can pull it off.


And actually, that's something that the Succinct team has shown with SP1 Hypercube. But we want to have this requirement of less than 10 kilowatts total power envelope. And the reason this 10 kilowatt number comes up is that it's the amount of power that a Tesla home charger will draw.


And so there's millions of people that have these home chargers that draw 10 kilowatts. And so it's plausible that there's going to be potentially hundreds of enthusiasts all around the world in their garage, in their offices, who will be running these clusters of GPUs. And the number that we have in mind roughly is 16 GPUs.


These are high-end gaming GPUs, think a 5090 consumes roughly 500 watts. And so if you have 16 of them, that's 8 kilowatts roughly. And then you also have some overhead for the cooling and things like that.


[Anna Rose] (10:15 - 10:18)
And the requirement is 10 kilowatts.


[Justin Drake] (10:18 - 10:19)
Yes, exactly.


[Anna Rose] (10:19 - 10:32)
Interesting.


In that case, you are focused on the GPU side of things. There have been teams that did a lot of their benchmarking on CPUs. Are Ethproofs more focused on this sort of GPU approach?


[Justin Drake] (10:33 - 11:39)
No. So we're totally agnostic to technology. But one of the things that we're starting to see is patterns.


So I've started this zkVM tracker, which now has 32 different zkVMs, it's this spreadsheet. And one by one, these zkVMs are integrating with Ethproofs. And what we're seeing by looking at the data is that there's these big, massive trends at play.


So one of them, for example, is that hash-based proving is winning over all of the other approaches. Another trend is that GPU proving is also winning. And that was maybe not clear, you know, two or three years ago, one could have said maybe a very large cluster of CPUs, especially if they're multicore, could win out.


It also could have been possible that FPGAs win out. But we've seen through the data, including teams pivoting away from alternatives and joining the fold. And then, you know, there's all sorts of other patterns that come up.


So, for example, RISC-V is winning out as an ISA for the zkVMs.


[Anna Rose] (11:39 - 12:08)
But it's like it's not competing against the zkEVM projects, is it?


Like zkEVMs to me are like L2s. They have a purpose. They are there because you can like port, you know, existing Solidity contracts there.


But yeah, when you're talking about sort of some of these trends and like Rust and RISC-V, would you say that this is sort of a new space entirely? Do you actually see that eventually eating the zkEVM space?


[Justin Drake] (12:08 - 13:08)
So technically, what we want for Ethproofs is zkEVMs. But traditionally, if you rewind the clock five, six years ago, the way that people would build these zkEVMs is monolithically, right? They would literally try and build a circuit for every EVM opcode. And that was a huge amount of effort.


And maybe here, the best example is Linear. So they started six years ago and only just a few weeks ago or months ago, they completed every single opcode and now they can prove all Ethereum blocks. But Linear is the only team that is still alive that's doing that.


All the other teams took a different approach, which is to build a zkEVM for a much lower level ISA, something which is much, much, much, much simpler than the EVM, and then basically take high level EVM implementations and compile them to the lower level ISA.


[Anna Rose] (13:08 - 13:30)
Yeah, interesting. So but your goal, and this gets to a question, I mean, I think we should define Ethproofs, what it is and all of that. But you're saying that Ethproofs goal still is actually the zkEVM side of things.


It's not like ushering in an era of the EVM being like discarded. You still want the EVM at its center.


[Justin Drake] (13:31 - 14:34)
Absolutely. We need to be backwards compatible every time we make an upgrade. The EVM is going to stay part of Ethereum for the rest of eternity.


And really what we're trying to do by snarkifying the EVM is to scale the L1. So traditionally, the way that you would scale the L1 is by increasing the gas limit, but then all of the validators, the attesters, need to keep up with all of that throughput. And unfortunately, the amount of work that they have to do to replay the transactions, to re-execute them, is linear in the amount of throughput.


And so the best that we could hope for is maybe a 10x increase, let's say, in the gas limit by really pushing the optimisations in those execution layer clients. But the alternative approach is that we have this new set of entities called provers that will prove in realtime the Ethereum chain so that the validators only have to verify those proofs. They don't even have to download the blocks and that they can still attest to the validity of those blocks.


[Nico Mohnblatt] (14:35 - 14:44)
So is there already a plan of how to integrate SNARKs into consensus? Or at this point, are we just saying, let's try to prove blocks and we'll see where we use it later?


[Justin Drake] (14:45 - 16:49)
Yes, so there is a plan in multiple phases. The very first phase is just allow anyone to just prove blocks. And this is something that can be done permissionlessly.


And in some sense, this is what Ethproofs is all about, just having people opt in to proving Ethereum blocks. This phase after that is what I call mandatory proofs and delayed execution. So right now, in order to keep up with the chain, it's kind of hard because you have to generate the proof for the attesters within a few seconds of the generation of the block.


You don't have a full slot. And the idea of delayed proving or delayed execution is to give a full slot or almost a full slot for the prover. So think of the block being committed to at the beginning of the slot and whoever produced the block having the ability to provide the proof towards the end of the slot.


Now, once we have this delayed proving, we can also make the proofs mandatory, meaning that the block is only valid if whoever produced it also produced the proof. So that's phase one. And then the endgame, which I guess you could call phase two, is enshrining the zkVM.


So in the first two phases, basically, we would have a plurality of zkVMs that would snarkify the EVM. And here, it's very similar to the situation with client diversity. So with consensus layer clients and execution layer clients, we have five or six different clients.


And if there's a bug in one, two or even three of them, Ethereum survives. And the assumption that we have is that every single client is buggy. And it's the same thing with the zkVMs. We assume that each individual one is buggy, but as a group, they're actually quite secure. In order to reach the endgame of enshrining a single zkVM, we need to have end-to-end formal verification and very high confidence in the one that we pick.


[Anna Rose] (16:50 - 17:11)
Wow. I didn't realise that there was sort of a competition towards this endgame. Or do you think, is the EF kind of looking at developing their own zkVM, like a bespoke one for this particular case, based on what you're seeing?


Or do you think you will choose one of the open source kind of client, quote unquote, zkVM projects?


[Justin Drake] (17:11 - 19:38)
So on the competition point of things, I actually like to zoom out and look at the big picture. In the very, very early days, it was all about feasibility and viability of snarking, right? We had theoretical breakthroughs and practical breakthroughs.


And I remember, maybe it was like three years ago or so, where PSE for the very first time managed to prove an entire Ethereum block. And it took two days to prove this one Ethereum block. And then there's kind of this second phase where people are optimising for throughput and cost.


And then now we're kind of entering this phase where we're optimising for latency. And what I think will happen in the future is that people will be optimising for simplicity and security. And so, yeah, we're still quite far away.


And really, there is no competition at this point. We're all working together to get the diversity of zkVMs that we need. But to answer your question, the most likely candidate I would say right now for enshrinement would be something along the lines of JOLT.


So JOLT stands for Just One Lookup Table. And the whole point is that it's extremely simple. It's just this one lookup table in some sense.


And it's fairly easy to audit. The code base is fairly minimal. Now, unfortunately, JOLT isn't perfect.


You know, it's not post-quantum secure, for example. And there's all sorts of things that we can improve. But directionally, this is the way that we're thinking.


And one kind of side note is that DeFim Foundation is indeed developing its own zkVM, but not to snarkify the EVM. It's to aggregate post-quantum signatures for the consensus layer. So right now we have BLS signatures, where the B stands for Dan Bonnet.


And basically, you know, we have a million signatures that are generated every single epoch, and those need to be aggregated. And we need the equivalent that is post-quantum. And the approach that we've taken is to do all of this with just hashes.


So we have hash-based one-time signatures. We make those multi-time signatures with something called XMSS. The details don't really matter.


And then these signatures are aggregated using a post-quantum snark, specifically one that is also hash-based.


[Anna Rose] (19:40 - 20:32)
I want us to take a step out of what we're talking about here and just define the Ethproofs project, because I hear it being used sort of in different ways. For me, it came to me as like almost a community, a website, a benchmarking tool, a place where you could start to compare these different zkVMs on specific metrics in a much more kind of like even-handed way than it had been before, where it was often coming from individual teams benchmarking on their own hardware in their own kind of perfect setups. But it sounds like it's bigger than that.


So can you just tell us what exactly Ethproofs is? Is it an initiative? Is it a website?


Is it a group? What is it? Is it the actual word Ethproofs, like proofs of ETH?


Like, is it the actual ZKPs? Yeah. What is it?


[Justin Drake] (20:33 - 21:17)
Yeah. So you could say that Ethproofs is a meme. And, you know, it comes from Ethereum proof.


So it's a portmanteau. And actually before we named it Ethproofs, the website that we were considering was proofs.ethereum.org. So it was like literally proofs of Ethereum blocks.


Now, what we're trying to do here is to accelerate the bottleneck that we have towards dramatically scaling the L1. And here there's another meme, which is gigagas L1. What does that mean?


It means one gigagas per second of throughput on the L1. And the only way that we can get there is by having this diversity of real-time zkVMs.


[Anna Rose] (21:17 - 21:17)
 What is gigagas? What is that?


[Justin Drake] (21:18 - 21:39)
So the unit of computation in Ethereum is denominated in gas. A transfer, for example, is 21,000 gas. Today we're processing about two megagas per second.


And the goal is to basically increase that by roughly a factor of 500 to get to one gigagas per second.


[Anna Rose] (21:40 - 22:03)
That's the speed that you want to get to. So you've already improved it from what it was in the past. I mean, I did live through the era of the insane gas prices and slow speeds.


But, yeah, what would that even look like? And I guess I'm having trouble understanding how ZK and zkVMs and proofs in general help with the gigagas or with the gas prices.


[Justin Drake] (22:04 - 23:02)
Yeah. So let's move away from gas because that's a bit of an abstract concept. And let's go back to TPS, transactions per second.


So the goal is to have 10,000 TPS. So an average transaction will be roughly 100,000 gas. And so one gigagas per second divided by 100,000 gas is 10,000 TPS.


And 10,000 TPS is a lot of transactions. It's more than basically any existing L1 today by several times. And you're right that when Ethereum started, it was doing less than 10 transactions per second, and now we've grown over time and a lot of the throughput is through the L2s and the ultimate vision that I'm hoping we can enable with ZK is 10,000 TPS on L1 and 10 million TPS for all of the L2s combined.


And the mean there is one teragas. It's a thousand gigagas per second.


[Anna Rose] (23:03 - 23:29)
Crazy. Okay. This would allow for, I'm guessing, just like more complicated programmes, more happening speed.


Like what else does this enable if you have such a setup with the 10,000 TPS? Like what does it actually change? Because things already feel like for what we're using this for, at least what I'm using for, it feels a lot faster than it used to be.


So like, what do you want to do with it that you would need so much more?


[Justin Drake] (23:29 - 24:55)
Ethereum is an extremely ambitious project and what we're aiming for here is to be the internet of value. And there's going to be some really high value transactions that people will want to do. And if we can achieve 10,000 TPS, that would be roughly 0.1 high value transaction per day per human. So it's not a lot, but it's kind of enough to get you started. And some of these high value transactions will include minting assets, you know, settling other L2s or, you know, doing high value trades or whatever it is. Now, in my opinion, the L1 is not sufficient, right?


0.1 transactions per day per human is not enough. We need something closer to 1,000x that, which is a hundred transactions per day per human, and we can get that through the L2s. And the good news is that we have a whole scaling roadmap for the blobs that will give us sufficient data availability to secure the 10 million transactions per second.


And because the L2s scale horizontally, you know, you can imagine, for example, 1,000 L2s, each with one gigagas, so that the union of them will be a teragas. Because of this horizontal scaling, there is no sequential computation bottleneck. It's very, very scalable.


[Anna Rose] (24:56 - 25:13)
This hundred transactions per person per day, is that based on the current financial system? Is that sort of what you found is what is required to be able to at least match what exists today, but in the sort of trad-fi world, like credit cards and all of that, I guess? Yeah.


[Justin Drake] (25:13 - 25:56)
If you look at systems like Visa and MasterCard and PayPal, they'll do tens of thousands of transactions per second. And if you look at exchanges, you know, things like NASDAQ or NYSE and all sorts of other exchanges, they might do hundreds of thousands of transactions per second. And actually they're designed for more, for millions of transactions per second.


But what I expect will happen is that once we provide the capacity, there's going to be this notion called induced demand. People are just going to find use cases for these things. And one of my particular theses, which is a little abstract, is that we're going to have AI agents help us make all of these transactions on our behalf.


[Justin Drake] (25:56 - 25:57)
Makes sense.


[Justin Drake] (25:57 - 26:08)
And, you know, ultimately, you know, we visit way more than a hundred websites per day, potentially. We perform a lot of actions.


And so why not also do so in the financial world?


[Anna Rose] (26:09 - 26:19)
I was going to actually ask about agents because it sounds like that acceleration of more and more and more, it's coming alongside potential future requirements that these agents bring.


[Justin Drake] (26:19 - 26:29)
I mean, one of my theses is that the richest entities in the world are going to be AIs and AIs are going to store their value in cryptocurrencies like ETH.


[Anna Rose] (26:29 - 26:36)
Very likely, actually. It's a good thesis. OK, going back to Ethproofs, so what is it?


[Justin Drake] (26:38 - 27:09)
OK, so the meme is kind of this, this abstraction, but like concretely, we're trying to accelerate this main bottleneck. And as you said, we have all sorts of different arms to Ethproofs. One of them is the website, which has benchmarks for the average case, meaning the actual load that hits the Ethereum blockchain.


But it's also looking at what we call prover killers, which are these artificially crafted adversarial blocks that will try and make it as hard as possible for the provers to prove those blocks.


[Anna Rose] (27:10 - 27:10)
Wow.


[Justin Drake] (27:10 - 29:10)
And that is going to help us guide some of the hard forks that we need to do to L1, in particular, some opcode repricing. So there's some opcodes out there like ModX, modular exponentiation, where the gas price is much, much, much, much lower than the cost of doing the proving. Now, we also have some bounties.


So we've announced three $100,000 bounties for whoever can prove every single Ethereum block in realtime for a period of time and publishes the proofs on Ethproofs. We also have ETH proof calls. So we've had three so far.


The first one was introducing the 30 zkVM teams. The second one was all about real-time proving and all of the innovations that got us there. And the third one was titled GigaGas L1.


And the fourth one, which is going to be on August 22nd, anyone is welcome to join, by the way, is going to be dedicated to RISC-V. It's going to ask the question, should we enshrine RISC-V as the native ISA within Ethereum? And one advantage that we would have here is that we would allow developers to use Rust to write their smart contracts and they would also benefit from much better performance because the gas costs, the gas schedule would basically be set roughly one gas for one RISC-V cycle.


And so you kind of have access to bare metal performance, if you will. And then the other final advantage is that Ethereum as a project would be dramatically simpler.


Why? Because RISC-V as an ISA is just a few dozens of lines of Python to describe, whereas the EVM is more of a monster, kind of thousands of lines of code to describe.


[Nico Mohnblatt] (29:11 - 29:26)
Actually, this takes me back to the patterns you were mentioning that are arising, looking at all these zkVMs. Do you think RISC-V is there just because it's kind of one of the first ones for which we had a zkVM? Or is it really that it has like an advantage over all other ISAs?


[Justin Drake] (29:26 - 29:30)
Come to Ethproof's call number four and we'll figure it out. Good advertising.


[Nico Mohnblatt] (29:31 - 29:47)
I have to say, I highly recommend the calls. I've been to, I think, two of the three and they are incredibly eye opening, especially in terms of like what teams are doing. And it's incredible to see all these teams collaborate and work together and share their ideas, which is something that we did not see three, four months ago.


[Justin Drake] (29:48 - 30:20)
Absolutely. Like previously, a lot of the teams were not very impressed by their colleagues. And the reason is that everyone was claiming that they were the fastest zkVM.


And the reason was that everyone was using a different benchmark and that led to lots of frustration and lots of wasted time because every zkVM team had to dig into the benchmarks of their competitors to highlight what was going wrong and where they were cheating. But now that we have this fair playing ground, we can actually have much more objective and healthy collaboration.


[Anna Rose] (30:21 - 30:35)
I want to go back, though, to what you had said about zkEVMs and the EVM being central because what you just said with like the RISC-V ISA potentially being enshrined, doesn't that kind of contradict that? Like, doesn't that actually suggest that the EVM may be replaced?


[Justin Drake] (30:36 - 31:44)
The EVM wouldn't be replaced, but there would be a new ISA that would be available under the EVM and the EVM would now be defined as RISC-V bytecode. So right now, the EVM is defined abstractly as an ISA. So in the yellow paper or in this thing called EELS, which is a specification of the EVM in Python, we kind of abstractly say, OK, it doesn't add up code, it does addition, there's a multiplication up code, it does multiplication.


And then, you know, there's all sorts of clients that go implement this specification. What we would do instead is that we would take one implementation, we would compile it down to RISC-V, and that would be the definition of the EVM, like this just string of RISC-V bytecode. And then now kind of the abstractly defined ISA would be RISC-V that would live under the EVM.


So as a contract developer, you always have the option to hit the EVM. But if you're sophisticated, you can go directly to RISC-V and benefit from more performance.


[Anna Rose] (31:44 - 31:52)
Right now, the EVM lives on what ISA on itself?


[Justin Drake] (31:52 - 31:54)
Yes, it is the ISA.


[Anna Rose] (31:54 - 31:57)
So you'd basically be putting a layer underneath it that it would compile down to.


[Justin Drake] (31:58 - 32:09)
So the new ISA, the enshrined ISA would be RISC-V and the EVM would be essentially a smart contract that lives on top of RISC-V.


[Anna Rose] (32:09 - 32:56)
Whoa, does that open it up to all sorts of other languages then? Like at the base layer? I mean, so far we've heard about zkVMs, just a little backstory.


zkVMs have actually been around for a while and it wasn't always RISC-V, right? Like zkVMs were privacy focused. I mean, Miden, I think, was earlier and they were billing themselves as a zkVM.


So, yeah, like now RISC-V is the thing. I'm curious if RISC-V becomes that underlying ISA, but I don't know if I'm supposed to say ISA or ISA, but I'll go with you. You're starting to build the EVM on top of that.


Then could you put like, I mean, this is probably false, but like some Python on top of that? Could you have Python, like EVM, like on the main chain somehow?


[Justin Drake] (32:57 - 32:58)
Yeah, that's a fantastic question.


[Anna Rose] (32:58 - 33:11)
Sorry, is it fantastic or is it just a very non-CS grad? Like I didn't study computer science. So maybe Python is a dumb example, but you kind of know what I'm saying, right?


There's probably better language examples than that.


[Justin Drake] (33:11 - 33:50)
No, that's a great question. So it turns out that there's this piece of infrastructure called LLVM and it's a compiler tool chain that allows you to take high level programming languages and compile them to a whole suite of backends. And one of the supported backends is RISC-V.


And so if you support RISC-V, you support all of the high level languages that LLVM allows for. Exactly. And those include Rust, C, C++, I believe things like Haskell.


Oh boy, I don't know about Haskell.


[Nico Mohnblatt] (33:51 - 33:53)
Sorry. I think Go is in there.


[Justin Drake] (33:54 - 35:01)
Yeah, I mean, I actually think that Go is a separate thing. Like they have their own compiler, which does support RISC-V, but it doesn't go through LLVM. That's my understanding, but I'm not an expert in this kind of tooling.


But the point is that there's a lot of tooling around compilers and compilers are extremely difficult to build. And so you're right, Anna, you said that there were alternative ISAs that were developed. So there's the Miden ISA, there's the Valida ISA, there's the Cairo ISA and various other ISAs.


And for all of these ISAs, potentially with the exception of Valida, that we don't have the equivalent LLVM integration. And so most of the developers are just cut off. And maybe one example here is Cairo.


Cairo is quite an old ISA, but it doesn't have this LLVM integration. And one of the teams, Kakarot, spent a whole three years trying to build the EVM directly in this low-level programming language, and they gave up after three years. It was just too difficult.


And so there's a lot of value to having this compiler tooling.


[Anna Rose] (35:02 - 35:20)
Interesting. I still, like, Ethproofs, is it an organisation coming out? I'm going to keep coming back to this, I think, because I think it's an interesting project.


Is it a company? Is it like a part of EF? Is there a group dedicated only to Ethproofs?


[Justin Drake] (35:20 - 36:35)
And so let me give you the origin story and some of the history. So a bit more than a year ago, the RISC Zero folks were chatting to us and they had this idea. What if we had an archive somewhere of proofs of every single historical Ethereum block?


And I said, yes, this is a fantastic idea. Let's let's do it. And originally, as I said, the URL was going to be proofs.ethereum.org.


But back in the day, the Ethereum Foundation was very supportive of this abstraction mindset where we try and delegate as much as possible to the community. And so we decided to have a separate GitHub organisation for Ethproofs and have as a plan the idea of spinning it out of the Ethereum Foundation. But what happened in the last few months is that the Ethereum Foundation was reorged internally and now it's kind of much more acceptable, if you will, for Ethproofs to be part of the Ethereum Foundation.


And now we have a dedicated team called the zkEVM team that is helping with the development of the website and is using the website to try and fulfil and accelerate its goals.


[Anna Rose] (36:36 - 37:08)
Wow. Comparing it to the L2B, which you mentioned was an inspiration, that, as far as I understand, had always been independent. It was just like a place where they could benchmark the different L2s based on the framework that Vitalik had actually outlined with these like five stages and all of that.


In this case, it sounds a bit like the metrics and the judgments, that's also still evolving, right? It's not like one. Am I correct there?


Or do you feel like it's already been set and now you're just kind of rolling it out?


[Justin Drake] (37:08 - 38:16)
Yeah, so directionally, there's a huge emphasis on latency. So the website does show both the proving cost and the proving latency, but the most important metric is latency because we're trying to unlock real-time proving. Similarly to L2B, we have this notion of pizzas.


So basically, we have eight pizza slices and they can each have one of three colours, green, yellow or red, and they'll capture different things about zkVMs, including things like the security. So, for example, how many bits of security are you targeting? Are you audited?


Are you formally verified? Things like that. So these criteria have been set, at least they exist today, but it is very much possible that they will evolve a little bit in the future.


Now, if you're willing to think potentially 2026 or 2027, it's possible that Ethproofs will grow quite a bit its remit beyond just scaling the L1. It's possible that it will also include a whole section on privacy and client-side proving.


[Anna Rose] (38:18 - 38:56)
Oh, that's so funny, Justin. I was going to bring that up later, but I don't know if you heard, we had Muthu on the show a couple of weeks ago and we talked a lot about Ethproofs in that episode, but you weren't there. So I'm really glad we're getting a chance to actually speak to someone from the group.


But Muthu did bring up this idea of currently there's no metric for how well it functions on client-side, how much privacy can these even have. And so the focus has been on this latency and scaling and like the very kind of scalability, succinctness kind of side of things. But yeah, this actually had come up.


So that's really cool to hear that you're also thinking about it.


[Justin Drake] (38:56 - 39:16)
Right. And this is something that has been teased publicly, which is that the PSE group is also going to be reorged within the Ethereum Foundation and it's going to focus exclusively on privacy. PSE stands for Privacy and Scaling Exploration, and the acronym is going to be changed a little bit so they only focus on privacy.


[Justin Drake] (39:16 - 39:17)
It's cleaner.


[Justin Drake] (39:17 - 39:18)
Or just give it a different meaning.


[Anna Rose] (39:20 - 39:25)
Nice. Yeah, give E a new meaning, or no, give S a new meaning, I guess.


[Justin Drake] (39:25 - 39:26)
Yes, exactly.


[Nico Mohnblatt] (39:27 - 39:39)
Actually, I'm interested in some of the pizzas and sort of the targets there. I see that verification time also comes up, proof size also come up. Where are these being applied?


Why do we care about verification time and proof size in this case?


[Justin Drake] (39:40 - 40:16)
In the context of scaling the L1, we would have the validators download these proofs and they would download, more likely than not, something like three proofs, and so we want to make sure that within the 12 seconds it's reasonable to be able to download three proofs, and the latest target that we have is about 300 kilobytes, and the reason is that three times 300 kilobytes is under one megabyte, and that's something reasonable to download within 12 seconds.


And it's a similar thing with verification time. We're going to be verifying three proofs and we want to make sure that the total CPU time is negligible.


[Nico Mohnblatt] (40:17 - 40:31)
That makes a lot of sense. And then security, I see that some teams are operating around 80 bits of security, others 100, the gold standard used to be 128. How do we pick a security level and why do we pick it?


[Justin Drake] (40:31 - 42:40)
Most of the teams nowadays will pick a hundred bits of security. The two exceptions that I know are ZisK on the good side of things, they're picking 128 bits, and Airbender, I believe, are only using something like 79 bits or roughly 80 bits. Now, what the Ethereum foundation is trying to push towards is this 128 bits of security, but the downside is that the proof sizes tend to be larger, especially if you're using the hash-based SNARKs that we're recommending.


Because usually the way that these zkVMs are built is that there's an internal hash-based snark, and then there's an optional wrapper, for example, a Groth16 wrapper, so that the on-chain verification is cheap. But we don't have any on-chain verification. We only have off-chain verification of proof, so we don't need the wrapper.


And the wrapper actually adds a bunch of latency, which we don't want. And so we're encouraging the teams to not have this wrapper. But now the total proof size does come into consideration.


Now, another subtlety is, do you use a proximity gaps conjecture or not? So for hash-based SNARKs, you have two options. You can either have provable security in the random oracle model, or you can make use of a conjecture.


And actually, one of the things that I'd like to tease for the very first time on this episode is that the Ethereum foundation is going to put a $1 million bounty to prove the conjecture that the community is currently using. So it's the up-to-capacity proximity gaps conjecture for Reed-Solomon codes, and in practise, almost all of the teams are using the conjecture. And what the conjecture gives us, more or less, is that it allows us to reduce the proof sizes by a factor of two.


And if we can prove the conjecture, fantastic. If the conjecture is found to not be true, then it's possible that all of the teams would basically have to adjust their proof sizes exactly.


[Nico Mohnblatt] (42:42 - 42:52)
Just to clarify, is this the conjecture, sort of the vanilla correlated agreement conjecture, or is this the strengthened version that we see in WHIR and maybe BaseFold?


[Justin Drake] (42:52 - 44:04)
So just to give a little bit of context for the listener, traditionally we've had FRI be like the number one tool that we use in these hash-based SNARKs, and here we only need what's called correlated agreement. And recently there's been kind of these FRI 2.0, if you will, like next generation FRI, which have better properties, one of which is called WHIR, and here it uses a strengthened version of the conjecture called mutual correlated agreement. So there's this additional word mutual.


Now, I think most experts will tell you that if something has correlated agreement, then almost certainly it also has mutual correlated agreement. So what we'll be encouraging is the stronger version, the mutual correlated agreement, and this has upsides for the aggregation of post-quantum signatures where we're really leaning into WHIR because it has very small proof sizes and is very friendly to recursion. So if we want to aggregate signatures and do so so that there's multiple aggregators that can themselves be kind of meta-aggregated, then we're going to make heavy use of recursion and this is where WHIR shines.


[Nico Mohnblatt] (44:05 - 44:15)
Back to sort of metrics and performance, is there also like a pizza for how much energy I'm using or what kind of rig, how much CPUs I'm using, or maybe just how expensive it is to prove?


[Justin Drake] (44:16 - 46:01)
On the website right now, we have two modes of proving. We have a single GPU, a single 4090, and then we have the cluster of GPUs and almost all of the teams that have integrated now, there's eight of them, are using the single GPU. And one of the things that we track is indeed the cost in dollars.


And the way that we calculate this cost is that we look at how expensive it is to rent the machine that the prover is using per hour, for example, on AWS, and then we use the total proving time to calculate the effective cost. And one of the really shocking things to me is that these proofs are extremely cheap, on the order of one cent per Ethereum block. And if you think of a block as having, let's say, roughly a hundred transactions, each individual transaction will only cost a hundredth of a cent to prove.


Now, one question that's often brought up is, what are the incentives for the provers to do the proving? And it turns out that the answer is just the same that we have today. So we are going to require the users to pay for the proving cost, at least one hundredth of a cent per transaction.


And if they're willing to pay that through the tip, then the prover will have the incentive to include that transaction in the block because the proving cost is less than what they receive as a tip, and ultimately that transaction will make it in the chain. And what I'm hoping will happen is that over the next year or so, the proving cost is going to decrease by another factor of ten, and it will just be so, so small that the users won't even realise that it's part of the tip.


[Anna Rose] (46:01 - 46:46)
I want to just clarify something here because we talk about zkVMs, VMs, virtual machines, but then we also talk about this actor called the prover. And we've talked about this in past episodes, but I've always understood the zkVMs or users of the zkVM as the customer and provers as the suppliers of proofs, specifically in the proving marketplace context, which a lot of zkVMs are also providing. So I'm just wondering when we're doing this benchmarking and you talk about prover cost, this is where I'm kind of confused.


Like, are you talking about the prover marketplace, talking about the provers acting in that marketplace connected to a specific zkVM, or you talk about the cost of the request coming from the zkVM?


[Justin Drake] (46:46 - 47:30)
So the entity that would be doing the proving is the block producer, also known as the block builder. So when they build a block, they're also responsible for the proof. Now they have the option to generate the proof in whatever way they wish.


So option one might be that they themselves have a cluster of GPUs and they do it themselves. They could deal with a centralised trusted provider that they interact with, or they could interact with a permissionless and decentralised marketplace, but whatever approach they take, ultimately we're talking about paying for the energy cost and the depreciation cost of the hardware. And that's on the order of one cent per block.


[Anna Rose] (47:30 - 47:52)
But is that because there's these marketplaces that have emerged and there's more provers, or is it because that's how much it costs for that? Like, I actually don't even know what the actor is on the zkVM side, but what is it? It's the zkVM itself?


It's the sequencer? It's the, I mean, you called it the block producer. Yeah.


What is it actually when you talk about these, like this connection point?


[Justin Drake] (47:53 - 48:20)
So the zkVM is just pure software, and then there's two entities on both sides of the zkVM. On the one hand, you have the prover that is going to operate hardware and kind of crunch through a lot of numbers to generate a proof, and then on the other hand, you have the verifier who will consume the proof, and that's going to be the attester. So on the left side, it's the block builder, they're the prover, on the right side, attester, verifier.


[Anna Rose] (48:20 - 48:27)
But what is the zkVM, like, you're talking about a cost, and I'm trying to figure out who defines the cost. Is it the verifier then?


[Justin Drake] (48:27 - 48:48)
No, it's the prover. So the prover will have to rent hardware, for example, in AWS, and it will cost them one cent worth of AWS resources to generate this one proof for the one block. On the verifier side of things, there's almost no cost, right?


They have to download the proof, which is less than 300 kilobytes, and it takes, let's say, 10 milliseconds to verify, and it's essentially free.


[Anna Rose] (48:49 - 49:03)
But then if you're talking about, like, these cost comparisons, like, are you comparing then different provers? And if you're comparing provers, are you comparing, like, systems of proving, or are you actually comparing zkVM cost? This is where I'm confused.


[Justin Drake] (49:04 - 49:48)
Yeah, great question. So, usually a zkVM will come with a reference prover, which is kind of this reference piece of software with proving algorithms. And you're right that the proving algorithm can be swapped out, and some teams do do that, and the hardware itself can be swapped out.


So you could use AWS, but you could use Vast.ai, or you could use all sorts of other combinations. And what's starting to happen is that people are trying to use the best combination in order to get the most efficient proofs, and you're right that a marketplace is a natural economic mechanism to kind of bubble up these most efficient provers who will end up winning most of the jobs.


[Nico Mohnblatt] (49:48 - 50:03)
By the way, this is something we also see on the Ethproofs website, right? You have zkVMs that are listed, but you also have prover infrastructure that's listed, and you have some provers running different VMs and maybe using VMs from, like, a competitor. Yeah, it's interesting.


[Justin Drake] (50:04 - 50:25)
Exactly. And you can see on Ethproofs which CPU they're using, how many cores it has, how much RAM they have. We're trying to put as much information as possible, and we even want to be in a position where anyone can reproduce the numbers themselves.


So ideally, we'd have a Docker image, kind of a one-click redeploy on AWS, for example, and you can reproduce.


[Nico Mohnblatt] (50:25 - 50:29)
Yeah, I was going to ask, is everything sort of self-reported, like time and cost?


[Justin Drake] (50:29 - 50:53)
So time is not self-reported, and the reason is that we can just measure the time between when the block was produced and when we received the proof. The machine used right now is self-reported, but what anyone can do permissionlessly is kind of get the exact same hardware and then rerun the prover, assuming that it's either the binaries are available or the source code is available.


[Nico Mohnblatt] (50:54 - 51:15)
By the way, availability of binary source code, this is another thing where I found Ethproofs was incredible, is it has really pushed teams to open source their code and open source their binaries because you just had this spreadsheet that lists everyone's name, and then you had two columns in the middle, big red box if you were not open source and big green box if you were, and I find that amazing.


[Justin Drake] (51:16 - 51:49)
Absolutely, and for liveness, one of the requirements is that the prover be open source, because ultimately, if most of the proving is done by a few entities, if those entities go offline, we want anyone to have the option to reboot a prover anywhere in the world, and I think teams are starting to understand that open sourcing the provers is important, and at the beginning of the year, we had basically zero, maybe one prover that was open source, and now we're at a point where we're starting to get close to the majority.


[Anna Rose] (51:51 - 52:28)
I want to throw back once more to that episode I did with Muthu, and I don't know if either of you heard this, it was actually Guillermo was the co-host on it, but in that, you know, Ligero, they're focused a lot more on this like client side. I mean, he was talking about doing proofs on a phone and that you don't even need GPUs. So how do you kind of compare teams like that?


Like in today's model with the pizza pie that you have, it sounds like you don't have, I mean, you don't have a privacy one, and you don't yet have this client side, but yeah, would you be changing the requirements at some point to something like, can you do it on a phone? Or would you add this as like another category?


[Justin Drake] (52:29 - 52:54)
Yeah, so right now we're focused on the server side proving, and it's very much geared towards latency and cost. We would need to find a new benchmark for the client side approving, and one of the interesting things that could potentially involve client side proving at L1 and privacy is this thing called wormhole, which is a fantastic idea.


[Anna Rose] (52:54 - 52:56)
Not the bridge, I guess.


[Justin Drake] (52:56 - 54:06)
Not the bridge, no. Much better wormhole. So the idea is that you can take any Ethereum address and send funds to another Ethereum address and prove that this other Ethereum address is unspendable.


So basically on-chain, it just looks like a normal transaction. There's no way to discriminate against it. And then once you can prove that this address is unspendable, you can go mint another address with the exact same balance.


And for that, we would need to do a hard fork, which enables this wormhole. And, you know, one of the very important things here is the security and simplicity of the proof. And the reason is that, you know, just like Zcash, if there's a soundness bug in the proof, you can just mint an unlimited number of tokens.


So, you know, just like we have this minimal zkVM for signature aggregation, we could imagine potentially the same zkVM, which has been end-to-end verified, also be used for privacy and wormholes.


[Anna Rose] (54:06 - 54:55)
Interesting. I was reading a quote from Tarun today, and I don't know if I'm going to butcher it, but he was talking a little bit about how, you know, in this DeFi context, the crazy thing about adding the ZK all over the place is you can start to do something similar to what you just described with the wormholes. It wasn't about unspendable, but I think it was about like proof of liquidity somewhere or using things as collateral from one part of a chain to an L2 to somewhere else, because the minute you have these proofs, you don't have to do these like lock and hold and bridge, all those concepts where we had to move kind of like, you know, you're not really moving, but like you're theoretically moving tokens to a new place where they could be used as collateral. Here, you just need the proof and then you have collateral in different places.


And yeah, it sounds a little bit like an echo of what you just described.


[Justin Drake] (54:55 - 55:09)
Absolutely. And the way that I think about it is that SNARKs are like the third very powerful tool in the context of blockchain. So we have hashes and signatures, and then there's SNARKs, and you can do all sorts of things.


So we already mentioned...


[Anna Rose] (55:09 - 55:11)
What was the third one? Hashes?


[Justin Drake] (55:12 - 55:14)
Hashes, signatures, and SNARKs is the third one.


[Anna Rose] (55:14 - 55:16)
Oh, okay. Okay. Sorry.


Got it. Cool.


[Justin Drake] (55:16 - 57:18)
And as, you know, Michael Saylor would say, there's no fourth best. That's pretty much all that you need. And, you know, we've already talked about SNARKs unlocking scalability and privacy, but they can unlock all sorts of other things.


So one of them is composability. And there's two aspects of composability here. One is synchronous composability across rollups.


Once you have real-time proving, that becomes possible. And also there's this notion called cross-rollup bundle safety. So, and this is maybe what Tarun was talking about, and it's basically what Agglayer is working on, which is that if you have some sort of an intent for a super transaction that touches multiple rollups, and you want to make sure that it doesn't get unbundled in its constituent transactions, you can basically gate the settlement of this super transaction with a proof that the intent is indeed satisfied. It's all a little bit abstract, but that's one of the things that you can do.


And then the other thing that SNARKs give us, kind of surprisingly, is safety of rollups. So today, rollups have two kinds of attack surfaces. The first one is that there could be bugs in the implementations, whether it's the fraud-proof game or the SNARK circuit, and then the other attack surface is governance.


So if you take a typical EVM-equivalent rollup, they fundamentally need governance in order to stay in sync with the EVM, which itself is a living beast. And so with SNARKs, what we can do is we can expose this precompile on Ethereum called the execute-precompile to have native rollups. So a native rollup is one which reuses the L1 execution, the L1 EVM.


It's an exact copy, if you will, of the EVM. And by definition, it's bug-free, and by construction, it doesn't require governance either.


[Anna Rose] (57:18 - 57:33)
Did you have this from a few years ago, though? Wasn't this like the based rollup or something? Wasn't there...


I don't know if I'm remembering this correctly, but I remember hearing you on a show. I don't think you spoke on this show about it, but yeah, where you were talking about something like this.


[Justin Drake] (57:33 - 58:19)
Yes. So there are these two different concepts. One is based rollups and the other is native rollups, and they touch different layers of the rollup stack.


So the based rollups touch the sequencing, and the idea is that you're sharing the same sequencer as the L1. So you have the validators basically order the transactions of your rollup. The native rollups are those where you're using the execution layer of the L1, namely a copy of the EVM.


And it turns out when you combine these two, when you're both based and native, you become an ultrasound rollup, and you benefit from much, much simpler synchronous composability than otherwise.


[Nico Mohnblatt] (58:20 - 58:41)
So looking into the future of like Ethproofs and zkVMs, do we expect to see sort of a disruption to the patterns that have emerged? Do we expect to see things like lattice-based proof make their way in? Especially if we're looking for maybe smaller proofs, maybe that's something we can get from lattices.


And are we expecting to see maybe RISC-V get swept away? Although I did sort of ask something in that direction earlier.


[Justin Drake] (58:42 - 58:53)
Yeah. So it's always tempting to say, hey, you know, there's been so much research done, like surely we must be done by now. But I'm always surprised at how much innovation we can still...


[Anna Rose] (58:53 - 58:54)
There's always a cool new thing.


[Justin Drake] (58:54 - 59:35)
There's always a cool new thing. And like right now, I think we're witnessing like two mini-revolutions within the hash-based SNARKs, which are kind of maybe worth highlighting. One is this migration from univariate polynomials to multilinear polynomials.


And, you know, the emergence of sum-check as this very efficient primitive to build SNARKs. And then the other like mini-revolution, which is related actually, is the move away from FRI 1.0 to FRI 2.0 and constructions related to that, because more and more we're going to rely on recursion. And, you know, WHIR has just this extremely good recursion.


[Anna Rose] (59:35 - 59:47)
By WHIR, just for anyone who's maybe used to it, it's spelled it's W-H-I-R. It's the WHIR-STIR work that we've done actually an episode on that too. I'll add a link.


Anyway, sorry, Justin, keep going.


[Justin Drake] (59:48 - 1:00:30)
Exactly. And you can think of STIR as being FRI 1.5. So it was an improvement, but WHIR kind of is even better than STIR. I guess one revolution, mini-revolution is the formal verification of circuits.


So traditionally, when you want to formally verify something, you need to do two things. You need to prove that your circuits are sound and that they're complete. And it turns out that the vast majority are soundness bugs with under-constrained circuits.


And there's this new tool called Picus from Veridise, which only tackles the under-constrained part, but in practise, it just finds all of the bugs, almost all of the bugs.


[Anna Rose] (1:00:31 - 1:00:32)
Oh, because most of them are under-constrained.


[Justin Drake] (1:00:32 - 1:00:47)
Yeah, exactly. Yeah. So there was actually this piece of research done where some academics looked at a hundred snark bugs that they could find.


And something like 98 percent or something was under-constrained.


[Anna Rose] (1:00:48 - 1:00:49)
Was that the zkSecurity work?


[Justin Drake] (1:00:50 - 1:00:51)
It's on the website, yes.


[Anna Rose] (1:00:51 - 1:00:57)
OK, I don't know if they did it all, but we actually I think we featured it in our recent State of ZK report as well. Interesting.


[Justin Drake] (1:00:58 - 1:02:57)
And so, you know, we've had RISC-Zero, for example, use them. SP1 is starting to use them. And it's a surprisingly powerful tool.


Yeah, about going back to, you know, your question of bigger revolutions, it is possible that lattices will make a big move. And, you know, I don't want to share like too much alpha, but I was told very recently by a researcher that I really, really trust that there was a massive breakthrough for lattice-based SNARKs, potentially a 100x improvement over the state of the art. And not only is it a performance improvement, but it's also potentially a simplification and complexity improvement.


And so if that's the case, we could see, you know, a revival of these SNARKs. Another thing that you asked about was the ISAs. So right now, it's clear that RISC-V is winning.


But is it just because, you know, all of the tooling is there and really it might be kind of an unstable equilibrium over the very long term thing, like 10, 20 years, whereby if we were to really invest in a from-scratch design and also have the tooling, then we could squeeze more from RISC-V. But one of the kind of surprising outcomes of the last few months and years is that the constraints of hardware kind of mirror themselves to a surprising extent in the design of SNARKs. So you have kind of similar notions of locality and similar, the physical constraints almost kind of match with the cryptography.


At least that's one of the theses that Justin Thaler has been talking about a lot in the context of Jolt. And if that's the case, then we're in for a treat because RISC-V was designed for hardware. And if it's one-to-one, then we can expect hardware to be a long-term equilibrium.


[Anna Rose] (1:02:59 - 1:03:30)
What is the state of things with hardware? Because you talked about this earlier on. You said, like, we had explored FPGAs.


We talked about this around the time of ZK-11. You gave a presentation on like ZK ASICs. This was like something that was being considered.


There was the ZPrize, which was focused very much on hardware as well. That effort generated all this innovation. Yeah.


What is the state of hardware? Because now it sounds like we're just kind of going to go with GPUs. And even, you know, there's some teams talking about moving it all the way to the phone.


So then you're not dealing with specialised hardware at all.


[Justin Drake] (1:03:31 - 1:04:23)
Yeah. So in my opinion, like the main bottleneck that we have now is reducing power. You know, we have basically low latency with these big clusters.


We want to reduce power. And, you know, ASICs really do promise potentially an order of magnitude, maybe more on power. Now, you're right that previously there was some attempts looking at FPGAs.


And one famous example here is Irreducible. And, you know, they recently pivoted away from FPGAs and hardware. They're no longer doing any of that.


They've become a client-side proving shop. And I think that's a testament to the fact that, you know, in the short term, really, GPUs have crushed everything. And potentially it's, you know, has been helped by the advent of AI, right?


Where there's just so many billions of dollars that are poured into these GPUs that are just so, so, so efficient.


[Anna Rose] (1:04:23 - 1:04:24)
They're getting so powerful.


[Justin Drake] (1:04:24 - 1:05:37)
But, you know, fundamentally, if you were to design your circuits to be friendly to prime fields and extension fields and, you know, FFTs and all of the basic operations and your hashes, then you should be able to squeeze an order of magnitude. But the real question is, can a ragtag, you know, ASIC startup with $20 million compete with NVIDIA, which is a $2 trillion company? And, you know, we'll see very soon.


And the reason is that there's this company called Fabric that received a few tens of millions of dollars and they actually taped out. They've received the chips. They've also received what's called the package, which is kind of the envelope around the die.


But the factory that puts the two together somehow is at capacity and can't do it for them. So I'm hoping that in a few weeks, potentially a few months, we will have numbers from Fabric. And they basically designed this ZPU, if you will, like this GPU designed for ZK.


And they claim, you know, 10x potentially performance improvement that we'll see.


[Anna Rose] (1:05:37 - 1:06:15)
I think the challenge, we talked about this a couple of times and I've mentioned it on the show, one of the big challenges here is also this ever evolving proving systems. And if we're moving away, like if we start doing lattices, does that require different hardware? I mean, I don't know, but I'm just like, I think if you're seeing this innovation on the cryptography side, moving at the speed it is, it's been, from what I understand, really hard to just like make a snapshot, freeze things and decide like, yeah, we're going to build this FPGA or rather we're going to build this ASIC, which is like such an investment for this particular system.


It will make it super fast, but like watch all these other systems kind of like run circles around it in the future.


[Justin Drake] (1:06:16 - 1:06:42)
Yeah, so that's one of the selling points of a system like Fabric is that it's programmable. So they have these basic building blocks where you can change the prime field, for example. And they even have CPUs on the die.


So they have RISC-V actually CPUs that can do all sorts of glue logic around these specialised blocks that will do things like FFTs and MSMs.


[Anna Rose] (1:06:43 - 1:08:16)
Cool.


I want to ask you, this is a little bit more just your opinion or what you think is coming. I don't know if Ethproofs has any impact on it or if it's really paying attention to this, but the business model of zkVMs has been something that we've thought about. I think we've talked about a couple of times on the show, something I've been thinking about.


Like 2023, we saw these co-processors come out and we had a few Prover networks. Then we had like the zkVM definition. I mean, RISC-Zero was, I think, the first to really kind of like pioneer this the way we understand it today.


zkVMs start to become Prover marketplaces. So they're sort of like merging these two business models and what it feels like. And then you also have people who have done the verification layer.


So they're like looking to decentralise the verification. You have these different teams that are sort of moving from different quadrants of the Proving stack. And a lot of the time what it looks like is they're trying to find the business model that will work because a zkVM on its own, like doesn't really have a place for a token.


Do you know what I mean? Like it's a virtual machine. It's not a network or marketplace.


It doesn't need a utility token alone. So that's, I think, where the Prover marketplaces come in. But yeah, I'm just curious what you make of these kind of moving business models.


If you think it's sustainable enough to have this many teams too running individual Prover marketplaces, because I think a lot of them are kind of pivoting in that direction. And you just said there's 32 of them, but like, can they all do that? Yeah.


[Justin Drake] (1:08:17 - 1:10:12)
I guess naively you could assume that zkVMs are public goods right there. Like these pieces of open source hardware, software, anyone can copy them. But nowadays, I changed my opinion.


I'd say that's a little myopic. And I actually have a little bit of a story here. So maybe three, four years ago, we approached one of the zkVM teams, the Ethereum Foundation, and we actually tried to acquire them.


And you know, like the ballpark figure for acquisition, acquihires is on the order of a million dollars per head. So you know, one strategy could be to build a company with like 30 excellent engineers and then get acquihired by some other entity.


And I really do think that zkVMs are going to eat the world, like very large parts of computation are going to become snarkified, even if they fall outside of the remit of blockchains. You know, anything that's kind of high integrity, high value, like whether it's banking, medical, or aeronautics and rockets, or all of that, I think, too, will be snarkified. And I think companies like Google will just have, you know, hundreds of engineers working on these things, potentially.


And NVIDIA, you know, will be manufacturing these ZPUs that will unlock this snarkification.


Now, what happened a few years ago, like two or three years ago, is that the VCs came in. So the Defend Foundation was offering like a few tens of millions of dollars for one team. The VCs came in with hundreds of millions of dollars, literally, across, you know, dozens of teams. And they made this big VC bet. And it looks like it's paying off.


And the reason is that just a few days ago, Succinct launched its Prover network and marketplace with the Proof token. And the Proof token has a fully diluted valuation of something like $1.5 billion.


[Anna Rose] (1:10:13 - 1:10:16)
Today, we never know how these things go.


[Justin Drake] (1:10:16 - 1:11:09)
Right, right, right.


We will see. But at least there is a possibility that there will be this cash injection. And from my perspective, it's fantastic because it is, you know, directly funding these public goods in the context of Ethereum layer one, where the business model is not that big.


I mean, if you look at all of the L2s, and you look at all of the throughput, you know, this teragas, 10 million transactions per second, even if you're talking of fractions of a cent of proven cost per transaction, it does add up across the whole network. And so there might be business models, for example, like building the hardware, or having, like, really, really fast algorithms. And, you know, we've seen all sorts of companies like Supranational, like Ingonyama, like ZAN, have like, you know, million dollar kind of contracts to try and accelerate systems.


[Anna Rose] (1:11:09 - 1:11:41)
I hear you on that. I just, I, to me, I don't feel like it's proven yet. I agree with you that there was this like VC boom, this attempt to just inject so much capital, maybe too much capital, maybe too much capital to like a very unpolished product market fit.


You know, like as an industry, almost like that's, and I think that's why we've seen so many teams shift around. Like, to me, it's like, we'll know if it's successful, once a lot of these things have launched and then existed in the wild for a couple years. Yeah, that's my take.


[Justin Drake] (1:11:42 - 1:11:52)
Yeah, I mean, I wouldn't be surprised if a company like RISC-Zero or Succinct gets acquired for over a billion dollars by Microsoft or Google or something like that.


[Anna Rose] (1:11:52 - 1:11:54)
By like a FAANG or something like that.


[Justin Drake] (1:11:54 - 1:11:54)
Yeah.


[Anna Rose] (1:11:55 - 1:11:57)
I don't know if people still use that acronym, but...


[Justin Drake] (1:11:57 - 1:12:11)
Because, you know, we're limited by talent. There's very, very little talent in our space. You know, the hundreds of engineers will make up most of the talent.


And, you know, a company like RISC-Zero and SP1, they have dozens of these top engineers.


[Anna Rose] (1:12:12 - 1:12:36)
Cool. You've been talking a lot about the EF and the structure of the EF and PSE. And we kind of had meant to talk about this a bit earlier in the episode.


Even your own title, like when I was asking you before the show, how do I introduce you? Are you a researcher at EF? You're like, actually, I'm sort of in a different domain now.


So, yeah, can you share a little bit about what those changes have been like, from your perspective, from where you're sitting?


[Justin Drake] (1:12:36 - 1:14:08)
Yeah, so a little bit of a story. I've been at the EF for eight years. And when I came in, you know, I was expecting a very professional team of researchers securing this like decabillion network, but it was kind of a ragtag team of five people.


And, you know, it was an extremely flat organisation, a little chaotic and disorganised, but it also kind of really empowered those who were like very proactive and autonomous. And, you know, now I think we've grown to a size, the EFM Foundation, about 250 people where we need a little bit more structure. And indeed, now we have this notion of a cluster of teams.


So we have at least two clusters. We have the protocol cluster and the echo dev cluster. The protocol one is the technical one.


And we also have PSC, which I guess will be this third cluster. And then within the protocol cluster, I'm part of the architecture team. So there's something like 20 teams within the protocol cluster.


Each team has a team lead. There's also projects and project leads. So there's kind of lots of hierarchy now.


And, you know, it's been working very, very well, at least for the last few months. You know, just to give you a little bit of a sense for pace, this $1 million bounty that I teased earlier in the episode, that's an idea that I had on Saturday. So that was August 9th.


We're recording August 11th. You know, I spent the weekend talking to a few people trying to get permission to announce it on the podcast. And we're moving extremely fast.


[Anna Rose] (1:14:09 - 1:14:12)
And is that a lot faster than it was before all of this reorg?


[Justin Drake] (1:14:12 - 1:14:23)
Yeah, I think so. Because now, you know, we have very, you know, there's clarity as to who are the decision makers, and you can just go to them and get a green light from them and you'll be good.


[Anna Rose] (1:14:23 - 1:15:37)
Cool. I want to close with sort of a throwback to the start of the episode. We talked about the meme, the ultrasound money meme, and kind of like you putting it out in the universe, and then it's sort of it's been redefined in a lot of ways.


We have a lot of ZK memes. We've heard ZK is endgame, ZK for everyone, ZK is the future. But what's the one that everyone like there's this one going around now, which is just like, ZK everything, or ZK, I don't know, like, and often these are coming from individual teams.


Yeah, I'm just wondering, the thing is the goals that you've outlined on Ethproofs, like in a lot of what we've just talked about over the last hour and a half, it's been very much focused on this infrastructure on the ground for developer like you like if you if you're deep in it, you can see these benefits. But then you have these slogans, which are extremely high level, they don't really say anything. Yeah, and there's and I'm just wondering about that sort of, I don't want to say disconnect, because maybe you do need both.


But I'm curious what you make of all that, as a meme lord yourself, you know, I guess meme maker, I should say.


[Justin Drake] (1:15:37 - 1:16:19)
My favourite version of the meme is that realtime ZK is the endgame. Okay.


The realtime is important. I mean, when you zoom out, and you so one of the roles of memes is to recruit, right, like, we need more talent. And it just so happens that empirically speaking, a lot of people come when there is a little bit of froth.


So I personally came in late 2013, you know, Bitcoin was bubbling all of the way to $1,000. And, you know, we have this expression come for the money, stay for the tech. And I think that does apply for for a lot of people.


[Anna Rose] (1:16:20 - 1:16:26)
I'm guilty of it, too. I'm a 2017 joiner. Also an era of bubble, for sure.


[Justin Drake] (1:16:26 - 1:16:58)
Yeah, and we're gonna have a 2025 bubble. And we're gonna have, you know, hopefully, 10s of 1000s, if not, you know, hundreds of 1000s of developers, or, you know, candidate developers come in, and we need to have some sort of an inspirational story to tell them. And so yes, like, whenever there's this disconnect, there's the opportunity to have promises not be fulfilled.


But you need a little bit of that spark in order to get people in. And, you know, it's, think of it as a form of advertisement.


[Anna Rose] (1:16:59 - 1:17:38)
That frothiness you talk about, like so far, a lot of it's been blockchain related, like blockchain, you know, crypto froth, price action. But ZK is this other, it's like this, this side quest where yes, there is, you know, there are teams that have tied themselves to blockchains, but ZK is not inherently a monetary thing, right? It's our and we are seeing it being used by Web2 companies out in the real world.


I'm so excited to see that to sort of see it almost break out of blockchain. But can it retain the froth, quote unquote frothiness without that sort of monetary side of it? Does it always have to be linked to a blockchain for that to work?


[Justin Drake] (1:17:39 - 1:17:51)
No, you're absolutely right. And, you know, one of the things that Vitalik said a few years ago is that even if all blockchains go to zero, it would still have been worth it. And the reason is that...


[Anna Rose] (1:17:51 - 1:17:52)
To have created ZK.


[Justin Drake] (1:17:52 - 1:17:53)
Exactly. Yes.


[Anna Rose] (1:17:53 - 1:18:01)
I love that. To have accelerated ZK, actually, not even create... it existed before, but it was really accelerated through this.


[Justin Drake] (1:18:01 - 1:19:12)
Yeah. And I remember a few years ago, I was I was looking through all of the speakers at Crypto, you know, Crypto is this top cryptography conference. And also, I was looking at the ePrint papers published, and I was shocked at the percentages that were basically motivated by blockchains.


You know, in crypto specifically, like, close to half of the abstract kind of mentioned Bitcoin or Ethereum or blockchain. And, you know, we've provided literally hundreds of millions of dollars at this point of funding. So, for example, you know, the Ethereum Foundation gave a very large grant to the Stanford Blockchain Group, and that funds SBC every year.


And also that has led to so much research out of Dan Boneh's group. And we have just this fantastic symbiosis between cryptography and blockchains. And, you know, you mentioned the froth.


I think the good news is that ZK has such strong technical fundamentals that the froth can't get too out of control, right? There's really this tether, as opposed to a notion like monetary premium, which is much, much more untethered.


[Anna Rose] (1:19:14 - 1:19:33)
Interesting. I feel like this is like, this is a conversation I want to keep having with folks. But I mean, I have been having with folks actually behind the scenes this, like, Tarun will often ask this, like, well, ZK is important, but is it fundable?


Is it going to make anyone money?


And yeah, I don't know. 


[Justin Drake] (1:19:33 - 1:19:38)
Well, NVIDIA is going to be producing zkVM accelerators.


[Anna Rose] (1:19:38 - 1:19:40)
You know this? Like, this is a fact?


[Justin Drake] (1:19:40 - 1:19:50)
Okay, I'm willing to bet anyone, you know, a large amount of money that within 10 years, NVIDIA will be not only generating, producing GPUs, but also, you know, ZPUs, I guess.


[Anna Rose] (1:19:50 - 1:19:56)
That's so cool as an idea. Wow. I mean, I've never heard anyone make that bet, but I like it.


[Justin Drake] (1:19:56 - 1:21:11)
I mean, you know, a very large percentage of the energy consumption in the world will go towards AIs. And so, you know, we need to have like the most energy efficient AIs that we can, hence why we have all of this effort around GPUs. And if indeed you have a thesis that most of, or big chunks of the computation will be verifiable, then because the overheads are so high energetically, we need to have these hardware accelerations.


Like the order of magnitude overhead today is 10,000x. So let me just kind of walk you through this. If you take a typical CPU, let's say running at five watts, it will have a clock speed of let's say 1.5 gigahertz. The way that a prover would have to keep up with this is with 100 GPUs, each running at 15 megahertz, and each GPU consumes 500 watts, which is a hundred times more. So there's a hundred times more power consumption per GPU, and you have a hundred GPUs, hence the 10,000x overhead. We need to get that overhead down if we're going to have significant portions of computation be verifiable.


[Anna Rose] (1:21:11 - 1:22:00)
It's funny, like even talking about it in this context, it makes me think of early internet, like saw security and how kind of Wild West and dangerous it was. That's sort of like what AI is now, which is like completely unwieldy, unprovable, unverifiable. Like it's completely kind of could be bullshit, could be scam.


You have no idea. So much of a mess. And a lot of it's working because it's novelty.


Like all the sort of bad actors haven't like fully activated yet. So it's like at the happy time of the internet, we're like kind of in a bit of a happy time of the AI, kind of. I mean, there's a lot of people who are scared of it, too.


But yeah, but that verifiability, I mean, I always think of it as a little bit of a check on on AI and just kind of giving it a bit more of that like structure, security, making sure you can actually trust what you're dealing with. And there ZK comes in.


[Justin Drake] (1:22:01 - 1:22:24)
Even if we somehow reduce the overhead from 10,000x to just 100x, you know, training in AI, for example, will will cost a hundred megawatt hours or something. And so if you have 100x overhead over that, that will be like 10 gigawatt hours. That is an obscene amount of energy.


And so the more we can optimise, the better.


[Anna Rose] (1:22:25 - 1:22:38)
It's funny, though, I think you're talking more on the scaling front. You're like, we can make it like more efficient. And I'm I'm actually talking on the verify, like the trying to verify things about AI, that more like using it to prove.


[Justin Drake] (1:22:38 - 1:23:01)
Yeah, basically, you want to prove, for example, that training was done properly. So you have a public set of data, you want to make sure that the training was done properly. The overhead to training is going to be 100x, you know, in the best case likely.


And so, you know, energy becomes the bottleneck. I mean, energy is already the bottleneck for AI without verifiability. It's going to be 100 times worse with verifiability.


[Anna Rose] (1:23:02 - 1:23:24)
Crazy. Cool.


Justin, thank you so much for coming on the show. I feel like especially the last little bit, I feel like there's little sparks of conversation. We could probably continue talking about a bunch of these things.


But we have run out of time. So yeah, thanks so much for coming back and for talking to us about Ethproofs, the future of Ethereum, and like kind of the future of the digital internet world.


[Nico Mohnblatt] (1:23:24 - 1:23:34)
Yeah, thank you both. Justin, this is super exciting and looking forward to see the conjecture up to capacity bound being proven and having proven security of small proofs now.


[Anna Rose] (1:23:36 - 1:23:37)
And the lattice stuff you're talking about.


[Nico Mohnblatt] (1:23:37 - 1:23:39)
And the lattice stuff, absolutely.


[Justin Drake] (1:23:39 - 1:23:40)
Yeah. Thanks, guys.


[Anna Rose] (1:23:40 - 1:23:50)
All right. Thanks again.


I want to say thank you to the podcast team, Rachel, Henrik, Tanya, and Kai, and to our listeners. Thanks for listening.